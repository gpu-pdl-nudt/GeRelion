/***************************************************************************
 *
 * Author: "HY. SU"
 * PDL NUDT
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * This complete copyright notice must be included in any revised version of the
 * source code. Additional authorship citations may be added, but existing
 * author citations must be preserved.
 ***************************************************************************/
#include "src/ml_optimiser.h"
#include "src/math_function.h"
 
#include <thrust/device_vector.h>
#include <thrust/device_ptr.h>
#include <thrust/host_vector.h>
#include <thrust/sort.h>
#include <thrust/execution_policy.h>
#include <thrust/generate.h>
#include <thrust/equal.h>
#include <thrust/sequence.h>
#include <thrust/for_each.h>
#include <iostream>
#include <stdlib.h>
//#define DEBUG
//#define DEBUG_CHECKSIZES
//#define CHECKSIZES
//Some global threads management variables
extern "C"{
void relion_gpu_scal(const int N, double *alpha, double *X, int stride);
//void calculate_weight_gpu(const int nr_images, double *exp_Mweight_dev, double *exp_min_diff2, double *pdf_orientation_dev, double *pdf_offset_dev, int xdim_Mweight, int exp_nr_trans, double *exp_thisparticle_sumweight_dev);
//void centerFFT_gpu(double *in, double *out, int nr_images, int dim, int xdim, int ydim, int zdim, bool forward);
}

void MlOptimiser::expectation_gpu()
{

//#define DEBUG_EXP
#ifdef DEBUG_EXP
	std::cerr << "Entering expectation" << std::endl;
#endif

	// Initialise some stuff
	// A. Update current size (may have been changed to ori_size in autoAdjustAngularSampling) and resolution pointers
	updateImageSizeAndResolutionPointers();

	// B. Initialise Fouriertransform, set weights in wsum_model to zero, etc
	expectationSetup();

#ifdef DEBUG_EXP
	std::cerr << "Expectation: done setup" << std::endl;
#endif

	// C. Calculate expected minimum angular errors (only for 3D refinements)
	// And possibly update orientational sampling automatically
	// TODO: also implement estimate angular sampling for 3D refinements
	if (!((iter==1 && do_firstiter_cc) || do_always_cc) && !do_skip_align)
	{
		// Set the exp_metadata (but not the exp_imagedata which is not needed for calculateExpectedAngularErrors)
		int n_trials_acc = (mymodel.ref_dim==3) ? 100 : 10;
		n_trials_acc = XMIPP_MIN(n_trials_acc, mydata.numberOfOriginalParticles());
		getMetaAndImageDataSubset(0, n_trials_acc-1, false);
		calculateExpectedAngularErrors(0, n_trials_acc-1);
	}

	// D. Update the angular sampling (all nodes except master)
	if ( iter > 1 && (do_auto_refine) )
		updateAngularSampling();

	// E. Check whether everything fits into memory, possibly adjust nr_pool and setup thread task managers
	expectationSetupCheckMemory();

#ifdef DEBUG_EXP
	std::cerr << "Expectation: done setupCheckMemory" << std::endl;
#endif
	if (verb > 0)
	{
		std::cout << " Expectation iteration " << iter;
		if (!do_auto_refine)
			std::cout << " of " << nr_iter;
		std::cout << std::endl;
		init_progress_bar(mydata.numberOfOriginalParticles());
	}

	int barstep = XMIPP_MAX(1, mydata.numberOfOriginalParticles() / 60);
	long int prev_barstep = 0, nr_ori_particles_done = 0;

	// Now perform real expectation over all particles
	// Use local parameters here, as also done in the same overloaded function in MlOptimiserMpi
	long int my_first_ori_particle, my_last_ori_particle;

	//nr_get2d = new int[sampling.NrDirections()*sampling.NrPsiSamplings()*sampling.oversamplingFactorOrientations(adaptive_oversampling)]; value_get2d = new double[sampling.NrDirections()*sampling.NrPsiSamplings()*sampling.oversamplingFactorOrientations(adaptive_oversampling)]; ok_get2d = true; memset(nr_get2d, 0, sizeof(int)*sampling.NrDirections()*sampling.NrPsiSamplings()*sampling.oversamplingFactorOrientations(adaptive_oversampling)); memset(value_get2d, 0, sizeof(double)*sampling.NrDirections()*sampling.NrPsiSamplings()*sampling.oversamplingFactorOrientations(adaptive_oversampling));
	//printf("%d %d %d\n", sampling.NrDirections(), sampling.oversamplingFactorOrientations(adaptive_oversampling), exp_local_Fimgs_shifted[0].zyxdim);

	
	while (nr_ori_particles_done < mydata.numberOfOriginalParticles())  //should be Parallel with GPU, the workloads of one image of each particle is assigined to one thread block,
	{
		//my_first_ori_particle = 0;//nr_ori_particles_done;
		my_first_ori_particle = nr_ori_particles_done;
		my_last_ori_particle = XMIPP_MIN(mydata.numberOfOriginalParticles() - 1, my_first_ori_particle + nr_pool - 1);  //enlarge the value of nr_pool
		//my_last_ori_particle = mydata.numberOfOriginalParticles()-1;//XMIPP_MIN(mydata.numberOfOriginalParticles() - 1, my_first_ori_particle + mydata.numberOfOriginalParticles() - 1);
	long t1, t2, t3 ,t4;
	t1 = t2 = t3 = t4 = 0;
		struct timeval start, end;

		gettimeofday(&start, NULL);
		// Get the metadata for these particles
		getMetaAndImageDataSubset(my_first_ori_particle, my_last_ori_particle); //Not parallelized, will be extracted before the loop
		gettimeofday(&end, NULL);
		t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
		// perform the actual expectation step on several particles
		gettimeofday(&start, NULL);
		//expectationSomeParticles_gpu(my_first_ori_particle, my_last_ori_particle); // The parallel function
		if (mode == 1)
			expectationSomeParticles_gpu(my_first_ori_particle, my_last_ori_particle);
		else if (mode == 2)
			expectationSomeParticles_compare(my_first_ori_particle, my_last_ori_particle); // The parallel function
		gettimeofday(&end, NULL);
		t2 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
		// Set the metadata for these particles
		gettimeofday(&start, NULL);
		setMetaDataSubset(my_first_ori_particle, my_last_ori_particle);
		gettimeofday(&end, NULL);
		t3 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
		// Also monitor the changes in the optimal orientations and classes
		gettimeofday(&start, NULL);
		monitorHiddenVariableChanges(my_first_ori_particle, my_last_ori_particle);
		gettimeofday(&end, NULL);
		t4 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

	std::cout << my_last_ori_particle << "expectation " << t1/1000000. << " " << t2/1000000. << " " << t3/1000000. << " " << t4/1000000. << std::endl;

		nr_ori_particles_done += my_last_ori_particle - my_first_ori_particle + 1;

		if (verb > 0 && nr_ori_particles_done - prev_barstep > barstep)
		{
			prev_barstep = nr_ori_particles_done;
			progress_bar(nr_ori_particles_done);
		}
	}

	//for (int i = 0; i <exp_nr_rot; i++) printf("%d %f ", nr_get2d[i], value_get2d[i]); printf("\n");
	if (verb > 0)
		progress_bar(mydata.numberOfOriginalParticles());

	// Clean up some memory
	for (int iclass = 0; iclass < mymodel.nr_classes; iclass++)
		mymodel.PPref[iclass].data.clear();
#ifdef DEBUG_EXP
	std::cerr << "Expectation: done " << std::endl;
#endif

}

void MlOptimiser::expectationSomeParticles_gpu(long int my_first_ori_particle, long int my_last_ori_particle)
{

#ifdef TIMING
	timer.tic(TIMING_ESP);
#endif

//#define DEBUG_EXPSINGLE
#ifdef DEBUG_EXPSINGLE
	std::cerr << "Entering expectationSomeParticles..." << std::endl;
#endif

#ifdef TIMING
    timer.tic(TIMING_ESP);
    timer.tic(TIMING_ESP_READ);
#endif

    // Use global variables for thread visibility
	exp_my_first_ori_particle = my_first_ori_particle;
    exp_my_last_ori_particle = my_last_ori_particle;
    exp_nr_ori_particles = exp_my_last_ori_particle - exp_my_first_ori_particle + 1;

    // Find out how many particles there are in these ori_particles
    exp_nr_particles = 0;
    for (long int i = my_first_ori_particle; i <= my_last_ori_particle; i++)
    	exp_nr_particles += mydata.ori_particles[i].particles_id.size();

    // If there are more than one particle in each ori_particle, then do these in parallel with threads
    if (nr_pool == 1 && exp_nr_particles/exp_nr_ori_particles > 1) 
    {
    	int my_pool = exp_nr_particles/exp_nr_ori_particles;
    	exp_ipart_ThreadTaskDistributor->resize(my_pool, 1);
    }

    // TODO: MAKE SURE THAT ALL PARTICLES IN SomeParticles ARE FROM THE SAME AREA, SO THAT THE R_mic CAN BE RE_USED!!!

	// In the first iteration, multiple seeds will be generated
	// A single random class is selected for each pool of images, and one does not marginalise over the orientations
	// The optimal orientation is based on signal-product (rather than the signal-intensity sensitive Gaussian)
    // If do_firstiter_cc, then first perform a single iteration with K=1 and cross-correlation criteria, afterwards

    // Generally: use all references
    iclass_min = 0;
    iclass_max = mymodel.nr_classes - 1;
    // low-pass filter again and generate the seeds
    if (do_generate_seeds)
    {
    	if (do_firstiter_cc && iter == 1)
    	{
    		// In first (CC) iter, use a single reference (and CC)
    		iclass_min = iclass_max = 0;
    	}
    	else if ( (do_firstiter_cc && iter == 2) || (!do_firstiter_cc && iter == 1))
		{
			// In second CC iter, or first iter without CC: generate the seeds
    		// Now select a single random class
    		// exp_part_id is already in randomized order (controlled by -seed)
    		// WARNING: USING SAME iclass_min AND iclass_max FOR SomeParticles!!
			iclass_min = iclass_max = divide_equally_which_group(mydata.numberOfOriginalParticles(), mymodel.nr_classes, exp_my_first_ori_particle);
		}
    }

	// TODO: think of a way to have the different images in a single series have DIFFERENT offsets!!!
	// Right now, they are only centered with a fixed relative translation!!!!

// Thid debug is a good one to step through the separate steps of the expectation to see where trouble lies....
//#define DEBUG_ESP_MEM
#ifdef DEBUG_ESP_MEM
	char c;
	std::cerr << "Before getFourierTransformsAndCtfs, press any key to continue... " << std::endl;
	std::cin >> c;
#endif

	// Read all image of this series into memory, apply old origin offsets and store Fimg, Fctf, exp_old_xoff and exp_old_yoff in vectors./

	//exp_ipart_ThreadTaskDistributor->reset();
	//global_ThreadManager->run(globalGetFourierTransformsAndCtfs);

	//The GPU function for GetFourierTransformsAndCtfs, do not use the pthread programming
	long t1, t2, t3, t4;
	t1 = t2 = t3 = t4 = 0;
	struct timeval start, end;

		gettimeofday(&start, NULL);
		//doThreadGetFourierTransformsAndCtfs_gpu();
		do_nothing();
		//doThreadGetFourierTransformsAndCtfs();
		gettimeofday(&end, NULL);
		t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
	if (do_realign_movies )//&& movie_frame_running_avg_side > 0)
	{
		calculateRunningAveragesOfMovieFrames();
	}

#ifdef DEBUG_ESP_MEM
	std::cerr << "After getFourierTransformsAndCtfs, press any key to continue... " << std::endl;
	std::cin >> c;
#endif

	#ifdef TIMING
    timer.toc(TIMING_ESP_READ);
#endif

	// Initialise significant weight to minus one, so that all coarse sampling points will be handled in the first pass
	exp_significant_weight.clear();
	exp_significant_weight.resize(exp_nr_particles);
	for (int n = 0; n < exp_nr_particles; n++)
		exp_significant_weight[n] = -1.;

	// Number of rotational and translational sampling points
	exp_nr_trans = sampling.NrTranslationalSamplings();

	exp_nr_dir = sampling.NrDirections();
	exp_nr_psi = sampling.NrPsiSamplings();
	exp_nr_rot = exp_nr_dir * exp_nr_psi;
	// Only perform a second pass when using adaptive oversampling
	int nr_sampling_passes = (adaptive_oversampling > 0) ? 2 : 1;
	// Pass twice through the sampling of the entire space of rot, tilt and psi
	// The first pass uses a coarser angular sampling and possibly smaller FFTs than the second pass.
	// Only those sampling points that contribute to the highest x% of the weights in the first pass are oversampled in the second pass
	// Only those sampling points will contribute to the weighted sums in the third loop below

	for (exp_ipass = 0; exp_ipass < nr_sampling_passes; exp_ipass++)
	{

		if (strict_highres_exp > 0.)
			// Use smaller images in both passes and keep a maximum on coarse_size, just like in FREALIGN
			exp_current_image_size = coarse_size;
		else if (adaptive_oversampling > 0)
			// Use smaller images in the first pass, larger ones in the second pass
			exp_current_image_size = (exp_ipass == 0) ? coarse_size : mymodel.current_size;
		else
			exp_current_image_size = mymodel.current_size;

		// Use coarse sampling in the first pass, oversampled one the second pass
		exp_current_oversampling = (exp_ipass == 0) ? 0 : adaptive_oversampling;
		exp_nr_oversampled_rot = sampling.oversamplingFactorOrientations(exp_current_oversampling);
		exp_nr_oversampled_trans = sampling.oversamplingFactorTranslations(exp_current_oversampling);


#ifdef DEBUG_ESP_MEM

	std::cerr << "Before getAllSquaredDifferences, use top to see memory usage and then press any key to continue... " << std::endl;
	std::cin >> c;
#endif


		// Calculate the squared difference terms inside the Gaussian kernel for all hidden variables
		gettimeofday(&start, NULL);
		getAllSquaredDifferences_gpu();
		gettimeofday(&end, NULL);
		t2 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
#ifdef DEBUG_ESP_MEM
	std::cerr << "After getAllSquaredDifferences, use top to see memory usage and then press any key to continue... " << std::endl;
	std::cin >> c;
#endif

		// Now convert the squared difference terms to weights,
		// also calculate exp_sum_weight, and in case of adaptive oversampling also exp_significant_weight
		//convertAllSquaredDifferencesToWeights();
		gettimeofday(&start, NULL);
		convertAllSquaredDifferencesToWeights_gpu();
		gettimeofday(&end, NULL);
		t3 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
		
		if (exp_ipass == 0)
		{
			cudaFree(exp_local_sqrtXi2_dev);
			cudaFree(exp_local_Fimgs_shifted_dev);
			cudaFree(exp_local_Fimgs_shifted_nomask_dev);
			cudaFree(exp_Minvsigma2s_dev);
			cudaFree(exp_local_Fctfs_dev);		
		}

		//cudaMemcpy(exp_Mweight.data, exp_Mweight_dev, exp_nr_particles*exp_Mweight_dev_size*sizeof(double), cudaMemcpyDeviceToHost);
		double* exp_min_diff2_host = new double[exp_nr_particles];
		cudaMemcpy(exp_min_diff2_host, exp_min_diff2_dev, exp_nr_particles*sizeof(double), cudaMemcpyDeviceToHost);
		exp_min_diff2.resize(exp_nr_particles);
		for (int i = 0; i < exp_nr_particles; i++) 
			exp_min_diff2[i] = exp_min_diff2_host[i];

		free(exp_min_diff2_host);
		cudaFree(exp_min_diff2_dev);
		cudaFree(exp_Mweight_dev);


#ifdef DEBUG_ESP_MEM
	std::cerr << "After convertAllSquaredDifferencesToWeights, press any key to continue... " << std::endl;
	std::cin >> c;
#endif

	}// end loop over 2 exp_ipass iterations

	//for (int i = 0; i <exp_nr_rot; i++) printf("%d %f ", nr_get2d[i], value_get2d[i]); printf("\n");
	// For the reconstruction step use mymodel.current_size!
	exp_current_image_size = mymodel.current_size;

#ifdef DEBUG_ESP_MEM
	std::cerr << "Before storeWeightedSums, press any key to continue... " << std::endl;
	std::cin >> c;
#endif
		gettimeofday(&start, NULL);
		//storeWeightedSums();
		storeWeightedSums_gpu();
		gettimeofday(&end, NULL);
		t4 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

		cudaFree(exp_local_sqrtXi2_dev);
		cudaFree(exp_local_Fimgs_shifted_dev);
		cudaFree(exp_local_Fimgs_shifted_nomask_dev);
		cudaFree(exp_Minvsigma2s_dev);
		cudaFree(exp_local_Fctfs_dev);	
		cudaFree(exp_Fimgs_dev);
		cudaFree(exp_Fimgs_nomask_dev);
		
	std::cout << my_last_ori_particle << " expectationSomeParticles_gpu " << t1/1000000. << " " << t2/1000000. << " " << t3/1000000. << " " << t4/1000000. << std::endl;
	std::cout << my_last_ori_particle << " getSquared_line_time ";
	TIMER_TIME



	std::cout << my_last_ori_particle << " getSquared_line_count ";


	TIMER_COUNT




	std::cout << my_last_ori_particle << " convertAll_gpu " << t_convert_cpu.time << " " << t_convert_prepare.time << " " << t_convert_kernel.time << " " << std::endl;
/*	
		cudaFree(exp_local_sqrtXi2_dev);
		cudaFree(exp_local_Fimgs_shifted_dev);
		cudaFree(exp_local_Fimgs_shifted_nomask_dev);
		cudaFree(exp_Minvsigma2s_dev);
		cudaFree(exp_local_Fctfs_dev);
*/
	// Now calculate the optimal translation for each of the individual images in the series
	//if (mydata.maxNumberOfImagesPerOriginalParticle(my_first_ori_particle, my_last_ori_particle) > 1 && !(do_firstiter_cc && iter == 1))
	//	getOptimalOrientationsForIndividualImagesInSeries();

#ifdef DEBUG_ESP_MEM
	std::cerr << "After storeWeightedSums, press any key to continue... " << std::endl;
	std::cin >> c;
#endif
#ifdef DEBUG_EXPSINGLE
		std::cerr << "Leaving expectationSingleParticle..." << std::endl;
#endif

#ifdef TIMING
	timer.toc(TIMING_ESP);
#endif

}

void MlOptimiser::maximization_gpu()
{

	if (verb > 0)
	{
		std::cout << " Maximization ..." << std::endl;
		init_progress_bar(mymodel.nr_classes);
	}

	// First reconstruct the images for each class
	for (int iclass = 0; iclass < mymodel.nr_classes; iclass++)
	{
		if (mymodel.pdf_class[iclass] > 0.)
		{
			(wsum_model.BPref[iclass]).reconstruct(mymodel.Iref[iclass], gridding_nr_iter, do_map,
					mymodel.tau2_fudge_factor, mymodel.tau2_class[iclass], mymodel.sigma2_class[iclass],
					mymodel.data_vs_prior_class[iclass], mymodel.fsc_halves_class[iclass], wsum_model.pdf_class[iclass],
					false, false, nr_threads, minres_map);

		}
		else
		{
			mymodel.Iref[iclass].initZeros();
		}

		if (verb > 0)
			progress_bar(iclass);
	}

	// Then perform the update of all other model parameters
	maximizationOtherParameters();

	// Keep track of changes in hidden variables
	updateOverallChangesInHiddenVariables();

	// This doesn't really work, and I need the original priors for the polishing...
	//if (do_realign_movies)
	//	updatePriorsForMovieFrames();

	if (verb > 0)
		progress_bar(mymodel.nr_classes);

}

void MlOptimiser::maximizationOtherParameters_gpu()
{
	// Note that reconstructions are done elsewhere!
#ifdef DEBUG
	std::cerr << "Entering maximizationOtherParameters" << std::endl;
#endif

	// Calculate total sum of weights, and average CTF for each class (for SSNR estimation)
	double sum_weight = 0.;
	for (int iclass = 0; iclass < mymodel.nr_classes; iclass++)
		sum_weight += wsum_model.pdf_class[iclass];

	// Update average norm_correction
	if (do_norm_correction)
	{
		mymodel.avg_norm_correction = wsum_model.avg_norm_correction / sum_weight;
	}

	if (do_scale_correction && !(iter==1 && do_firstiter_cc) )
	{
		double avg_scale_correction = 0., nr_part = 0.;
		for (int igroup = 0; igroup < mymodel.nr_groups; igroup++)
		{

#ifdef DEVEL_BFAC
			// TMP
			if (verb>0)
			{
				for (int i=0; i<XSIZE(wsum_model.wsum_signal_product_spectra[igroup]); i++)
				{
					std::cout <<" igroup= "<<igroup<< " i= "<<i<<" "<<wsum_model.wsum_signal_product_spectra[igroup](i)<<" "<<wsum_model.wsum_reference_power_spectra[igroup](i)<<std::endl;
				}
			}
#endif

			double sumXA = wsum_model.wsum_signal_product_spectra[igroup].sum();
			double sumAA = wsum_model.wsum_reference_power_spectra[igroup].sum();
			if (sumAA > 0.)
				mymodel.scale_correction[igroup] = sumXA / sumAA;
			else
				mymodel.scale_correction[igroup] = 1.;
			avg_scale_correction += (double)(mymodel.nr_particles_group[igroup]) * mymodel.scale_correction[igroup];
			nr_part += (double)(mymodel.nr_particles_group[igroup]);

		}

		// Constrain average scale_correction to one.
		avg_scale_correction /= nr_part;
		for (int igroup = 0; igroup < mymodel.nr_groups; igroup++)
		{
			mymodel.scale_correction[igroup] /= avg_scale_correction;
//#define DEBUG_UPDATE_SCALE
#ifdef DEBUG_UPDATE_SCALE
			if (verb > 0)
			{
				std::cerr<< "Group "<<igroup+1<<": scale_correction= "<<mymodel.scale_correction[igroup]<<std::endl;
				for (int i = 0; i < XSIZE(wsum_model.wsum_reference_power_spectra[igroup]); i++)
					if (wsum_model.wsum_reference_power_spectra[igroup](i)> 0.)
						std::cerr << " i= " << i << " XA= " << wsum_model.wsum_signal_product_spectra[igroup](i)
											<< " A2= " << wsum_model.wsum_reference_power_spectra[igroup](i)
											<< " XA/A2= " << wsum_model.wsum_signal_product_spectra[igroup](i)/wsum_model.wsum_reference_power_spectra[igroup](i) << std::endl;

			}
#endif
		}

	}

	// Update model.pdf_class vector (for each k)
	for (int iclass = 0; iclass < mymodel.nr_classes; iclass++)
	{
		mymodel.pdf_class[iclass] = wsum_model.pdf_class[iclass] / sum_weight;

		// for 2D also update priors of translations for each class!
		if (mymodel.ref_dim == 2)
		{
			if (wsum_model.pdf_class[iclass] > 0.)
				mymodel.prior_offset_class[iclass] = wsum_model.prior_offset_class[iclass] / wsum_model.pdf_class[iclass];
			else
				mymodel.prior_offset_class[iclass].initZeros();
		}

		// Use sampling.NrDirections(0, true) to include all directions (also those with zero prior probability for any given image)
		for (int idir = 0; idir < sampling.NrDirections(0, true); idir++)
		{
			mymodel.pdf_direction[iclass](idir) = wsum_model.pdf_direction[iclass](idir) / sum_weight;
		}
	}

	// Update sigma2_offset
	// Factor 2 because of the 2-dimensionality of the xy-plane
	if (!fix_sigma_offset)
		mymodel.sigma2_offset = (wsum_model.sigma2_offset) / (2. * sum_weight);

	// TODO: update estimates for sigma2_rot, sigma2_tilt and sigma2_psi!

	// Also refrain from updating sigma_noise after the first iteration with first_iter_cc!
	if (!fix_sigma_noise && !(iter == 1 && do_firstiter_cc))
	{
		for (int igroup = 0; igroup < mymodel.nr_groups; igroup++)
		{
			// Factor 2 because of the 2-dimensionality of the complex-plane
			FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(mymodel.sigma2_noise[igroup])
			{
				DIRECT_MULTIDIM_ELEM(mymodel.sigma2_noise[igroup], n) =
						DIRECT_MULTIDIM_ELEM(wsum_model.sigma2_noise[igroup], n ) /
							(2. * wsum_model.sumw_group[igroup] * DIRECT_MULTIDIM_ELEM(Npix_per_shell, n));
			}
		}
	}

	// After the first iteration the references are always CTF-corrected
    if (do_ctf_correction)
    	refs_are_ctf_corrected = true;

	// Some statistics to output
	mymodel.LL = 	wsum_model.LL;
	if ((iter==1 && do_firstiter_cc) || do_always_cc)
		mymodel.LL /= sum_weight; // this now stores the average ccf
	mymodel.ave_Pmax = wsum_model.ave_Pmax / sum_weight;

	// After the first, special iteration, apply low-pass filter of -ini_high again
	if (iter == 1 && do_firstiter_cc)
	{
		initialLowPassFilterReferences();
		if (ini_high > 0.)
		{
			// Adjust the tau2_class and data_vs_prior_class, because they were calculated on the unfiltered maps
			// This is merely a matter of having correct output in the model.star file (these values are not used in the calculations)
			double radius = mymodel.ori_size * mymodel.pixel_size / ini_high;
			radius -= WIDTH_FMASK_EDGE / 2.;
			double radius_p = radius + WIDTH_FMASK_EDGE;

			for (int iclass = 0; iclass < mymodel.nr_classes; iclass++)
			{
				for (int rr = 0; rr < XSIZE(mymodel.tau2_class[iclass]); rr++)
				{
					double r = (double)rr;
					if (r < radius)
						continue;
					else if (r > radius_p)
					{
						DIRECT_A1D_ELEM(mymodel.tau2_class[iclass], rr) = 0.;
						DIRECT_A1D_ELEM(mymodel.data_vs_prior_class[iclass], rr) = 0.;
					}
					else
					{
						double raisedcos = 0.5 - 0.5 * cos(PI * (radius_p - r) / WIDTH_FMASK_EDGE);
						DIRECT_A1D_ELEM(mymodel.tau2_class[iclass], rr) *= raisedcos * raisedcos;
						DIRECT_A1D_ELEM(mymodel.data_vs_prior_class[iclass], rr) *= raisedcos * raisedcos;
					}
				}
			}
		}

		if (do_generate_seeds && mymodel.nr_classes > 1)
		{
			// In the first CC-iteration only a single reference was used
			// Now copy this one reference to all K references, for seed generation in the second iteration
			for (int iclass = 1; iclass < mymodel.nr_classes; iclass++)
			{
				mymodel.tau2_class[iclass] =  mymodel.tau2_class[0];
				mymodel.data_vs_prior_class[iclass] = mymodel.data_vs_prior_class[0];
				mymodel.pdf_class[iclass] = mymodel.pdf_class[0] / mymodel.nr_classes;
				mymodel.pdf_direction[iclass] = mymodel.pdf_direction[0];
				mymodel.Iref[iclass] = mymodel.Iref[0];
			}
			mymodel.pdf_class[0] /= mymodel.nr_classes;
		}

	}

#ifdef DEBUG
	std::cerr << "Leaving maximizationOtherParameters" << std::endl;
#endif
}

void MlOptimiser::doThreadGetFourierTransformsAndCtfs_syn( )
{
		
		// Only first thread initialises
	//if (thread_id == 0)
	//{
		exp_starting_image_no.clear();
		exp_power_imgs.clear();
		exp_highres_Xi2_imgs.clear();
		exp_Fimgs.clear();
		exp_Fimgs_nomask.clear();
		exp_Fctfs.clear();
		exp_old_offset.clear();
		exp_prior.clear();
		exp_local_oldcc.clear();
		exp_ipart_to_part_id.clear();
		exp_ipart_to_ori_part_id.clear();
		exp_ipart_to_ori_part_nframe.clear();
		exp_iimg_to_ipart.clear();

		// Resize to the right size instead of using pushbacks
		exp_starting_image_no.resize(exp_nr_particles);

		// First check how many images there are in the series for each particle...
		// And calculate exp_nr_images
		exp_nr_images = 0;
		for (long int ori_part_id = exp_my_first_ori_particle,  ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	    {

#ifdef DEBUG_CHECKSIZES
			if (ori_part_id >= mydata.ori_particles.size())
			{
				std::cerr<< "ori_part_id= "<<ori_part_id<<" mydata.ori_particles.size()= "<< mydata.ori_particles.size() <<std::endl;
				REPORT_ERROR("ori_part_id >= mydata.ori_particles.size()");
			}
#endif
			for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
			{
				long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
				int iipart = exp_ipart_to_part_id.size();
				exp_starting_image_no.at(iipart) = exp_nr_images;
				exp_nr_images += mydata.getNrImagesInSeries(part_id);
				exp_ipart_to_part_id.push_back(part_id);
				exp_ipart_to_ori_part_id.push_back(ori_part_id);
				exp_ipart_to_ori_part_nframe.push_back(i);
				for (int i = 0; i < mydata.getNrImagesInSeries(part_id); i++)
					exp_iimg_to_ipart.push_back(iipart);
			}
	    }
		// Then also resize vectors for all images
		exp_power_imgs.resize(exp_nr_images);
		exp_highres_Xi2_imgs.resize(exp_nr_images);
		exp_Fimgs.resize(exp_nr_images);
		exp_Fimgs_nomask.resize(exp_nr_images);
		exp_Fctfs.resize(exp_nr_images);
		exp_old_offset.resize(exp_nr_images);
		exp_prior.resize(exp_nr_images);
		exp_local_oldcc.resize(exp_nr_images);

	//}
	//global_barrier->wait();

	FourierTransformer transformer;
	//size_t first_ipart, last_ipart;
	//first_ipart = exp_my_first_ori_particle;
	//last_ipart = exp_my_last_ori_particle;
	//while (exp_ipart_ThreadTaskDistributor->getTasks(first_ipart, last_ipart))
	//{

		for (long int ipart = 0; ; ipart++)
		{
			// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
			// but some, e.g. the last, batch of pooled particles may be smaller
			if (ipart >= exp_nr_particles)
				break;

#ifdef DEBUG_CHECKSIZES
			if (ipart >= exp_ipart_to_part_id.size())
			{
				std::cerr<< "ipart= "<<ipart<<" exp_ipart_to_part_id.size()= "<< exp_ipart_to_part_id.size() <<std::endl;
				REPORT_ERROR("ipart >= exp_ipart_to_part_id.size()");
			}
#endif
			long int part_id = exp_ipart_to_part_id[ipart];

			// Prevent movies and series at the same time...
			if (mydata.getNrImagesInSeries(part_id) > 1 && do_realign_movies)
				REPORT_ERROR("Not ready yet for dealing with image series at the same time as realigning movie frames....");

			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{

				FileName fn_img;
				Image<double> img, rec_img;
				MultidimArray<Complex > Fimg, Faux;
				MultidimArray<double> Fctf;
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				int group_id = mydata.getGroupId(part_id, iseries);

				// Get the norm_correction
				double normcorr = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM);

				// Get the optimal origin offsets from the previous iteration
				Matrix1D<double> my_old_offset(2), my_prior(2);
				XX(my_old_offset) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF);
				YY(my_old_offset) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF);
				XX(my_prior)      = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF_PRIOR);
				YY(my_prior)      = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF_PRIOR);
				// Uninitialised priors were set to 999.
				if (XX(my_prior) > 998.99 && XX(my_prior) < 999.01)
					XX(my_prior) = 0.;
				if (YY(my_prior) > 998.99 && YY(my_prior) < 999.01)
					YY(my_prior) = 0.;

				// Get the old cross-correlations
				exp_local_oldcc.at(my_image_no) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_DLL);

				// If we do local angular searches, get the previously assigned angles to center the prior
				// Only do this for the first image in the series, as this prior work per-particle, not per-image
				// All images in the series use the same rotational sampling, brought back to "exp_R_mic=identity"
				if (do_skip_align || do_skip_rotate)
				{
					// No need to block the threads global_mutex, as nr_pool will be set to 1 anyway for do_skip_align!
					if (do_skip_align)
					{
						// Rounded translations will be applied to the image upon reading,
						// set the unique translation in the sampling object to the fractional difference
						Matrix1D<double> rounded_offset = my_old_offset;
						rounded_offset.selfROUND();
						rounded_offset = my_old_offset - rounded_offset;
						sampling.setOneTranslation(rounded_offset);
					}

					// Also set the rotations
					double old_rot, old_tilt, old_psi;
					old_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT);
					old_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT);
					old_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI);
					sampling.setOneOrientation(old_rot, old_tilt, old_psi);

				}
				else if (mymodel.orientational_prior_mode != NOPRIOR && iseries == 0)
				{
					// First try if there are some fixed prior angles
					double prior_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT_PRIOR);
					double prior_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT_PRIOR);
					double prior_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI_PRIOR);

					// If there were no defined priors (i.e. their values were 999.), then use the "normal" angles
					if (prior_rot > 998.99 && prior_rot < 999.01)
						prior_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT);
					if (prior_tilt > 998.99 && prior_tilt < 999.01)
						prior_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT);
					if (prior_psi > 998.99 && prior_psi < 999.01)
						prior_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI);

					// For tilted series: convert the angles back onto the untilted ones...
					// Calculate the angles back from the Euler matrix because for tilt series exp_R_mic may have changed them...
					Matrix2D<double> A, R_mic(3,3);
					R_mic(0,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_0);
					R_mic(0,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_1);
					R_mic(0,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_2);
					R_mic(1,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_0);
					R_mic(1,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_1);
					R_mic(1,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_2);
					R_mic(2,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_0);
					R_mic(2,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_1);
					R_mic(2,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_2);
					if (!R_mic.isIdentity())
					{
						Euler_angles2matrix(prior_rot, prior_tilt, prior_psi, A);
						A = R_mic.inv() * A;
						Euler_matrix2angles(A, prior_rot, prior_tilt, prior_psi);
					}

					//global_mutex.lock();

					// Select only those orientations that have non-zero prior probability
					sampling.selectOrientationsWithNonZeroPriorProbability(prior_rot, prior_tilt, prior_psi,
							sqrt(mymodel.sigma2_rot), sqrt(mymodel.sigma2_tilt), sqrt(mymodel.sigma2_psi));

					long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
					if (nr_orients == 0)
					{
						std::cerr << " sampling.NrDirections()= " << sampling.NrDirections() << " sampling.NrPsiSamplings()= " << sampling.NrPsiSamplings() << std::endl;
						REPORT_ERROR("Zero orientations fall within the local angular search. Increase the sigma-value(s) on the orientations!");
					}
					int threadBlockSize = (nr_orients > 100) ? 10 : 1;

					exp_iorient_ThreadTaskDistributor->resize(nr_orients, threadBlockSize);

					//global_mutex.unlock();

				}

				// Unpack the image from the imagedata
				img().resize(mymodel.ori_size, mymodel.ori_size);
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(img())
				{
					DIRECT_A2D_ELEM(img(), i, j) = DIRECT_A3D_ELEM(exp_imagedata, my_image_no, i, j);
				}
				img().setXmippOrigin();
				if (has_converged && do_use_reconstruct_images)
				{
					rec_img().resize(mymodel.ori_size, mymodel.ori_size);
					FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(rec_img())
					{
						DIRECT_A2D_ELEM(rec_img(), i, j) = DIRECT_A3D_ELEM(exp_imagedata, exp_nr_images + my_image_no, i, j);
					}
					rec_img().setXmippOrigin();
				}
//#define DEBUG_SOFTMASK
#ifdef DEBUG_SOFTMASK
					Image<double> tt;
					tt()=img();
					tt.write("Fimg_unmasked.spi");
					std::cerr << "written Fimg_unmasked.spi; press any key to continue..." << std::endl;
					char c;
					std::cin >> c;
#endif
				// Apply the norm_correction term
				if (do_norm_correction)
				{
//#define DEBUG_NORM
#ifdef DEBUG_NORM
					if (normcorr < 0.001 || normcorr > 1000. || mymodel.avg_norm_correction < 0.001 || mymodel.avg_norm_correction > 1000.)
					{
						std::cerr << " ** normcorr= " << normcorr << std::endl;
						std::cerr << " ** mymodel.avg_norm_correction= " << mymodel.avg_norm_correction << std::endl;
						std::cerr << " ** fn_img= " << fn_img << " part_id= " << part_id << std::endl;
						std::cerr << " ** iseries= " << iseries << " ipart= " << ipart << " part_id= " << part_id << std::endl;
						int group_id = mydata.getGroupId(part_id, iseries);
						std::cerr << " ml_model.sigma2_noise[group_id]= " << mymodel.sigma2_noise[group_id] << " group_id= " << group_id <<std::endl;
						std::cerr << " part_id= " << part_id << " iseries= " << iseries << std::endl;
						std::cerr << " img_id= " << img_id << std::endl;
						REPORT_ERROR("Very small or very big (avg) normcorr!");
					}
#endif
					img() *= mymodel.avg_norm_correction / normcorr;
				}

				// Apply (rounded) old offsets first
				my_old_offset.selfROUND();
				selfTranslate(img(), my_old_offset, DONT_WRAP);
				if (has_converged && do_use_reconstruct_images)
					selfTranslate(rec_img(), my_old_offset, DONT_WRAP);

				exp_old_offset.at(my_image_no) = my_old_offset;
				// Also store priors on translations
				exp_prior.at(my_image_no) = my_prior;

				// Always store FT of image without mask (to be used for the reconstruction)
				MultidimArray<double> img_aux;
				img_aux = (has_converged && do_use_reconstruct_images) ? rec_img() : img();
				CenterFFT(img_aux, true);
				transformer.FourierTransform(img_aux, Faux);
				windowFourierTransform(Faux, Fimg, mymodel.current_size);

				// Here apply the beamtilt correction if necessary
				// This will only be used for reconstruction, not for alignment
				// But beamtilt only affects very high-resolution components anyway...
				//
				double beamtilt_x = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_BEAMTILT_X);
				double beamtilt_y = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_BEAMTILT_Y);
				double Cs = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS);
				double V = 1000. * DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE);
				double lambda = 12.2643247 / sqrt(V * (1. + V * 0.978466e-6));
				if (ABS(beamtilt_x) > 0. || ABS(beamtilt_y) > 0.)
					selfApplyBeamTilt(Fimg, beamtilt_x, beamtilt_y, lambda, Cs, mymodel.pixel_size, mymodel.ori_size);

				exp_Fimgs_nomask.at(my_image_no) = Fimg;

				long int ori_part_id = exp_ipart_to_ori_part_id[ipart];

				MultidimArray<double> Mnoise;
				if (!do_zero_mask)
				{
					// Make a noisy background image with the same spectrum as the sigma2_noise

					// Different MPI-distributed subsets may otherwise have different instances of the random noise below,
					// because work is on an on-demand basis and therefore variable with the timing of distinct nodes...
					// Have the seed based on the ipart, so that each particle has a different instant of the noise
					// Do this all inside a mutex for the threads, because they all use the same static variables inside ran1...
					// (So the mutex only goal is to make things exactly reproducible with the same random_seed.)
					//global_mutex.lock();

					//init_random_generator(random_seed + ori_part_id);
					if (do_realign_movies)
						init_random_generator(random_seed + part_id);
					else
						init_random_generator(random_seed + ori_part_id);

					// If we're doing running averages, then the sigma2_noise was already adjusted for the running averages.
					// Undo this adjustment here in order to get the right noise in the individual frames
					MultidimArray<double> power_noise = sigma2_fudge * mymodel.sigma2_noise[group_id];
					if (do_realign_movies)
						power_noise *= (2. * movie_frame_running_avg_side + 1.);

					// Create noisy image for outside the mask
					MultidimArray<Complex > Fnoise;
					Mnoise.resize(img());
					transformer.setReal(Mnoise);
					transformer.getFourierAlias(Fnoise);
					// Fill Fnoise with random numbers, use power spectrum of the noise for its variance
					FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(Fnoise)
					{
						int ires = ROUND( sqrt( (double)(kp * kp + ip * ip + jp * jp) ) );
						if (ires >= 0 && ires < XSIZE(Fnoise))
						{
							double sigma = sqrt(DIRECT_A1D_ELEM(power_noise, ires));
							DIRECT_A3D_ELEM(Fnoise, k, i, j).real = rnd_gaus(0., sigma);
							DIRECT_A3D_ELEM(Fnoise, k, i, j).imag = rnd_gaus(0., sigma);
						}
						else
						{
							DIRECT_A3D_ELEM(Fnoise, k, i, j) = 0.;
						}
					}
					// Back to real space Mnoise
					transformer.inverseFourierTransform();
					Mnoise.setXmippOrigin();

					// unlock the mutex now that all calss to random functions have finished
					//global_mutex.unlock();

					softMaskOutsideMap(img(), particle_diameter / (2. * mymodel.pixel_size), (double)width_mask_edge, &Mnoise);

				}
				else
				{
					softMaskOutsideMap(img(), particle_diameter / (2. * mymodel.pixel_size), (double)width_mask_edge);
				}
				// Inside Projector and Backprojector the origin of the Fourier Transform is centered!
				CenterFFT(img(), true);

				// Store the Fourier Transform of the image Fimg
				transformer.FourierTransform(img(), Faux);

				// Store the power_class spectrum of the whole image (to fill sigma2_noise between current_size and ori_size
				if (mymodel.current_size < mymodel.ori_size)
				{
					MultidimArray<double> spectrum;
					spectrum.initZeros(mymodel.ori_size/2 + 1);
					double highres_Xi2 = 0.;
					FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(Faux)
					{
						int ires = ROUND( sqrt( (double)(kp*kp + ip*ip + jp*jp) ) );
						// Skip Hermitian pairs in the x==0 column

						if (ires > 0 && ires < mymodel.ori_size/2 + 1 && !(jp==0 && ip < 0) )
						{
							double normFaux = norm(DIRECT_A3D_ELEM(Faux, k, i, j));
							DIRECT_A1D_ELEM(spectrum, ires) += normFaux;
							// Store sumXi2 from current_size until ori_size
							if (ires >= mymodel.current_size/2 + 1)
								highres_Xi2 += normFaux;
						}
					}

					// Let's use .at() here instead of [] to check whether we go outside the vectors bounds
					exp_power_imgs.at(my_image_no) = spectrum;
					exp_highres_Xi2_imgs.at(my_image_no) = highres_Xi2;
				}
				else
				{
					exp_highres_Xi2_imgs.at(my_image_no) = 0.;
				}
				//std::cout << "exp_highres_Xi2_imgs: " <<  exp_highres_Xi2_imgs.at(my_image_no)  << "for my_image_no: " << my_image_no << std::endl; 
			
				// We never need any resolutions higher than current_size
				// So resize the Fourier transforms
				windowFourierTransform(Faux, Fimg, mymodel.current_size);

				// Also store its CTF
				Fctf.resize(Fimg);

				// Now calculate the actual CTF
				if (do_ctf_correction)
				{
					CTF ctf;
					ctf.setValues(DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_U),
							DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_V),
							DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_ANGLE),
							DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE),
							DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS),
							DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_Q0),
							DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_BFAC));

					ctf.getFftwImage(Fctf, mymodel.ori_size, mymodel.ori_size, mymodel.pixel_size,
							ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
				}
				else
				{
					Fctf.initConstant(1.);
				}

				// Store Fimg and Fctf
				exp_Fimgs.at(my_image_no) = Fimg;
				exp_Fctfs.at(my_image_no) = Fctf;

			} // end loop iseries
		}// end loop ipart
	//} // end while threadTaskDistributor

	// All threads clear out their transformer object when they are finished
	// This is to prevent a call from the first thread to fftw_cleanup, while there are still active plans in the transformer objects....
	// The multi-threaded code with FFTW objects is really a bit of a pain...
	//if (thread_id != 0)
	//{
	//	transformer.clear();
	//}

	// Wait until all threads have finished
	//global_barrier->wait();

	// Only the first thread cleans up the fftw-junk in the transformer object
	//if (thread_id == 0)
	//{
	transformer.cleanup();
	//}

}


/*

void MlOptimiser::doThreadGetFourierTransformsAndCtfs_gpu()
{
		
		exp_starting_image_no.clear();
		exp_power_imgs.clear();
		exp_highres_Xi2_imgs.clear();
		exp_Fimgs.clear();
		exp_Fimgs_nomask.clear();
		exp_Fctfs.clear();
		exp_old_offset.clear();
		exp_prior.clear();
		exp_local_oldcc.clear();
		exp_ipart_to_part_id.clear();
		exp_ipart_to_ori_part_id.clear();
		exp_ipart_to_ori_part_nframe.clear();
		exp_iimg_to_ipart.clear();
	
		// Resize to the right size instead of using pushbacks
		exp_starting_image_no.resize(exp_nr_particles);
	
		// First check how many images there are in the series for each particle...
		// And calculate exp_nr_images
		exp_nr_images = 0;
		for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	 	{
	
#ifdef DEBUG_CHECKSIZES
			if (ori_part_id >= mydata.ori_particles.size())
			{
				std::cerr<< "ori_part_id= "<<ori_part_id<<" mydata.ori_particles.size()= "<< mydata.ori_particles.size() <<std::endl;
				REPORT_ERROR("ori_part_id >= mydata.ori_particles.size()");
			}
#endif
			for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
			{
				long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
				int iipart = exp_ipart_to_part_id.size();
				exp_starting_image_no.at(iipart) = exp_nr_images;
				exp_nr_images += mydata.getNrImagesInSeries(part_id);
				exp_ipart_to_part_id.push_back(part_id);
				exp_ipart_to_ori_part_id.push_back(ori_part_id);
				exp_ipart_to_ori_part_nframe.push_back(i);
				for (int i = 0; i < mydata.getNrImagesInSeries(part_id); i++)
					exp_iimg_to_ipart.push_back(iipart);
			}
		}
		// Then also resize vectors for all images
		exp_power_imgs.resize(exp_nr_images);
		exp_highres_Xi2_imgs.resize(exp_nr_images);
		exp_Fimgs.resize(exp_nr_images);
		exp_Fimgs_nomask.resize(exp_nr_images);
		exp_Fctfs.resize(exp_nr_images);
		exp_old_offset.resize(exp_nr_images);
		exp_prior.resize(exp_nr_images);
		exp_local_oldcc.resize(exp_nr_images);
	
		
		FourierTransformer transformer;
		//size_t first_ipart, last_ipart;
		
		//first_ipart = exp_my_first_ori_particle;
		//last_ipart = exp_my_last_ori_particle;
		
		double *normcorr_host;
		double *beamtilt_x_host,*beamtilt_y_host;
		double *Cs_host,*lambda_host;
		//double *translated_local_Matrixs;
		double *exp_old_offset_host;
		
		normcorr_host = new double[exp_nr_images];
		beamtilt_x_host = new double[exp_nr_images];
		beamtilt_y_host = new double[exp_nr_images];
		Cs_host = new double[exp_nr_images];
		lambda_host = new double[exp_nr_images];
		exp_old_offset_host = new double[exp_nr_images*2];
		//translated_local_Matrixs = new double[exp_nr_images*3*3]; //The image is 2D 
		for (long int ipart = 0; ipart < exp_nr_images; ipart++)
		{
			int part_id = exp_ipart_to_part_id[ipart];
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				int group_id = mydata.getGroupId(part_id, iseries);
	
				// Get the norm_correction
				normcorr_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM);
	
				// Get the optimal origin offsets from the previous iteration
				Matrix1D<double> my_old_offset(2), my_prior(2);
				XX(my_old_offset) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF);
				YY(my_old_offset) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF);
				XX(my_prior)	  = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF_PRIOR);
				YY(my_prior)	  = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF_PRIOR);
					
				// Uninitialised priors were set to 999.
				if (XX(my_prior)   > 998.99 && XX(my_prior)   < 999.01)
					XX(my_prior)   = 0.;
				if (YY(my_prior) > 998.99 && YY(my_prior) < 999.01)
					YY(my_prior)= 0.;
	
				// Get the old cross-correlations
				exp_local_oldcc.at(my_image_no) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_DLL);
	
				// If we do local angular searches, get the previously assigned angles to center the prior
				// Only do this for the first image in the series, as this prior work per-particle, not per-image
				// All images in the series use the same rotational sampling, brought back to "exp_R_mic=identity"
				if (do_skip_align || do_skip_rotate)
				{
						// No need to block the threads global_mutex, as nr_pool will be set to 1 anyway for do_skip_align!
					if (do_skip_align)
					{
							// Rounded translations will be applied to the image upon reading,
							// set the unique translation in the sampling object to the fractional difference
						Matrix1D<double> rounded_offset = my_old_offset;
						rounded_offset.selfROUND();
						rounded_offset = my_old_offset - rounded_offset;
						sampling.setOneTranslation(rounded_offset);
					}
	
						// Also set the rotations
					double old_rot, old_tilt, old_psi;
					old_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT);
					old_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT);
					old_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI);
					sampling.setOneOrientation(old_rot, old_tilt, old_psi);
	
				}
				else if (mymodel.orientational_prior_mode != NOPRIOR && iseries == 0)
				{
					// First try if there are some fixed prior angles
					double prior_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT_PRIOR);
					double prior_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT_PRIOR);
					double prior_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI_PRIOR);
	
					// If there were no defined priors (i.e. their values were 999.), then use the "normal" angles
					if (prior_rot > 998.99 && prior_rot < 999.01)
						prior_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT);
					if (prior_tilt > 998.99 && prior_tilt < 999.01)
						prior_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT);
					if (prior_psi > 998.99 && prior_psi < 999.01)
						prior_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI);
	
					// For tilted series: convert the angles back onto the untilted ones...
					// Calculate the angles back from the Euler matrix because for tilt series exp_R_mic may have changed them...
					Matrix2D<double> A, R_mic(3,3);
					R_mic(0,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_0);
					R_mic(0,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_1);
					R_mic(0,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_2);
					R_mic(1,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_0);
					R_mic(1,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_1);
					R_mic(1,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_2);
					R_mic(2,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_0);
					R_mic(2,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_1);
					R_mic(2,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_2);
					if (!R_mic.isIdentity())
					{
						Euler_angles2matrix(prior_rot, prior_tilt, prior_psi, A);
						A = R_mic.inv() * A;
						Euler_matrix2angles(A, prior_rot, prior_tilt, prior_psi);
					}
	
					// Select only those orientations that have non-zero prior probability
					sampling.selectOrientationsWithNonZeroPriorProbability(prior_rot, prior_tilt, prior_psi,
							sqrt(mymodel.sigma2_rot), sqrt(mymodel.sigma2_tilt), sqrt(mymodel.sigma2_psi));
	
					long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
					if (nr_orients == 0)
					{
						std::cerr << " sampling.NrDirections()= " << sampling.NrDirections() << " sampling.NrPsiSamplings()= " << sampling.NrPsiSamplings() << std::endl;
						REPORT_ERROR("Zero orientations fall within the local angular search. Increase the sigma-value(s) on the orientations!");
					}
					//int threadBlockSize = (nr_orients > 100) ? 10 : 1;
				}
					
				my_old_offset.selfROUND();
				//selfTranslate(img(), my_old_offset, DONT_WRAP);
				//if (has_converged && do_use_reconstruct_images)
				//	selfTranslate(rec_img(), my_old_offset, DONT_WRAP);
				
				exp_old_offset.at(my_image_no) = my_old_offset;
				// Also store priors on translations
				exp_prior.at(my_image_no) = my_prior;
				//Only for copy the offset of each image to GPU by using a large array
				exp_old_offset_host[my_image_no] = XX(my_old_offset);
				exp_old_offset_host[my_image_no+1] = YY(my_old_offset);
	
				beamtilt_x_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_BEAMTILT_X);
				beamtilt_y_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_BEAMTILT_Y);
				Cs_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS);
				double V = 1000. * DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE);
				lambda_host[my_image_no] = 12.2643247 / sqrt(V * (1. + V * 0.978466e-6));
					
			}
		}
		
		std::vector< Image<double> > local_images,local_rec_images;
		std::vector< MultidimArray<Complex> > local_Fimgs, local_Faux;
		std::vector< MultidimArray<double> > local_Fctf_images,local_img_aux;
	
		local_images.clear();
		local_rec_images.clear();
		local_Fimgs.clear();
		local_Faux.clear();
		local_Fctf_images.clear();
		local_img_aux.clear();
	
		local_images.resize(exp_nr_images);
		local_rec_images.resize(exp_nr_images);
		local_Fimgs.resize(exp_nr_images);
		local_Faux.resize(exp_nr_images);
		local_Fctf_images.resize(exp_nr_images);
		local_img_aux.resize(exp_nr_images);
			
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			// Prevent movies and series at the same time...
			if (mydata.getNrImagesInSeries(part_id) > 1 && do_realign_movies)
				REPORT_ERROR("Not ready yet for dealing with image series at the same time as realigning movie frames....");
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				int group_id = mydata.getGroupId(part_id, iseries);
	
				// Get the optimal origin offsets from the previous iteration
				// Unpack the image from the imagedata
				local_images[my_image_no]().resize(mymodel.ori_size, mymodel.ori_size);
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[my_image_no]())
				{
					DIRECT_A2D_ELEM(local_images[my_image_no](), i, j) = DIRECT_A3D_ELEM(exp_imagedata, my_image_no, i, j);
				}
				local_images[my_image_no]().setXmippOrigin();
				if (has_converged && do_use_reconstruct_images)
				{
					local_rec_images[my_image_no]().resize(mymodel.ori_size, mymodel.ori_size);
					FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_rec_images[my_image_no]())
					{
						DIRECT_A2D_ELEM(local_rec_images[my_image_no](), i, j) = DIRECT_A3D_ELEM(exp_imagedata, exp_nr_images + my_image_no, i, j);
					}
					local_rec_images[my_image_no]().setXmippOrigin();
				}
			}
		}

		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			// Prevent movies and series at the same time...
			if (mydata.getNrImagesInSeries(part_id) > 1 && do_realign_movies)
				REPORT_ERROR("Not ready yet for dealing with image series at the same time as realigning movie frames....");
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{

				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				//int group_id = mydata.getGroupId(part_id, iseries);
				// Apply the norm_correction term
				if (do_norm_correction)
				{
					local_images[my_image_no]() *= mymodel.avg_norm_correction / normcorr_host[my_image_no];
				}
	
				// Apply (rounded) old offsets first
				selfTranslate(local_images[my_image_no](), exp_old_offset.at(my_image_no), DONT_WRAP);
				if (has_converged && do_use_reconstruct_images)
					selfTranslate(local_rec_images[my_image_no](), exp_old_offset.at(my_image_no), DONT_WRAP);
			}
		}

		//The GPU codes correspond to the above CXX codes, copy images from exp_imagedata
		long int size_ = 0 ; 
		int image_size;
		image_size= mymodel.ori_size*mymodel.ori_size;
		size_ = exp_nr_images*image_size;
		//TODO: Apply the norm_correction term
		
		double *beamtilt_x_dev,*beamtilt_y_dev;
		double *Cs_dev,*lambda_dev;
		
		cudaMalloc((void**)&image_dev,size_*sizeof(double));
			
		cudaMalloc((void**)&rec_image_dev,size_*sizeof(double));
		cudaMalloc((void**)&translated_matrix_dev,exp_nr_images*9*sizeof(double));

		cudaMalloc((void**)&beamtilt_x_dev,exp_nr_images*sizeof(double));
		cudaMalloc((void**)&beamtilt_y_dev,exp_nr_images*sizeof(double));
		cudaMalloc((void**)&Cs_dev,exp_nr_images*sizeof(double));
		cudaMalloc((void**)&lambda_dev,exp_nr_images*sizeof(double));

			
		cudaMemcpy(translated_matrix_dev, exp_old_offset_host, exp_nr_images*2*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(beamtilt_x_dev, beamtilt_x_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(beamtilt_y_dev, beamtilt_y_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(Cs_dev, Cs_host,exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(lambda_dev, lambda_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
	
		double *raw_data_images, *raw_rec_images;
		raw_data_images = new double[size_];
		if (has_converged && do_use_reconstruct_images)
			raw_rec_images = new double[size_];
		
		
		for(int k = 0; k < exp_nr_images; k++)
		{
			memcpy(raw_data_images+k*image_size, local_images[k]().data, image_size*sizeof(double) );
			if (has_converged && do_use_reconstruct_images)
				memcpy(raw_rec_images+k*image_size, local_rec_images[k]().data, image_size*sizeof(double) );
		}
		
		//for(int k = 0; k < exp_nr_images; k++)
		//{
		//	FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[k]())
		//	{
		//		raw_data_images[k*image_size+i*mymodel.ori_size+j]=DIRECT_A2D_ELEM(local_images[k](), i, j);
		//		if (has_converged && do_use_reconstruct_images)
		//			raw_rec_images[k*image_size+i*mymodel.ori_size+j]=DIRECT_A2D_ELEM(local_rec_images[k](), i, j);
		//	}
		//}
		cudaMemcpy(image_dev, raw_data_images, size_*sizeof(double), cudaMemcpyHostToDevice);
		if(has_converged && do_use_reconstruct_images)
			cudaMemcpy(rec_image_dev, raw_rec_images, size_*sizeof(double), cudaMemcpyHostToDevice);

		int dim,xdim, ydim,zdim;
		xdim = mymodel.ori_size;
		ydim = mymodel.ori_size;
		zdim  = 1;
		dim = 2 ;

		//Do the CenterFFT for some partiles with GPU 
		double *local_images_dev;
		cudaMalloc((void**)&local_images_dev,size_*sizeof(double));
		cudaMemset(local_images_dev, 0,size_);
		//cudaMemcpy(local_img_dev, ((has_converged && do_use_reconstruct_images) ?rec_image_dev :image_dev ), size_*sizeof(double), cudaMemcpyDeviceToDevice);
		centerFFT_gpu(((has_converged && do_use_reconstruct_images) ?rec_image_dev :image_dev ) , local_images_dev, exp_nr_images, dim, xdim,  ydim, zdim, true);

		//Foutier Transform for some particles on GPU
		cufftDoubleComplex *local_Faux_dev;
		
		cudaMalloc((void **)&local_Faux_dev, exp_nr_images*zdim*ydim*(xdim/2+1)*2*sizeof(double));
		transformer.FourierTransform_gpu(local_images_dev, local_Faux_dev, exp_nr_images, xdim,  ydim, zdim);

		//WindowFourierTransform for some particles on GPU 
		int ndim  = (zdim>1) ? 3:((ydim>1) ? 2 : 1);
		int newhdim = mymodel.current_size/2 +1;
		int newdim = mymodel.current_size;
		int mem_size;
		if(ndim == 1)
		{
			 mem_size = newhdim*2*sizeof(double);
		}
		else if(ndim == 2)
		{
			mem_size = newdim*newhdim*2*sizeof(double);
		}
		else if(ndim == 3)
		{
			mem_size = newdim*newdim*newhdim*2*sizeof(double);
		}

		cudaMalloc((void **)&exp_Fimgs_nomask_dev, exp_nr_images*mem_size);
			//exp_Fimgs_nomask_dev =NULL;
		windowFourierTransform_gpu(local_Faux_dev,
								 exp_Fimgs_nomask_dev,
								 mymodel.current_size,
								 exp_nr_images,
 								 2,
 								 (xdim/2)+1,
 								 ydim,
 								 zdim);


		selfApplyBeamTilt_gpu(exp_Fimgs_nomask_dev, beamtilt_x_dev, beamtilt_y_dev,
				lambda_dev, Cs_dev, mymodel.pixel_size, mymodel.ori_size, exp_nr_images,  2, newhdim,  newdim);

		double *Mnoise_dev;
			Mnoise_dev = NULL;
		softMaskOutsideMap_gpu(image_dev, particle_diameter / (2. * mymodel.pixel_size), (double)width_mask_edge, Mnoise_dev, exp_nr_images, xdim, ydim,zdim);

		centerFFT_gpu(image_dev , local_images_dev, exp_nr_images, dim, xdim,  ydim, zdim, true);

		transformer.FourierTransform_gpu(local_images_dev, local_Faux_dev, exp_nr_images, xdim,  ydim, zdim);

		// Store the power_class spectrum of the whole image (to fill sigma2_noise between current_size and ori_size

			double *local_Faux_host;
			int size_fft = exp_nr_images*(xdim/2+1)*ydim* zdim*2;
			local_Faux_host = new double[size_fft];
			cudaMemcpy(local_Faux_host,local_Faux_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
			for(int i = 0; i < exp_nr_images; i++)
			{
				local_Faux[i].resize(zdim,ydim,(xdim/2+1));
				memcpy(local_Faux[i].data, local_Faux_host+i*(xdim/2+1)*ydim* zdim*2, (xdim/2+1)*ydim* zdim*2*sizeof(double));
			}
			delete [] local_Faux_host;
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					if (mymodel.current_size < mymodel.ori_size)
					{
						MultidimArray<double> spectrum;
						spectrum.initZeros(mymodel.ori_size/2 + 1);
						double highres_Xi2 = 0.;
						FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Faux[my_image_no])
						{
							int ires = ROUND( sqrt( (double)(kp*kp + ip*ip + jp*jp) ) );
							// Skip Hermitian pairs in the x==0 column
	
							if (ires > 0 && ires < mymodel.ori_size/2 + 1 && !(jp==0 && ip < 0) )
							{
								double normFaux = norm(DIRECT_A3D_ELEM(local_Faux[my_image_no], k, i, j));
								DIRECT_A1D_ELEM(spectrum, ires) += normFaux;
								// Store sumXi2 from current_size until ori_size
								if (ires >= mymodel.current_size/2 + 1)
									highres_Xi2 += normFaux;
							}
						}
	
						// Let's use .at() here instead of [] to check whether we go outside the vectors bounds
						exp_power_imgs.at(my_image_no) = spectrum;
						exp_highres_Xi2_imgs.at(my_image_no) = highres_Xi2;
					}
					else
					{
						exp_highres_Xi2_imgs.at(my_image_no) = 0.;
					}
					std::cout << "exp_highres_Xi2_imgs: " <<  exp_highres_Xi2_imgs.at(my_image_no)  << "for my_image_no: " << my_image_no << std::endl; 
			
				}
			}
	
			cudaMalloc((void **)&exp_Fimgs_dev, exp_nr_images*mem_size);
 			windowFourierTransform_gpu(local_Faux_dev,
								 exp_Fimgs_dev,
								 mymodel.current_size,
								 exp_nr_images,
 								 2,
 								 (xdim/2)+1,
 								 ydim,
								 zdim);

			double *local_Faux_no_maskhost;
			int Fourier_image_size = newdim*newhdim;
			
			size_fft = exp_nr_images*Fourier_image_size*2;
			local_Faux_host = new double[size_fft];
			local_Faux_no_maskhost = new double[size_fft];
			cudaMemcpy(local_Faux_host,exp_Fimgs_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
			cudaMemcpy(local_Faux_no_maskhost,exp_Fimgs_nomask_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
			
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					// Also store its CTF
					//local_Fctf_images[my_image_no].resize(local_Fimgs[my_image_no]);
					local_Fctf_images[my_image_no].resize(newdim, newhdim);
			
					// Now calculate the actual CTF
					if (do_ctf_correction)
					{
						CTF ctf;
						ctf.setValues(DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_U),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_V),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_ANGLE),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_Q0),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_BFAC));
		
						ctf.getFftwImage(local_Fctf_images[my_image_no], mymodel.ori_size, mymodel.ori_size, mymodel.pixel_size,
								ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
		//#define DEBUG_CTF_FFTW_IMAGE
#ifdef DEBUG_CTF_FFTW_IMAGE
						Image<double> tt;
						tt()=local_Fctf_images[my_image_no];
						tt.write("relion_ctf.spi");
						std::cerr << "Written relion_ctf.spi, now exiting..." << std::endl;
						exit(1);
#endif
		//#define DEBUG_GETCTF
#ifdef DEBUG_GETCTF
						std::cerr << " intact_ctf_first_peak= " << intact_ctf_first_peak << std::endl;
						ctf.write(std::cerr);
						Image<double> tmp;
						tmp()=local_Fctf_images[my_image_no];
						tmp.write("Fctf.spi");
						tmp().resize(mymodel.ori_size, mymodel.ori_size);
						ctf.getCenteredImage(tmp(), mymodel.pixel_size, ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
						tmp.write("Fctf_cen.spi");
						std::cerr << "Written Fctf.spi, Fctf_cen.spi. Press any key to continue..." << std::endl;
						char c;
						std::cin >> c;
#endif
					}
					else
					{
						local_Fctf_images[my_image_no].initConstant(1.);
					}
		
					// Store Fimg and Fctf

					exp_Fimgs[my_image_no].resize(1,newdim, newhdim);
					exp_Fimgs_nomask[my_image_no].resize(1,newdim, newhdim);
					memcpy(exp_Fimgs[my_image_no].data, local_Faux_host+my_image_no*newhdim*newdim*2, newhdim*newdim*2*sizeof(double));
					//exp_Fimgs.at(my_image_no) = local_Fimgs[my_image_no];
					
					memcpy(exp_Fimgs_nomask[my_image_no].data, local_Faux_no_maskhost+my_image_no*newhdim*newdim*2, newhdim*newdim*2*sizeof(double));
					//exp_Fimgs_nomask.at(my_image_no) = local_Fimgs[my_image_no];
					
					exp_Fctfs.at(my_image_no) = local_Fctf_images[my_image_no];
		
			} // end loop iseries
		}// end loop ipart
		
		transformer.free_memory_gpu();
		transformer.cleanup();

		cudaFree(translated_matrix_dev);
		cudaFree(beamtilt_x_dev);
		cudaFree(beamtilt_y_dev);
		cudaFree(Cs_dev);
		cudaFree(lambda_dev);
		cudaFree(image_dev);
		cudaFree(rec_image_dev);
		cudaFree(local_images_dev);
		cudaFree(local_Faux_dev);
		//cudaFree(local_Fimgs_dev);

		
		delete [] local_Faux_host;
		delete [] normcorr_host;
		delete [] beamtilt_x_host;
		delete [] beamtilt_y_host;
		delete [] Cs_host;
		delete [] lambda_host;
		delete []exp_old_offset_host;
		delete [] raw_data_images;
		if (has_converged && do_use_reconstruct_images)
			delete []raw_rec_images;
	
}

*/
void MlOptimiser::doThreadGetFourierTransformsAndCtfs_gpu()
{
		
		exp_starting_image_no.clear();
		exp_power_imgs.clear();
		exp_highres_Xi2_imgs.clear();
		exp_Fimgs.clear();
		exp_Fimgs_nomask.clear();
		exp_Fctfs.clear();
		exp_old_offset.clear();
		exp_prior.clear();
		exp_local_oldcc.clear();
		exp_ipart_to_part_id.clear();
		exp_ipart_to_ori_part_id.clear();
		exp_ipart_to_ori_part_nframe.clear();
		exp_iimg_to_ipart.clear();
	
		// Resize to the right size instead of using pushbacks
		exp_starting_image_no.resize(exp_nr_particles);
	
		// First check how many images there are in the series for each particle...
		// And calculate exp_nr_images
		exp_nr_images = 0;
		for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	 	{
	
#ifdef DEBUG_CHECKSIZES
			if (ori_part_id >= mydata.ori_particles.size())
			{
				std::cerr<< "ori_part_id= "<<ori_part_id<<" mydata.ori_particles.size()= "<< mydata.ori_particles.size() <<std::endl;
				REPORT_ERROR("ori_part_id >= mydata.ori_particles.size()");
			}
#endif
			for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
			{
				long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
				int iipart = exp_ipart_to_part_id.size();
				exp_starting_image_no.at(iipart) = exp_nr_images;
				exp_nr_images += mydata.getNrImagesInSeries(part_id);
				exp_ipart_to_part_id.push_back(part_id);
				exp_ipart_to_ori_part_id.push_back(ori_part_id);
				exp_ipart_to_ori_part_nframe.push_back(i);
				for (int i = 0; i < mydata.getNrImagesInSeries(part_id); i++)
					exp_iimg_to_ipart.push_back(iipart);
			}
		}
		// Then also resize vectors for all images
		exp_power_imgs.resize(exp_nr_images);
		exp_highres_Xi2_imgs.resize(exp_nr_images);
		exp_Fimgs.resize(exp_nr_images);
		exp_Fimgs_nomask.resize(exp_nr_images);
		exp_Fctfs.resize(exp_nr_images);
		exp_old_offset.resize(exp_nr_images);
		exp_prior.resize(exp_nr_images);
		exp_local_oldcc.resize(exp_nr_images);
	
		
		FourierTransformer transformer;
		size_t first_ipart, last_ipart;
		
		first_ipart = exp_my_first_ori_particle;
		last_ipart = exp_my_last_ori_particle;
		
		double *normcorr_host;
		double *beamtilt_x_host,*beamtilt_y_host;
		double *Cs_host,V_host,*lambda_host;
		//double *translated_local_Matrixs;
		double *exp_old_offset_host;
		
		normcorr_host = new double[exp_nr_images];
		beamtilt_x_host = new double[exp_nr_images];
		beamtilt_y_host = new double[exp_nr_images];
		Cs_host = new double[exp_nr_images];
		lambda_host = new double[exp_nr_images];
		exp_old_offset_host = new double[exp_nr_images*2];
		//translated_local_Matrixs = new double[exp_nr_images*3*3]; //The image is 2D 
		for (long int ipart = 0; ipart < exp_nr_images; ipart++)
		{
			int part_id = exp_ipart_to_part_id[ipart];
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				int group_id = mydata.getGroupId(part_id, iseries);
	
				// Get the norm_correction
				normcorr_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM);
	
				// Get the optimal origin offsets from the previous iteration
				Matrix1D<double> my_old_offset(2), my_prior(2);
				XX(my_old_offset) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF);
				YY(my_old_offset) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF);
				XX(my_prior)	  = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF_PRIOR);
				YY(my_prior)	  = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF_PRIOR);
					
				// Uninitialised priors were set to 999.
				if (XX(my_prior)   > 998.99 && XX(my_prior)   < 999.01)
					XX(my_prior)   = 0.;
				if (YY(my_prior) > 998.99 && YY(my_prior) < 999.01)
					YY(my_prior)= 0.;
	
				// Get the old cross-correlations
				exp_local_oldcc.at(my_image_no) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_DLL);
	
				// If we do local angular searches, get the previously assigned angles to center the prior
				// Only do this for the first image in the series, as this prior work per-particle, not per-image
				// All images in the series use the same rotational sampling, brought back to "exp_R_mic=identity"
				if (do_skip_align || do_skip_rotate)
				{
						// No need to block the threads global_mutex, as nr_pool will be set to 1 anyway for do_skip_align!
					if (do_skip_align)
					{
							// Rounded translations will be applied to the image upon reading,
							// set the unique translation in the sampling object to the fractional difference
						Matrix1D<double> rounded_offset = my_old_offset;
						rounded_offset.selfROUND();
						rounded_offset = my_old_offset - rounded_offset;
						sampling.setOneTranslation(rounded_offset);
					}
	
						// Also set the rotations
					double old_rot, old_tilt, old_psi;
					old_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT);
					old_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT);
					old_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI);
					sampling.setOneOrientation(old_rot, old_tilt, old_psi);
	
				}
				else if (mymodel.orientational_prior_mode != NOPRIOR && iseries == 0)
				{
					// First try if there are some fixed prior angles
					double prior_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT_PRIOR);
					double prior_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT_PRIOR);
					double prior_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI_PRIOR);
	
					// If there were no defined priors (i.e. their values were 999.), then use the "normal" angles
					if (prior_rot > 998.99 && prior_rot < 999.01)
						prior_rot = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT);
					if (prior_tilt > 998.99 && prior_tilt < 999.01)
						prior_tilt = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT);
					if (prior_psi > 998.99 && prior_psi < 999.01)
						prior_psi = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI);
	
					// For tilted series: convert the angles back onto the untilted ones...
					// Calculate the angles back from the Euler matrix because for tilt series exp_R_mic may have changed them...
					Matrix2D<double> A, R_mic(3,3);
					R_mic(0,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_0);
					R_mic(0,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_1);
					R_mic(0,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_2);
					R_mic(1,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_0);
					R_mic(1,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_1);
					R_mic(1,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_2);
					R_mic(2,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_0);
					R_mic(2,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_1);
					R_mic(2,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_2);
					if (!R_mic.isIdentity())
					{
						Euler_angles2matrix(prior_rot, prior_tilt, prior_psi, A);
						A = R_mic.inv() * A;
						Euler_matrix2angles(A, prior_rot, prior_tilt, prior_psi);
					}
	
					// Select only those orientations that have non-zero prior probability
					sampling.selectOrientationsWithNonZeroPriorProbability(prior_rot, prior_tilt, prior_psi,
							sqrt(mymodel.sigma2_rot), sqrt(mymodel.sigma2_tilt), sqrt(mymodel.sigma2_psi));
	
					long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
					if (nr_orients == 0)
					{
						std::cerr << " sampling.NrDirections()= " << sampling.NrDirections() << " sampling.NrPsiSamplings()= " << sampling.NrPsiSamplings() << std::endl;
						REPORT_ERROR("Zero orientations fall within the local angular search. Increase the sigma-value(s) on the orientations!");
					}
					//int threadBlockSize = (nr_orients > 100) ? 10 : 1;
				}
					
				my_old_offset.selfROUND();
				//selfTranslate(img(), my_old_offset, DONT_WRAP);
				//if (has_converged && do_use_reconstruct_images)
				//	selfTranslate(rec_img(), my_old_offset, DONT_WRAP);
				
				exp_old_offset.at(my_image_no) = my_old_offset;
				// Also store priors on translations
				exp_prior.at(my_image_no) = my_prior;
				//Only for copy the offset of each image to GPU by using a large array
				exp_old_offset_host[my_image_no*2] = XX(my_old_offset);
				exp_old_offset_host[my_image_no*2+1] = YY(my_old_offset);
	
				beamtilt_x_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_BEAMTILT_X);
				beamtilt_y_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_BEAMTILT_Y);
				Cs_host[my_image_no] = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS);
				double V = 1000. * DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE);
				lambda_host[my_image_no] = 12.2643247 / sqrt(V * (1. + V * 0.978466e-6));
					
			}
		}
		
		std::vector< Image<double> > local_images,local_rec_images;
		std::vector< MultidimArray<Complex> > local_Fimgs, local_Faux;
		std::vector< MultidimArray<double> > local_Fctf_images,local_img_aux;
	
		local_images.clear();
		local_rec_images.clear();
		local_Fimgs.clear();
		local_Faux.clear();
		local_Fctf_images.clear();
		local_img_aux.clear();
	
		local_images.resize(exp_nr_images);
		local_rec_images.resize(exp_nr_images);
		local_Fimgs.resize(exp_nr_images);
		local_Faux.resize(exp_nr_images);
		local_Fctf_images.resize(exp_nr_images);
		local_img_aux.resize(exp_nr_images);
			
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			// Prevent movies and series at the same time...
			if (mydata.getNrImagesInSeries(part_id) > 1 && do_realign_movies)
				REPORT_ERROR("Not ready yet for dealing with image series at the same time as realigning movie frames....");
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				int group_id = mydata.getGroupId(part_id, iseries);
	
				// Get the optimal origin offsets from the previous iteration
				// Unpack the image from the imagedata
				local_images[my_image_no]().resize(mymodel.ori_size, mymodel.ori_size);
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[my_image_no]())
				{
					DIRECT_A2D_ELEM(local_images[my_image_no](), i, j) = DIRECT_A3D_ELEM(exp_imagedata, my_image_no, i, j);
				}
				local_images[my_image_no]().setXmippOrigin();
				if (has_converged && do_use_reconstruct_images)
				{
					local_rec_images[my_image_no]().resize(mymodel.ori_size, mymodel.ori_size);
					FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_rec_images[my_image_no]())
					{
						DIRECT_A2D_ELEM(local_rec_images[my_image_no](), i, j) = DIRECT_A3D_ELEM(exp_imagedata, exp_nr_images + my_image_no, i, j);
					}
					local_rec_images[my_image_no]().setXmippOrigin();
				}
			}
		}

		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			// Prevent movies and series at the same time...
			if (mydata.getNrImagesInSeries(part_id) > 1 && do_realign_movies)
				REPORT_ERROR("Not ready yet for dealing with image series at the same time as realigning movie frames....");
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
	
				//FileName fn_img;
				//Image<double> img, rec_img;
				//MultidimArray<Complex > Fimg, Faux;
				//MultidimArray<double> Fctf;
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				// Which group do I belong?
				//int group_id = mydata.getGroupId(part_id, iseries);
				// Apply the norm_correction term
				if (do_norm_correction)
				{
					local_images[my_image_no]() *= mymodel.avg_norm_correction / normcorr_host[my_image_no];
				}
	
				// Apply (rounded) old offsets first
				//my_old_offset.selfROUND();
				selfTranslate(local_images[my_image_no](), exp_old_offset.at(my_image_no), DONT_WRAP);
				if (has_converged && do_use_reconstruct_images)
					selfTranslate(local_rec_images[my_image_no](), exp_old_offset.at(my_image_no), DONT_WRAP);
			}
		}

		//The GPU codes correspond to the above CXX codes, copy images from exp_imagedata
		long int size_ = 0 ; 
		//int exp_nr_images_done = 0;
		//int offset_rec_iamge;
		int image_size;
		image_size= mymodel.ori_size*mymodel.ori_size;
		size_ = exp_nr_images*image_size;
		//TODO: Apply the norm_correction term
		
		double *beamtilt_x_dev,*beamtilt_y_dev;
		double *Cs_dev,*lambda_dev;
		
		cudaMalloc((void**)&image_dev,size_*sizeof(double));
			
		cudaMalloc((void**)&rec_image_dev,size_*sizeof(double));
		cudaMalloc((void**)&translated_matrix_dev,exp_nr_images*9*sizeof(double));

		cudaMalloc((void**)&beamtilt_x_dev,exp_nr_images*sizeof(double));
		cudaMalloc((void**)&beamtilt_y_dev,exp_nr_images*sizeof(double));
		cudaMalloc((void**)&Cs_dev,exp_nr_images*sizeof(double));
		cudaMalloc((void**)&lambda_dev,exp_nr_images*sizeof(double));

			
		cudaMemcpy(translated_matrix_dev, exp_old_offset_host, exp_nr_images*2*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(beamtilt_x_dev, beamtilt_x_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(beamtilt_y_dev, beamtilt_y_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(Cs_dev, Cs_host,exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(lambda_dev, lambda_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);
	
		double *raw_data_images, *raw_rec_images;
		raw_data_images = new double[size_];
		if (has_converged && do_use_reconstruct_images)
			raw_rec_images = new double[size_];
		
		for(int k = 0; k < exp_nr_images; k++)
		{
			FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[k]())
			{
				raw_data_images[k*image_size+i*mymodel.ori_size+j]=DIRECT_A2D_ELEM(local_images[k](), i, j);
				if (has_converged && do_use_reconstruct_images)
					raw_rec_images[k*image_size+i*mymodel.ori_size+j]=DIRECT_A2D_ELEM(local_rec_images[k](), i, j);
			}
		}
		cudaMemcpy(image_dev, raw_data_images, size_*sizeof(double), cudaMemcpyHostToDevice);
		//delete [] raw_data_images;
		if(has_converged && do_use_reconstruct_images)
			cudaMemcpy(rec_image_dev, raw_rec_images, size_*sizeof(double), cudaMemcpyHostToDevice);

		//TODO: GPU function for mulit images from multi particles: scaling and selfTranslate

		//TODO: setting the XmippOrigin, pass them to the kernels
		//int xstart, ystart, zstart;
		//xstart = FIRST_XMIPP_INDEX(mymodel.ori_size); //The x dimention of the image
		//ystart = FIRST_XMIPP_INDEX(mymodel.ori_size); //The y dimention of the image
		//zstart = FIRST_XMIPP_INDEX(1);
		int dim,xdim, ydim,zdim;
		
		xdim = mymodel.ori_size;
		ydim = mymodel.ori_size;
		zdim  = 1;
		dim = 2 ;
		//\B8\F8??\B7\F9?\BD\F8\D0\D0?\B4\EA\C1\BF\C0\A9?\A3\AC\B5\AB\CA\C7??\B8\B1?\B5?\EA\C1\BF\C0\A9??\CA\FD\B2\BB?\B6\A8\CF\E0\B5\C8
		//if(do_norm_correction)
			//relion_gpu_scal(exp_nr_images, normcorr_host, image_dev, iamge_size);
		//Do the CenterFFT for some partiles with GPU 
		double *local_images_dev;
		cudaMalloc((void**)&local_images_dev,size_*sizeof(double));
		cudaMemset(local_images_dev, 0,size_);
		//cudaMemcpy(local_img_dev, ((has_converged && do_use_reconstruct_images) ?rec_image_dev :image_dev ), size_*sizeof(double), cudaMemcpyDeviceToDevice);
		centerFFT_gpu(image_dev , local_images_dev, exp_nr_images, dim, xdim,  ydim, zdim, true);

//#define DEBUG_CenterFFT
#ifdef DEBUG_CenterFFT
			  double *local_images_host;
			  local_images_host = new double[size_];
			  cudaMemcpy(local_images_host,image_dev,size_*sizeof(double), cudaMemcpyDeviceToHost);

			  for(int k = 0; k < exp_nr_images; k++)
			{
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[k]())
				{
					if(local_images_host[k*image_size+i*mymodel.ori_size+j]!=DIRECT_A2D_ELEM(local_images[k](), i, j))
					{
						std::cout << "The Raw image value for GPU and CPU is " <<local_images_host[k*image_size+i*mymodel.ori_size+j]<< "  "<< DIRECT_A2D_ELEM(local_images[k](), i, j)<< "  "<<local_images[k]().data[i*mymodel.ori_size+j] << std::endl;
						std::cerr << "The iamge data is not the same for CenterFFT between CPU and GPU " <<k << "  "<< i << "  "<< j<<std::endl;
					}
				}
			}
			delete [] local_images_host;
#endif
		// Always store FT of image without mask (to be used for the reconstruction)
		//Do the CenterFFT for some particles
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				MultidimArray<double> img_aux;
				local_img_aux[my_image_no] = (has_converged && do_use_reconstruct_images) ? local_rec_images[my_image_no]() : local_images[my_image_no]();
				CenterFFT(local_img_aux[my_image_no], true);
	
			}
		}
		
//#define COMPARE_GPU_CPU
#ifdef DEBUG_CenterFFT
			 // double *local_images_host;
			  local_images_host = new double[size_];
			  cudaMemcpy(local_images_host,local_images_dev,size_*sizeof(double), cudaMemcpyDeviceToHost);
			  for(int i =0 ;i < exp_nr_images; i ++)
			  	for(int j =0 ;j < (size_/exp_nr_images); j++)
			  	{
					if(local_images_host[i*image_size+j]!=local_img_aux[i].data[j])
					{
						std::cout << "The value for GPU and CPU is " <<local_images_host[i*image_size+j] <<local_img_aux[i].data[j] << std::endl;
						std::cerr << "The iamge data is not the same for CenterFFT between CPU and GPU" << i << j<<std::endl;
					}
			  	}
			  delete [] local_images_host;
#endif

		//Foutier Transform for some particles on GPU
		cufftDoubleComplex *local_Faux_dev;
		
		cudaMalloc((void **)&local_Faux_dev, exp_nr_images*zdim*ydim*(xdim/2+1)*sizeof(cufftDoubleComplex));
		transformer.FourierTransform_gpu(local_images_dev, local_Faux_dev, exp_nr_images, xdim,  ydim, zdim);
    
		//Fourier Transform for some particles
		int nr_images =0;
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				transformer.FourierTransform(local_img_aux[my_image_no], local_Faux[my_image_no]);
				nr_images++;
			}
		}
		
	double *local_Faux_host;
	int size_fft;
	int Fourier_image_size; 
	int yxdim_four; 
	int xdim_four ;
//#define DEBUG_FFT
#ifdef DEBUG_FFT
	//double *local_Faux_host;
	//int size_fft;
	 Fourier_image_size = local_Faux[0].zyxdim;
	 yxdim_four  = local_Faux[0].yxdim;
	 xdim_four = local_Faux[0].xdim;
	size_fft = nr_images*Fourier_image_size*2;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,local_Faux_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Faux[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Faux[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The FFT  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for Transform Fourier between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif

		//windowFourierTransform for some particles
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				windowFourierTransform(local_Faux[my_image_no] , local_Fimgs[my_image_no], mymodel.current_size);
			}
		}
		//WindowFourierTransform for some particles on GPU 
		//cufftDoubleComplex *local_Fimgs_dev;
		int ndim  = (local_Fimgs[0].zdim>1) ? 3:((local_Fimgs[0].ydim>1) ? 2 : 1);
		int newhdim = mymodel.current_size/2 +1;
		int newdim = mymodel.current_size;
		int mem_size;
		if(ndim == 1)
		{
			 mem_size = newhdim*2*sizeof(double);
		}
		else if(ndim == 2)
		{
			mem_size = newdim*newhdim*2*sizeof(double);
		}
		else if(ndim == 3)
		{
			mem_size = newdim*newdim*newhdim*2*sizeof(double);
		}
		//cudaMalloc((void **)&local_Fimgs_dev, nr_images*mem_size); 
		//cudaFree(exp_Fimgs_nomask_dev);
		cudaMalloc((void **)&exp_Fimgs_nomask_dev, nr_images*mem_size);
			//exp_Fimgs_nomask_dev =NULL;
		windowFourierTransform_gpu(local_Faux_dev,
								 exp_Fimgs_nomask_dev,
								 mymodel.current_size,
								 nr_images,
 								 2,
 								 (xdim/2)+1,
 								 ydim,
 								 zdim);
 		
//#define DEBUG_WINDOWFFT
#ifdef DEBUG_WINDOWFFT
	//double *local_Faux_host;
	//int size_fft;
	 Fourier_image_size = local_Fimgs[0].zyxdim;
	 yxdim_four  = local_Fimgs[0].yxdim;
	 xdim_four = local_Fimgs[0].xdim;
	size_fft = nr_images*Fourier_image_size*2;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,exp_Fimgs_nomask_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Fimgs[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Fimgs[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The windowFourierTransform_gpu  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for windowFourierTransform_gpu Fourier between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif

		//selfApplyBeam on GPU 
		//selfApplyBeamTilt_gpu(exp_Fimgs_nomask_dev, beamtilt_x_dev, beamtilt_y_dev,
		//		lambda_dev, Cs_dev, mymodel.pixel_size, mymodel.ori_size, nr_images,  2,  xdim,  ydim);
		
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				int group_id = mydata.getGroupId(part_id, iseries);
				if (ABS(beamtilt_x_host[my_image_no]) > 0. || ABS(beamtilt_y_host[my_image_no]) > 0.)
					selfApplyBeamTilt(local_Fimgs[my_image_no] , beamtilt_x_host[my_image_no], beamtilt_y_host[my_image_no], lambda_host[my_image_no], Cs_host[my_image_no], mymodel.pixel_size, mymodel.ori_size);

				exp_Fimgs_nomask.at(my_image_no) = local_Fimgs[my_image_no];
			}
		
		}

		selfApplyBeamTilt_gpu(exp_Fimgs_nomask_dev, beamtilt_x_dev, beamtilt_y_dev,
				lambda_dev, Cs_dev, mymodel.pixel_size, mymodel.ori_size, nr_images,  2,  local_Fimgs[0].xdim,  local_Fimgs[0].ydim);
		//cudaMemcpy(exp_Fimgs_nomask_dev,local_Fimgs_dev,nr_images*mem_size,cudaMemcpyDeviceToDevice);
//#define DEBUG_SELFAPPLY
#ifdef DEBUG_SELFAPPLY
	//double *local_Faux_host;
	//int size_fft;
	 Fourier_image_size = local_Fimgs[0].zyxdim;
	 yxdim_four  = local_Fimgs[0].yxdim;
	 xdim_four = local_Fimgs[0].xdim;
	size_fft = nr_images*Fourier_image_size*2;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,exp_Fimgs_nomask_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Fimgs[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Fimgs[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The Raw  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for between CPU and GPU " << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif			


//#define DEBUG_SOFTMASK_GPU
#ifdef DEBUG_SOFTMASK_GPU
			 // double *local_images_host;
			  local_images_host = new double[size_];
			  cudaMemcpy(local_images_host,image_dev,size_*sizeof(double), cudaMemcpyDeviceToHost);
			 for(int k = 0; k < exp_nr_images; k++)
			{
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[k]())
				{
					if(local_images_host[k*image_size+i*mymodel.ori_size+j]!=DIRECT_A2D_ELEM(local_images[k](), i, j))
					{
						std::cout << "The softMaskOutsideMap image value for GPU and CPU is " <<local_images_host[k*image_size+i*mymodel.ori_size+j]<< "  "<< DIRECT_A2D_ELEM(local_images[k](), i, j)<< "  "<<local_images[k]().data[i*mymodel.ori_size+j] << std::endl;
						std::cerr << "The iamge data is not the same for softMaskOutsideMap between CPU and GPU " <<k << "  "<< i << "  "<< j<<std::endl;
					}
				}
			}
			 delete [] local_images_host;
#endif

		double *Mnoise_dev;
			Mnoise_dev = NULL;
		softMaskOutsideMap_gpu(image_dev, particle_diameter / (2. * mymodel.pixel_size), (double)width_mask_edge, Mnoise_dev, nr_images, xdim, ydim,zdim);
/*
		for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];
	
			for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int my_image_no = exp_starting_image_no.at(ipart) + iseries;
				int group_id = mydata.getGroupId(part_id, iseries);

				long int ori_part_id = exp_ipart_to_ori_part_id[ipart];
	
				MultidimArray<double> Mnoise;
				if (!do_zero_mask)
				{
					// Make a noisy background image with the same spectrum as the sigma2_noise
	
					// Different MPI-distributed subsets may otherwise have different instances of the random noise below,
					// because work is on an on-demand basis and therefore variable with the timing of distinct nodes...
					// Have the seed based on the ipart, so that each particle has a different instant of the noise
					// Do this all inside a mutex for the threads, because they all use the same static variables inside ran1...
					// (So the mutex only goal is to make things exactly reproducible with the same random_seed.)
					//global_mutex.lock();
	
					//init_random_generator(random_seed + ori_part_id);
					if (do_realign_movies)
						init_random_generator(random_seed + part_id);
					else
						init_random_generator(random_seed + ori_part_id);
	
					// If we're doing running averages, then the sigma2_noise was already adjusted for the running averages.
					// Undo this adjustment here in order to get the right noise in the individual frames
					MultidimArray<double> power_noise = sigma2_fudge * mymodel.sigma2_noise[group_id];
					if (do_realign_movies)
						power_noise *= (2. * movie_frame_running_avg_side + 1.);
	
					// Create noisy image for outside the mask
					MultidimArray<Complex > Fnoise;
					Mnoise.resize(local_images[my_image_no]());
					transformer.setReal(Mnoise);
					transformer.getFourierAlias(Fnoise);
					// Fill Fnoise with random numbers, use power spectrum of the noise for its variance
					FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(Fnoise)
					{
						int ires = ROUND( sqrt( (double)(kp * kp + ip * ip + jp * jp) ) );
						if (ires >= 0 && ires < XSIZE(Fnoise))
						{
							double sigma = sqrt(DIRECT_A1D_ELEM(power_noise, ires));
							DIRECT_A3D_ELEM(Fnoise, k, i, j).real = rnd_gaus(0., sigma);
							DIRECT_A3D_ELEM(Fnoise, k, i, j).imag = rnd_gaus(0., sigma);
						}
						else
						{
							DIRECT_A3D_ELEM(Fnoise, k, i, j) = 0.;
						}
					}
					// Back to real space Mnoise
					transformer.inverseFourierTransform();
					Mnoise.setXmippOrigin();
	
					// unlock the mutex now that all calss to random functions have finished
					//global_mutex.unlock();
	
					softMaskOutsideMap(local_images[my_image_no](), particle_diameter / (2. * mymodel.pixel_size), (double)width_mask_edge, &Mnoise);
	
				}
				else
				{
					softMaskOutsideMap(local_images[my_image_no](), particle_diameter / (2. * mymodel.pixel_size), (double)width_mask_edge);
				}
#ifdef DEBUG_SOFTMASK
					tt()=local_images[my_image_no]();
					tt.write("Fimg_masked.spi");
					std::cerr << "written Fimg_masked.spi; press any key to continue..." << std::endl;
					exit(1);
					std::cin >> c;
#endif
				}
			}

//#define DEBUG_SOFTMASK
#ifdef DEBUG_SOFTMASK_GPU
			 // double *local_images_host;
			  local_images_host = new double[size_];
			  int local_sum = 0;
			  cudaMemcpy(local_images_host,image_dev,size_*sizeof(double), cudaMemcpyDeviceToHost);
			 for(int k = 0; k < exp_nr_images; k++)
			{
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[k]())
				{
					if(ABS(local_images_host[k*image_size+i*mymodel.ori_size+j]-DIRECT_A2D_ELEM(local_images[k](), i, j))>1.e-5)
					{
						std::cout << "The softMaskOutsideMap image value for GPU and CPU is " <<local_images_host[k*image_size+i*mymodel.ori_size+j]<< "  "<< DIRECT_A2D_ELEM(local_images[k](), i, j)<< "  "<<local_images[k]().data[i*mymodel.ori_size+j] << std::endl;
						std::cerr << "The iamge data is not the same for softMaskOutsideMap between CPU and GPU " <<k << "  "<< i << "  "<< j<<std::endl;
						//char c;
						//std::cin>>c;
					}
					//if(local_images_host[k*image_size+i*mymodel.ori_size+j] > (particle_diameter / (2. * mymodel.pixel_size)+width_mask_edge ))
					//	local_sum +=1;
				}
				//std::cout<<local_sum << std::endl;
			}
			
			 delete [] local_images_host;
#endif


			//Do centerFFT	for the softmask images
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
		
				// Inside Projector and Backprojector the origin of the Fourier Transform is centered!
					CenterFFT(local_images[my_image_no](), true);
				}
	
			}
		*/	
			centerFFT_gpu(image_dev , local_images_dev, exp_nr_images, dim, xdim,  ydim, zdim, true);
/*		
#ifdef DEBUG_CenterFFT
			 // double *local_images_host;
			  local_images_host = new double[size_];
			  cudaMemcpy(local_images_host,local_images_dev,size_*sizeof(double), cudaMemcpyDeviceToHost);
			for(int k = 0; k < exp_nr_images; k++)
			{
				FOR_ALL_DIRECT_ELEMENTS_IN_ARRAY2D(local_images[k]())
				{
					if(abs(local_images_host[k*image_size+i*mymodel.ori_size+j]-DIRECT_A2D_ELEM(local_images[k](), i, j))>1.e-8)
					{
						std::cout << "The CenterFFT image value for GPU and CPU is " <<local_images_host[k*image_size+i*mymodel.ori_size+j]<< "  "<< DIRECT_A2D_ELEM(local_images[k](), i, j)<< std::endl;
						std::cerr << "The iamge data is not the same for CenterFFT between CPU and GPU  i: " << i <<"  and  j: "<< j<<std::endl;
					}
				}
			}
#endif
*/
			// Store the Fourier Transform of the image Fimg
			/*for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					transformer.FourierTransform(local_images[my_image_no](), local_Faux[my_image_no]);
				}
			}
			*/
			transformer.FourierTransform_gpu(local_images_dev, local_Faux_dev, exp_nr_images, xdim,  ydim, zdim);
/*			
#ifdef DEBUG_FFT
	//double *local_Faux_host;
	//int size_fft;
	 Fourier_image_size = local_Faux[0].zyxdim;
	 yxdim_four  = local_Faux[0].yxdim;
	 xdim_four = local_Faux[0].xdim;
	size_fft = nr_images*Fourier_image_size*2;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,local_Faux_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Faux[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Faux[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The FFT  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for Transform Fourier between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif
*/
			//double *local_Faux_host;
			 size_fft = exp_nr_images*(xdim/2+1)*ydim* zdim*2;
			local_Faux_host = new double[size_fft];
			cudaMemcpy(local_Faux_host,local_Faux_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
			for(int i = 0; i < exp_nr_images; i++)
			{
				local_Faux[i].resize(zdim,ydim,(xdim/2+1));
				memcpy(local_Faux[i].data, local_Faux_host+i*(xdim/2+1)*ydim* zdim*2, (xdim/2+1)*ydim* zdim*2*sizeof(double));
			}
			delete [] local_Faux_host;
			// Store the power_class spectrum of the whole image (to fill sigma2_noise between current_size and ori_size
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					
					if (mymodel.current_size < mymodel.ori_size)
					{
						MultidimArray<double> spectrum;
						spectrum.initZeros(mymodel.ori_size/2 + 1);
						double highres_Xi2 = 0.;
						FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Faux[my_image_no])
						{
							int ires = ROUND( sqrt( (double)(kp*kp + ip*ip + jp*jp) ) );
							// Skip Hermitian pairs in the x==0 column
	
							if (ires > 0 && ires < mymodel.ori_size/2 + 1 && !(jp==0 && ip < 0) )
							{
								double normFaux = norm(DIRECT_A3D_ELEM(local_Faux[my_image_no], k, i, j));
								DIRECT_A1D_ELEM(spectrum, ires) += normFaux;
								// Store sumXi2 from current_size until ori_size
								if (ires >= mymodel.current_size/2 + 1)
									highres_Xi2 += normFaux;
							}
						}
	
						// Let's use .at() here instead of [] to check whether we go outside the vectors bounds
						exp_power_imgs.at(my_image_no) = spectrum;
						exp_highres_Xi2_imgs.at(my_image_no) = highres_Xi2;
					}
					else
					{
						exp_highres_Xi2_imgs.at(my_image_no) = 0.;
					}
				}
			}
/*	
				// We never need any resolutions higher than current_size
				// So resize the Fourier transforms
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					windowFourierTransform(local_Faux[my_image_no], local_Fimgs[my_image_no], mymodel.current_size);
				
				}
			}
	*/		
			//if(exp_Fimgs_dev!=NULL)
			//	cudaFree(exp_Fimgs_dev);
			cudaMalloc((void **)&exp_Fimgs_dev, nr_images*mem_size);
 			windowFourierTransform_gpu(local_Faux_dev,
								 exp_Fimgs_dev,
								 mymodel.current_size,
								 nr_images,
 								 2,
 								 (xdim/2)+1,
 								 ydim,
 								 zdim);
/*
 #ifdef DEBUG_WINDOWFFT
	//double *local_Faux_host;
	//int size_fft;
	Fourier_image_size = local_Fimgs[0].zyxdim;
	yxdim_four  = local_Fimgs[0].yxdim;
	xdim_four = local_Fimgs[0].xdim;
	size_fft = nr_images*Fourier_image_size*2;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,exp_Fimgs_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Fimgs[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Fimgs[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The windowFourierTransform_gpu  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for windowFourierTransform_gpu Fourier between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif
*/
 			double *local_Faux_no_maskhost;
			 Fourier_image_size = newdim*newhdim;
			
			size_fft = exp_nr_images*Fourier_image_size*2;
			local_Faux_host = new double[size_fft];
			local_Faux_no_maskhost = new double[size_fft];
			cudaMemcpy(local_Faux_host, exp_Fimgs_dev, size_fft*sizeof(double), cudaMemcpyDeviceToHost);
			cudaMemcpy(local_Faux_no_maskhost, exp_Fimgs_nomask_dev, size_fft*sizeof(double), cudaMemcpyDeviceToHost);
			
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					// Also store its CTF
					//local_Fctf_images[my_image_no].resize(local_Fimgs[my_image_no]);
					local_Fctf_images[my_image_no].resize(newdim, newhdim);
			
					// Now calculate the actual CTF
					if (do_ctf_correction)
					{
						CTF ctf;
						ctf.setValues(DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_U),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_V),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_ANGLE),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_Q0),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_BFAC));
		
						ctf.getFftwImage(local_Fctf_images[my_image_no], mymodel.ori_size, mymodel.ori_size, mymodel.pixel_size,
								ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
		//#define DEBUG_CTF_FFTW_IMAGE
#ifdef DEBUG_CTF_FFTW_IMAGE
						Image<double> tt;
						tt()=local_Fctf_images[my_image_no];
						tt.write("relion_ctf.spi");
						std::cerr << "Written relion_ctf.spi, now exiting..." << std::endl;
						exit(1);
#endif
		//#define DEBUG_GETCTF
#ifdef DEBUG_GETCTF
						std::cerr << " intact_ctf_first_peak= " << intact_ctf_first_peak << std::endl;
						ctf.write(std::cerr);
						Image<double> tmp;
						tmp()=local_Fctf_images[my_image_no];
						tmp.write("Fctf.spi");
						tmp().resize(mymodel.ori_size, mymodel.ori_size);
						ctf.getCenteredImage(tmp(), mymodel.pixel_size, ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
						tmp.write("Fctf_cen.spi");
						std::cerr << "Written Fctf.spi, Fctf_cen.spi. Press any key to continue..." << std::endl;
						char c;
						std::cin >> c;
#endif
					}
					else
					{
						local_Fctf_images[my_image_no].initConstant(1.);
					}
		
					// Store Fimg and Fctf

					exp_Fimgs[my_image_no].resize(1,newdim, newhdim);
					exp_Fimgs_nomask[my_image_no].resize(1,newdim, newhdim);
					memcpy(exp_Fimgs[my_image_no].data, local_Faux_host+my_image_no*newhdim*newdim*2, newhdim*newdim*2*sizeof(double));
					memcpy(exp_Fimgs_nomask[my_image_no].data, local_Faux_no_maskhost+my_image_no*newhdim*newdim*2, newhdim*newdim*2*sizeof(double));
					
					//exp_Fimgs.at(my_image_no) = local_Fimgs[my_image_no];
					
					/*memcpy(local_Fimgs[my_image_no].data, local_Faux_no_maskhost+my_image_no*newhdim*newdim*2, newhdim*newdim*2*sizeof(double));

					FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(exp_Fimgs_nomask[my_image_no])
					{
						Complex GPU_value, CPU_value;
						GPU_value = FFTW_ELEM(local_Fimgs[my_image_no], k, i, j);
						CPU_value = FFTW_ELEM(exp_Fimgs_nomask[my_image_no], k, i, j);
						if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
						{
							std::cout << "The exp_Fimgs_nomask  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
							std::cerr << "The iamge data is not the same for between CPU and GPU " << my_image_no<< "  "<<k<< "  "<< i << "  "<< j<<std::endl;
						}
						//FFTW_ELEM(exp_Fimgs_nomask[my_image_no], k, i, j) = FFTW_ELEM(local_Fimgs[my_image_no], k, i, j);
					}*/

					//exp_Fimgs_nomask.at(my_image_no) = local_Fimgs[my_image_no];
					
					exp_Fctfs.at(my_image_no) = local_Fctf_images[my_image_no];
		
			} // end loop iseries
		}// end loop ipart
		/*
			for (long int ipart = 0; ipart < exp_nr_particles; ipart++)
			{
				long int part_id = exp_ipart_to_part_id[ipart];
	
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
				{
					int my_image_no = exp_starting_image_no.at(ipart) + iseries;
					// Also store its CTF
					local_Fctf_images[my_image_no].resize(local_Fimgs[my_image_no]);
			
					// Now calculate the actual CTF
					if (do_ctf_correction)
					{
						CTF ctf;
						ctf.setValues(DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_U),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_V),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_DEFOCUS_ANGLE),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_VOLTAGE),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_CS),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_Q0),
								DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CTF_BFAC));
		
						ctf.getFftwImage(local_Fctf_images[my_image_no], mymodel.ori_size, mymodel.ori_size, mymodel.pixel_size,
								ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
		//#define DEBUG_CTF_FFTW_IMAGE
#ifdef DEBUG_CTF_FFTW_IMAGE
						Image<double> tt;
						tt()=local_Fctf_images[my_image_no];
						tt.write("relion_ctf.spi");
						std::cerr << "Written relion_ctf.spi, now exiting..." << std::endl;
						exit(1);
#endif
		//#define DEBUG_GETCTF
#ifdef DEBUG_GETCTF
						std::cerr << " intact_ctf_first_peak= " << intact_ctf_first_peak << std::endl;
						ctf.write(std::cerr);
						Image<double> tmp;
						tmp()=local_Fctf_images[my_image_no];
						tmp.write("Fctf.spi");
						tmp().resize(mymodel.ori_size, mymodel.ori_size);
						ctf.getCenteredImage(tmp(), mymodel.pixel_size, ctf_phase_flipped, only_flip_phases, intact_ctf_first_peak, true);
						tmp.write("Fctf_cen.spi");
						std::cerr << "Written Fctf.spi, Fctf_cen.spi. Press any key to continue..." << std::endl;
						char c;
						std::cin >> c;
#endif
					}
					else
					{
						local_Fctf_images[my_image_no].initConstant(1.);
					}
		
					// Store Fimg and Fctf
					exp_Fimgs.at(my_image_no) = local_Fimgs[my_image_no];
					exp_Fctfs.at(my_image_no) = local_Fctf_images[my_image_no];
		
			} // end loop iseries
		}// end loop ipart
		*/
		transformer.free_memory_gpu();
		transformer.cleanup();

		cudaFree(translated_matrix_dev);
		cudaFree(beamtilt_x_dev);
		cudaFree(beamtilt_y_dev);
		cudaFree(Cs_dev);
		cudaFree(lambda_dev);
		cudaFree(image_dev);
		cudaFree(rec_image_dev);
		cudaFree(local_images_dev);
		cudaFree(local_Faux_dev);
		//cudaFree(local_Fimgs_dev);

		

		delete [] normcorr_host;
		delete [] beamtilt_x_host;
		delete [] beamtilt_y_host;
		delete [] Cs_host;
		delete [] lambda_host;
		delete []exp_old_offset_host;
		delete [] raw_data_images;
		if (has_converged && do_use_reconstruct_images)
			delete []raw_rec_images;
	
}

void MlOptimiser::getAllSquaredDifferences_gpu()
{
	
#ifdef TIMING
		if (exp_ipass == 0) timer.tic(TIMING_ESP_DIFF1);
		else timer.tic(TIMING_ESP_DIFF2);
#endif
	
	//#define DEBUG_GETALLDIFF2
#ifdef DEBUG_GETALLDIFF2
		std::cerr << " ipass= " << exp_ipass << " exp_current_oversampling= " << exp_current_oversampling << std::endl;
		std::cerr << " sampling.NrPsiSamplings(exp_current_oversampling)= " << sampling.NrPsiSamplings(exp_current_oversampling) << std::endl;
		std::cerr << " sampling.NrTranslationalSamplings(exp_current_oversampling)= " << sampling.NrTranslationalSamplings(exp_current_oversampling) << std::endl;
		std::cerr << " sampling.NrSamplingPoints(exp_current_oversampling)= " << sampling.NrSamplingPoints(exp_current_oversampling) << std::endl;
		std::cerr << " sampling.oversamplingFactorOrientations(exp_current_oversampling)= "<<sampling.oversamplingFactorOrientations(exp_current_oversampling) << std::endl;
		std::cerr << " sampling.oversamplingFactorTranslations(exp_current_oversampling)= "<<sampling.oversamplingFactorTranslations(exp_current_oversampling) << std::endl;
#endif

long t1, t2, t3, t4;
t1 = t2 = t3 = t4 = 0;
struct timeval start, end;	

gettimeofday(&start, NULL);

		// Initialise min_diff and exp_Mweight for this pass
		//exp_Mweight.resize(exp_nr_particles, mymodel.nr_classes * sampling.NrSamplingPoints(exp_current_oversampling, false));
		//exp_Mweight.initConstant(-999.);

TIMER_START		
t_getsquare_prepare.start();
t_getsquare_exp_mweight.start();

		exp_Mweight_dev_size = mymodel.nr_classes *sampling.NrSamplingPoints(exp_current_oversampling, false); 

		cudaError_t cudaStat;

		cudaStat = cudaMalloc((void**)&exp_Mweight_dev,  exp_nr_particles * mymodel.nr_classes * sampling.NrSamplingPoints(exp_current_oversampling, false) * sizeof(double));
TIMER_END
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory allocation failed: exp_Mweight_dev %d\n", exp_nr_particles * exp_Mweight_dev_size);
    	//return EXIT_FAILURE;
    	}
t_getsquare_exp_mweight.stop();
TIMER_START
		init_exp_mweight_gpu(exp_Mweight_dev, -999., exp_nr_particles * mymodel.nr_classes * sampling.NrSamplingPoints(exp_current_oversampling, false)); 
cudaDeviceSynchronize();
TIMER_END

//		cudaMemset(exp_Mweight_dev, 0, exp_nr_particles * mymodel.nr_classes * sampling.NrSamplingPoints(exp_current_oversampling, false) * sizeof(double));
TIMER_START
		
gettimeofday(&end, NULL);
t3 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
		if (exp_ipass==0)
			exp_Mcoarse_significant.clear();
	
		cudaMalloc((void**)&exp_min_diff2_dev, exp_nr_particles*sizeof(double));

t_getsquare_prepare.stop();

t_getsquare_kernel.start();
		init_exp_min_diff2_gpu(exp_min_diff2_dev, 99e99, exp_nr_particles);
t_getsquare_kernel.stop();
TIMER_END

		// Use pre-sized vectors instead of push_backs!!
		exp_local_Fimgs_shifted.clear();
		exp_local_Fimgs_shifted.resize(exp_nr_images * sampling.NrTranslationalSamplings(exp_current_oversampling));
		exp_local_Fimgs_shifted_nomask.clear();
		exp_local_Fimgs_shifted_nomask.resize(exp_nr_images * sampling.NrTranslationalSamplings(exp_current_oversampling));
		exp_local_Minvsigma2s.clear();
		exp_local_Minvsigma2s.resize(exp_nr_images);
		exp_local_Fctfs.clear();
		exp_local_Fctfs.resize(exp_nr_images);
		exp_local_sqrtXi2.clear();
		exp_local_sqrtXi2.resize(exp_nr_images);
	

		// TODO: MAKE SURE THAT ALL PARTICLES IN SomeParticles ARE FROM THE SAME AREA, SO THAT THE R_mic CAN BE RE_USED!!!
		//for (exp_iseries = 0; exp_iseries < mydata.getNrImagesInSeries(part_id); exp_iseries++)
		for (exp_iseries = 0; exp_iseries < mydata.getNrImagesInSeries((mydata.ori_particles[exp_my_first_ori_particle]).particles_id[0]); exp_iseries++)
		{
	
			// Get all shifted versions of the (downsized) images, their (downsized) CTFs and their inverted Sigma2 matrices
			//exp_ipart_ThreadTaskDistributor->reset(); // reset thread distribution tasks
			//global_ThreadManager->run(globalThreadPrecalculateShiftedImagesCtfsAndInvSigma2s);
			//The synchronized function 
gettimeofday(&start, NULL);
			doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_gpu();
gettimeofday(&end, NULL);
t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);			
			// Get micrograph transformation matrix. Note that for all pooled particles (exp_my_first_particle-exp_my_last_particle)
			// the same exp_R_mic will be used in order to re-use the reference projections
			// This is the reason why all pooled particles should come from the same micrograph
			// TODO: THAT STILL NEEDS TO BE CONFIRMED!!!! CURRENTLY NO CHECK ON SAME-PARTICLENAME IN EACH POOL!!!
			// WORKAROUND FOR NOW: just set --pool 1
			//exp_R_mic = mydata.getMicrographTransformationMatrix((mydata.ori_particles[exp_my_first_ori_particle]).particles_id[0], exp_iseries);
			int my_image_no = exp_starting_image_no[0] + exp_iseries;
			// Get micrograph transformation matrix
			exp_R_mic.resize(3,3);
			exp_R_mic(0,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_0);
			exp_R_mic(0,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_1);
			exp_R_mic(0,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_2);
			exp_R_mic(1,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_0);
			exp_R_mic(1,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_1);
			exp_R_mic(1,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_2);
			exp_R_mic(2,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_0);
			exp_R_mic(2,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_1);
			exp_R_mic(2,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_2);
	
			// Loop from iclass_min to iclass_max to deal with seed generation in first iteration
			for (exp_iclass = iclass_min; exp_iclass <= iclass_max; exp_iclass++)
			{
				if (mymodel.pdf_class[exp_iclass] > 0.)
				{
	
					//exp_iorient_ThreadTaskDistributor->reset(); // reset thread distribution tasks
					//global_ThreadManager->run(globalThreadGetSquaredDifferencesAllOrientations);
gettimeofday(&start, NULL);
					doThreadGetSquaredDifferencesAllOrientations_gpu();
gettimeofday(&end, NULL);
t2 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);						
	
				} // end if mymodel.pdf_class[iclass] > 0.
			} // end loop iclass
		} // end loop iseries
//std::cout << "getAllSquaredDifferences_gpu " <<  t1/1000000. << " " << t2/1000000. << " " << t3/1000000. << std::endl;

#ifdef DEBUG_GETALLDIFF2b
		for (long int part_id = exp_my_first_particle, ipart = 0; part_id <= exp_my_last_particle; part_id++, ipart++)
		{
			if (exp_min_diff2[ipart] < 0.)
			{
				std::cerr << "Negative min_diff2...." << std::endl;
				std::cerr << " ipart= " << ipart << " part_id= "<<part_id<<std::endl;
				std::cerr << " do_firstiter_cc= " << do_firstiter_cc << std::endl;
				int group_id = mydata.getGroupId(part_id, 0);
				std::cerr << " group_id= " << group_id << std::endl;
				std::cerr << " ml_model.sigma2_noise[group_id]= " << mymodel.sigma2_noise[group_id] << std::endl;
			}
		}
#endif
	
#ifdef TIMING
		if (exp_ipass == 0) timer.toc(TIMING_ESP_DIFF1);
		else timer.toc(TIMING_ESP_DIFF2);
#endif
	
}

void MlOptimiser::doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_syn()
{
		size_t first_ipart = 0, last_ipart = 0;
		first_ipart = exp_my_first_ori_particle;
		last_ipart = exp_my_last_ori_particle;
		//while (exp_ipart_ThreadTaskDistributor->getTasks(first_ipart, last_ipart))
		{
			for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
			{
				// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
				// but some, e.g. the last, batch of pooled particles may be smaller
				//if (ipart >= exp_nr_particles)
				//	break;
	
				long int part_id = exp_ipart_to_part_id[ipart];

				int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
	
				// Downsize Fimg and Fctf (again) to exp_current_image_size, also initialise Fref and Fimg_shift to the right size
				MultidimArray<Complex > Fimg, Fshifted, Fimg_nomask, Fshifted_nomask;
				windowFourierTransform(exp_Fimgs[my_image_no], Fimg, exp_current_image_size);
				windowFourierTransform(exp_Fimgs_nomask[my_image_no], Fimg_nomask, exp_current_image_size);
	
	
				// Also precalculate the sqrt of the sum of all Xi2
				// (Could exp_current_image_size ever be different from mymodel.current_size? Probhably therefore do it here rather than in getFourierTransforms
				if ((iter == 1 && do_firstiter_cc) || do_always_cc)
				{
					double sumxi2 = 0.;
					FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Fimg)
					{
						sumxi2 += norm(DIRECT_MULTIDIM_ELEM(Fimg, n));
					}
					// Normalised cross-correlation coefficient: divide by power of reference (power of image is a constant)
					exp_local_sqrtXi2[my_image_no] = sqrt(sumxi2);
				}
	
				// Store all translated variants of Fimg
				int my_trans_image = my_image_no * exp_nr_trans * exp_nr_oversampled_trans;
				for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
				{
					// First get the non-oversampled translations as defined by the sampling object
					std::vector<Matrix1D <double> > oversampled_translations;
					sampling.getTranslations(itrans, exp_current_oversampling, oversampled_translations);
	
					// Then loop over all its oversampled relatives
					for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
					{

						// Shift through phase-shifts in the Fourier transform
						// Note that the shift search range is centered around (exp_old_xoff, exp_old_yoff)
						shiftImageInFourierTransform(Fimg, Fshifted, tab_sin, tab_cos, (double)mymodel.ori_size, oversampled_translations[iover_trans]);
						shiftImageInFourierTransform(Fimg_nomask, Fshifted_nomask, tab_sin, tab_cos, (double)mymodel.ori_size, oversampled_translations[iover_trans]);
	

						// Store the shifted image
						exp_local_Fimgs_shifted[my_trans_image] = Fshifted;
						exp_local_Fimgs_shifted_nomask[my_trans_image] = Fshifted_nomask;
						my_trans_image++;
					}
				}
	
	
				// Also store downsized Fctfs
				// In the second pass of the adaptive approach this will have no effect,
				// since then exp_current_image_size will be the same as the size of exp_Fctfs

				MultidimArray<double> Fctf;
				windowFourierTransform(exp_Fctfs[my_image_no], Fctf, exp_current_image_size);
				exp_local_Fctfs[my_image_no] = Fctf;
	
				// Get micrograph id (for choosing the right sigma2_noise)
				int group_id = mydata.getGroupId(part_id, exp_iseries);
	
				MultidimArray<double> Minvsigma2;
				Minvsigma2.initZeros(YSIZE(Fimg), XSIZE(Fimg));
				MultidimArray<int> * myMresol = (YSIZE(Fimg) == coarse_size) ? &Mresol_coarse : &Mresol_fine;
	
				// With group_id and relevant size of Fimg, calculate inverse of sigma^2 for relevant parts of Mresol
				FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(*myMresol)
				{
					int ires = DIRECT_MULTIDIM_ELEM(*myMresol, n);
					// Exclude origin (ires==0) from the Probability-calculation
					// This way we are invariant to additive factors
					if (ires > 0)
					{
						DIRECT_MULTIDIM_ELEM(Minvsigma2, n) = 1. / (sigma2_fudge * DIRECT_A1D_ELEM(mymodel.sigma2_noise[group_id], ires));
					}
				}
	
				exp_local_Minvsigma2s[my_image_no] = Minvsigma2;
			}
		}
	
		// Wait until all threads are finsished
		//global_barrier->wait();
	
#ifdef TIMING
		timer.toc(TIMING_DIFF_SHIFT);
#endif
	
	
}

void MlOptimiser::doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_gpu()
{

		int in_size = exp_Fimgs[0].zyxdim;
		double *local_exp_Fimgs, *local_exp_Fimgs_nomask;
		local_exp_Fimgs = (double*) malloc(in_size*2*sizeof(double)*exp_nr_images);
		local_exp_Fimgs_nomask = (double*) malloc(in_size*2*sizeof(double)*exp_nr_images);
		
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(exp_Fimgs[ipart])
			{
				Complex  img_value, ima_value_nomask ;
				img_value = FFTW_ELEM(exp_Fimgs[ipart], k, i, j);
				ima_value_nomask = FFTW_ELEM(exp_Fimgs_nomask[ipart], k, i, j);
				
				local_exp_Fimgs[(ipart*exp_Fimgs[ipart].zyxdim+k*exp_Fimgs[ipart].yxdim+i*exp_Fimgs[ipart].xdim+j)*2]=img_value.real;
				local_exp_Fimgs[(ipart*exp_Fimgs[ipart].zyxdim+k*exp_Fimgs[ipart].yxdim+i*exp_Fimgs[ipart].xdim+j)*2+1]=img_value.imag;
				
				local_exp_Fimgs_nomask[(ipart*exp_Fimgs_nomask[ipart].zyxdim+k*exp_Fimgs_nomask[ipart].yxdim+i*exp_Fimgs_nomask[ipart].xdim+j)*2]=ima_value_nomask.real;
				local_exp_Fimgs_nomask[(ipart*exp_Fimgs_nomask[ipart].zyxdim+k*exp_Fimgs_nomask[ipart].yxdim+i*exp_Fimgs_nomask[ipart].xdim+j)*2+1]=ima_value_nomask.imag;
			}
		}
		
		//WindowFourierTransform for some particles on GPU 
		cufftDoubleComplex *local_Fimgs_dev, *local_Fimg_nomask_dev;
		cufftDoubleComplex *local_Faux_dev, *local_Faux_nomask_dev;
		//cufftDoubleComplex *exp_mem;
		
		
		int ndim  = exp_Fimgs[0].getDim();
		//(local_Fimgs[0].zdim>1) ? 3:((local_Fimgs[0].ydim>1) ? 2 : 1);
		int newhdim = exp_current_image_size/2 +1;
		int newdim = exp_current_image_size;
		int window_img_size;
		int xdim = mymodel.current_size/2+1;
		int ydim = mymodel.current_size;
		int zdim = 1;
		
		if(ndim == 1)
		{
			 window_img_size = newhdim;
		}
		else if(ndim == 2)
		{
			window_img_size = newdim*newhdim;
		}
		else if(ndim == 3)
		{
			window_img_size = newdim*newdim*newhdim;
		}

		
		cudaMalloc((void **)&local_Fimgs_dev, exp_nr_images*window_img_size*sizeof(cufftDoubleComplex));
		cudaMalloc((void **)&local_Fimg_nomask_dev, exp_nr_images*window_img_size*sizeof(cufftDoubleComplex)); 
		cudaMalloc((void **)&local_Faux_dev, exp_nr_images*in_size*2*sizeof(double)); 
		cudaMalloc((void **)&local_Faux_nomask_dev, exp_nr_images*in_size*2*sizeof(double)); 

		cudaMemcpy(local_Faux_dev,local_exp_Fimgs,exp_nr_images*in_size*2*sizeof(double),cudaMemcpyHostToDevice );
		cudaMemcpy(local_Faux_nomask_dev,local_exp_Fimgs_nomask,exp_nr_images*in_size*2*sizeof(double),cudaMemcpyHostToDevice );
		//cudaFree(exp_Fimgs_nomask_dev);
		//cudaMalloc((void **)&exp_Fimgs_nomask_dev, nr_images*mem_size);
			//exp_Fimgs_nomask_dev =NULL;
		/*
		//WindowFourierTransform for some particles on GPU 
		
		cufftDoubleComplex *local_Fimgs_dev, *local_Fimg_nomask_dev;

		int ndim  = 2;
		//(local_Fimgs[0].zdim>1) ? 3:((local_Fimgs[0].ydim>1) ? 2 : 1);
		int newhdim = exp_current_image_size/2 +1;
		int newdim = exp_current_image_size;
		int mem_size;
		int xdim = mymodel.current_size/2+1;
		int ydim = mymodel.current_size;
		int zdim = 1;
		
		if(ndim == 1)
		{
			 mem_size = newhdim;
		}
		else if(ndim == 2)
		{
			mem_size = newdim*newhdim;
		}
		else if(ndim == 3)
		{
			mem_size = newdim*newdim*newhdim;
		}

		//cudaMalloc((void **)&exp_mem,mem_size*exp_nr_images);
		//cudaFree(exp_mem);
		cudaMalloc((void **)&local_Fimgs_dev, exp_nr_images*mem_size*sizeof(cufftDoubleComplex));
		cudaMalloc((void **)&local_Fimg_nomask_dev, exp_nr_images*mem_size*sizeof(cufftDoubleComplex)); 
		*/
		windowFourierTransform_gpu(exp_Fimgs_dev,
								 local_Fimgs_dev,
								 exp_current_image_size,
								 exp_nr_images,
 								 2,
 								 xdim,
 								 ydim,
 								 zdim);

 		windowFourierTransform_gpu(exp_Fimgs_nomask_dev,
								 local_Fimg_nomask_dev,
								 exp_current_image_size,
								 exp_nr_images,
 								 2,
 								 xdim,
 								 ydim,
 								 zdim);

/* 								 
#define DEBUG_WINDOWFFT
#ifdef DEBUG_WINDOWFFT
	double *local_Faux_host;
	double *local_Faux_nomask_host;
	int size_fft;
	int Fourier_image_size = local_Fimgs[0].zyxdim;
	int yxdim_four  = local_Fimgs[0].yxdim;
	int  xdim_four = local_Fimgs[0].xdim;
	size_fft = exp_nr_images*Fourier_image_size*2;
	local_Faux_host = new double[size_fft];
	local_Faux_nomask_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,local_Fimgs_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	cudaMemcpy(local_Faux_nomask_host,local_Fimg_nomask_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Fimgs[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Fimgs[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The windowFourierTransform_gpu  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for windowFourierTransform_gpu Fourier between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
				
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(local_Fimg_nomask[l])
		{
			Complex GPU_value, CPU_value;
			GPU_value.real = local_Faux_nomask_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2];
			GPU_value.imag= local_Faux_nomask_host[(l*Fourier_image_size+k*yxdim_four+i*xdim_four+j)*2+1];
			CPU_value = FFTW_ELEM(local_Fimg_nomask[l], k, i, j);
			if((abs(GPU_value.real - CPU_value.real)> 1.e-5)|| abs(GPU_value.imag - CPU_value.imag)>1.e-5)
			{
				std::cout << "The windowFourierTransform_gpu  image value for GPU and CPU is  " <<GPU_value.real<< "  "<< GPU_value.imag<< "  "<<CPU_value.real << " " << CPU_value.imag<< std::endl;
				std::cerr << "The iamge data is not the same for windowFourierTransform_gpu Fourier between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
	delete [] local_Faux_nomask_host;
#endif
	*/
	// Also precalculate the sqrt of the sum of all Xi2
			// (Could exp_current_image_size ever be different from mymodel.current_size? Probhably therefore do it here rather than in getFourierTransforms
/*
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			if ((iter == 1 && do_firstiter_cc) || do_always_cc)
			{
				double sumxi2 = 0.;
				FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(local_Fimgs[my_image_no])
				{
					sumxi2 += norm(DIRECT_MULTIDIM_ELEM(local_Fimgs[my_image_no], n));
				}
				// Normalised cross-correlation coefficient: divide by power of reference (power of image is a constant)
				exp_local_sqrtXi2[my_image_no] = sqrt(sumxi2);
			}
		}
	*/	
		//double *exp_local_sqrtXi2_dev;
		double *exp_local_sqrtXi2_host;
		cudaMalloc((void **)&exp_local_sqrtXi2_dev, exp_nr_images*sizeof(double));
		exp_local_sqrtXi2_host = (double *) malloc(exp_nr_images*sizeof(double));
		if ((iter == 1 && do_firstiter_cc) || do_always_cc)
		{
 			calculate_local_sqrtXi2_gpu(local_Fimgs_dev, exp_local_sqrtXi2_dev, exp_nr_images,  window_img_size);
		
			cudaMemcpy(exp_local_sqrtXi2_host, exp_local_sqrtXi2_dev, exp_nr_images*sizeof(double), cudaMemcpyDeviceToHost);
		
			for(int i = 0; i < exp_nr_images; i++)
			{
				exp_local_sqrtXi2[i] =exp_local_sqrtXi2_host[i];
				/*if((abs(exp_local_sqrtXi2[i] - exp_local_sqrtXi2_host[i])> 1.e-5))
				{
					std::cout << "The exp local sum for CPU and GPU are  " <<exp_local_sqrtXi2[i]<< "  "<< exp_local_sqrtXi2_host[i] << std::endl; 

				}*/
				
			}
		}

		double * oversampled_translations_host;
		oversampled_translations_host = (double *) malloc(exp_nr_trans*exp_nr_oversampled_trans*ndim*sizeof(double));
			// Store all translated variants of Fimg
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			int my_trans_image = my_image_no * exp_nr_trans * exp_nr_oversampled_trans;
			for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
			{
				// First get the non-oversampled translations as defined by the sampling object
				std::vector<Matrix1D <double> > oversampled_translations;
				sampling.getTranslations(itrans, exp_current_oversampling, oversampled_translations);

				// Then loop over all its oversampled relatives
				for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
				{

					// Shift through phase-shifts in the Fourier transform
					// Note that the shift search range is centered around (exp_old_xoff, exp_old_yoff)
					//shiftImageInFourierTransform(local_Fimgs[my_image_no], Fshifted, tab_sin, tab_cos, (double)mymodel.ori_size, oversampled_translations[iover_trans]);
					//shiftImageInFourierTransform(local_Fimg_nomask[my_image_no], Fshifted_nomask, tab_sin, tab_cos, (double)mymodel.ori_size, oversampled_translations[iover_trans]);

					for(int i=0 ; i < ndim; i++)
						oversampled_translations_host[(itrans*exp_nr_oversampled_trans+iover_trans)*ndim+i] = oversampled_translations[iover_trans].vdata[i];
					// Store the shifted image
					//exp_local_Fimgs_shifted[my_trans_image] = Fshifted;
					//exp_local_Fimgs_shifted_nomask[my_trans_image] = Fshifted_nomask;
					my_trans_image++;
				}
			}

		}
			// Also store downsized Fctfs
			// In the second pass of the adaptive approach this will have no effect,
			// since then exp_current_image_size will be the same as the size of exp_Fctfs
		//GPU  shift Images
		
		
		//cufftDoubleComplex *exp_local_Fimgs_shifted_dev, *exp_local_Fimgs_shifted_nomask_dev;
		cudaMalloc((void **)&exp_local_Fimgs_shifted_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* newhdim*newdim*zdim*2*sizeof(double)  );
		cudaMalloc((void **)&exp_local_Fimgs_shifted_nomask_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* newhdim*newdim*zdim*2*sizeof(double)  );
		
		shiftImageInFourierTransform_gpu(local_Fimg_nomask_dev,
		                          				   exp_local_Fimgs_shifted_nomask_dev,
								  	(double) mymodel.ori_size,oversampled_translations_host, 
								  	 exp_nr_images,   exp_nr_trans,  exp_nr_oversampled_trans, 
								  	 newhdim,  newdim,  zdim);


		shiftImageInFourierTransform_gpu(local_Fimgs_dev,
		                          				  exp_local_Fimgs_shifted_dev,
								  	(double) mymodel.ori_size,oversampled_translations_host, 
								  	 exp_nr_images,   exp_nr_trans,  exp_nr_oversampled_trans, 
								  	 newhdim,  newdim,  zdim);
								  	 
		

		double *exp_local_Fimgs_shifted_host, *exp_local_Fimgs_shifted_nomask_host;
		exp_local_Fimgs_shifted_host = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* newhdim*newdim*zdim*2*sizeof(double) );
		exp_local_Fimgs_shifted_nomask_host = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* newhdim*newdim*zdim*2*sizeof(double) );

		cudaMemcpy(exp_local_Fimgs_shifted_host, exp_local_Fimgs_shifted_dev, exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* newhdim*newdim*zdim*2*sizeof(double) , cudaMemcpyDeviceToHost);
		cudaMemcpy(exp_local_Fimgs_shifted_nomask_host, exp_local_Fimgs_shifted_nomask_dev, exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* newhdim*newdim*zdim*2*sizeof(double) , cudaMemcpyDeviceToHost);
		int img_size = newhdim*newdim*zdim;
		
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{

			long int part_id = exp_ipart_to_part_id[ipart];
			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			int my_trans_image = my_image_no * exp_nr_trans * exp_nr_oversampled_trans;
			for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
			{
				for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
				{
					exp_local_Fimgs_shifted[my_trans_image].resize(zdim, newdim,newhdim); //= Fshifted;
					exp_local_Fimgs_shifted_nomask[my_trans_image].resize(zdim, newdim,newhdim) ;//= Fshifted_nomask;
					
					memcpy(exp_local_Fimgs_shifted[my_trans_image].data, exp_local_Fimgs_shifted_host+my_trans_image*img_size*2, img_size*2*sizeof(double));		
					memcpy(exp_local_Fimgs_shifted_nomask[my_trans_image].data, exp_local_Fimgs_shifted_nomask_host+my_trans_image*img_size*2, img_size*2*sizeof(double));
					
					/*for(int i=0 ; i<img_size; i++)
					{
						
						if(abs(exp_local_Fimgs_shifted_host[(my_trans_image*img_size+i)*2]-exp_local_Fimgs_shifted[my_trans_image].data[i].real)>1.e-4||abs(exp_local_Fimgs_shifted_host[(my_trans_image*img_size+i)*2+1]-exp_local_Fimgs_shifted[my_trans_image].data[i].imag)>1.e-4)
						{
							std::cout << "The shifted image with GPU and CPU are  " <<exp_local_Fimgs_shifted_host[(my_trans_image*img_size+i)*2]<< "  "<< exp_local_Fimgs_shifted_host[(my_trans_image*img_size+i)*2+1]<< "  " <<exp_local_Fimgs_shifted[my_trans_image].data[i].real << "  " << exp_local_Fimgs_shifted[my_trans_image].data[i].imag << std::endl; 
							std::cout << "my_trans_image= " <<  my_trans_image << " position i: " << i << std::endl;
						}

						if(abs(exp_local_Fimgs_shifted_nomask_host[(my_trans_image*img_size+i)*2]-exp_local_Fimgs_shifted_nomask[my_trans_image].data[i].real)>1.e-4||abs(exp_local_Fimgs_shifted_nomask_host[(my_trans_image*img_size+i)*2+1]-exp_local_Fimgs_shifted_nomask[my_trans_image].data[i].imag)>1.e-4)
						{
							std::cout << "The shifted image for nomask with GPU and CPU are  " <<exp_local_Fimgs_shifted_nomask_host[(my_trans_image*img_size+i)*2]<< "  "<< exp_local_Fimgs_shifted_nomask_host[(my_trans_image*img_size+i)*2+1]<< "  " <<exp_local_Fimgs_shifted_nomask[my_trans_image].data[i].real << "  " << exp_local_Fimgs_shifted_nomask[my_trans_image].data[i].imag << std::endl; 
							std::cout << "my_trans_image= " <<  my_trans_image << " position i: " << i << std::endl;
						}
					}*/
					my_trans_image++;
				}
			}

		}

		free(exp_local_Fimgs_shifted_host);
		free(exp_local_Fimgs_shifted_nomask_host);
		//delete [] local_Faux_host;
		//delete [] local_Faux_nomask_host;
		double *local_Fctf_host, *local_Fctf_dev;
		int image_size_Fctf = exp_Fctfs[0].zyxdim;
		local_Fctf_host = (double *) malloc(exp_nr_images*image_size_Fctf*sizeof(double));
		
		for(int k = 0 ; k <exp_nr_images; k++ )
		{
			memcpy(local_Fctf_host+k*image_size_Fctf, exp_Fctfs[k].data, image_size_Fctf*sizeof(double)); 
		}
		cudaMalloc((void **)&local_Fctf_dev, image_size_Fctf*exp_nr_images*sizeof(double));
		cudaMemcpy(local_Fctf_dev,local_Fctf_host,image_size_Fctf*exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);

		cudaMalloc((void **)&exp_local_Fctfs_dev, (exp_nr_images*img_size*sizeof(double)));
		
		windowFourierTransform_gpu(local_Fctf_dev,
								 exp_local_Fctfs_dev,
								 exp_current_image_size,
								 exp_nr_images,
 								 2,
 								 exp_Fctfs[0].xdim,
 								 exp_Fctfs[0].ydim,
 								 zdim);
		double *local_Faux_host;
 		int fctf_image_size = newhdim*newdim;
		int yxdim_four  = exp_current_image_size;
		int xdim_four = exp_current_image_size/2+1;
		int size_fft = exp_nr_images*fctf_image_size;
		local_Faux_host = new double[size_fft];
		cudaMemcpy(local_Faux_host, exp_local_Fctfs_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
			// but some, e.g. the last, batch of pooled particles may be smaller
			//if (ipart >= exp_nr_particles)
			//	 break;

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			//MultidimArray<double> Fctf;
			///windowFourierTransform(exp_Fctfs[my_image_no], Fctf, exp_current_image_size);
			exp_local_Fctfs[my_image_no].resize(newdim, newhdim);
			memcpy(exp_local_Fctfs[my_image_no].data,local_Faux_host+my_image_no*fctf_image_size,  fctf_image_size*sizeof(double));
			
		}
 								 
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
			// but some, e.g. the last, batch of pooled particles may be smaller
			//if (ipart >= exp_nr_particles)
			//	 break;

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			MultidimArray<double> Fctf;
			windowFourierTransform(exp_Fctfs[my_image_no], Fctf, exp_current_image_size);
			exp_local_Fctfs[my_image_no] = Fctf;
		}
		
#ifdef DEBUG_WINDOWFFT
	//double *local_Faux_host;
	//int size_fft;
	int fctf_image_size = exp_local_Fctfs[0].zyxdim;
	yxdim_four  = exp_local_Fctfs[0].yxdim;
	xdim_four = exp_local_Fctfs[0].xdim;
	size_fft = exp_nr_images*fctf_image_size;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,exp_local_Fctfs_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(exp_local_Fctfs[l])
		{
			double GPU_value, CPU_value;
			GPU_value = local_Faux_host[l*fctf_image_size+k*yxdim_four+i*xdim_four+j];
			CPU_value = FFTW_ELEM(exp_local_Fctfs[l], k, i, j);
			if((abs(GPU_value - CPU_value)> 1.e-5))
			{
				std::cout << "The windowFourierTransform_gpu  image value for GPU and CPU is  " <<GPU_value<< "  "<<CPU_value << std::endl;
				std::cerr << "The iamge data is not the same for windowFourierTransform_gpu FCTF between CPU and GPU" << l << "  "<<k<< "  "<< i << "  "<< j<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif	

		int *group_id_host, *group_id_dev;
		group_id_host = (int *)malloc(exp_nr_images*sizeof(int));
		cudaMalloc((void **)&group_id_dev,exp_nr_images*sizeof(int) );
		// Get micrograph id (for choosing the right sigma2_noise)
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
			// but some, e.g. the last, batch of pooled particles may be smaller
			//if (ipart >= exp_nr_particles)
			//	 break;

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			
			int group_id = mydata.getGroupId(part_id, exp_iseries);
			
			group_id_host[ipart] = group_id;
			
			/*MultidimArray<double> Minvsigma2;
			Minvsigma2.initZeros(YSIZE(local_Fimgs[my_image_no]), XSIZE(local_Fimgs[my_image_no]));
			MultidimArray<int> * myMresol = (YSIZE(local_Fimgs[my_image_no]) == coarse_size) ? &Mresol_coarse : &Mresol_fine;

			// With group_id and relevant size of Fimg, calculate inverse of sigma^2 for relevant parts of Mresol
			FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(*myMresol)
			{
				int ires = DIRECT_MULTIDIM_ELEM(*myMresol, n);
				// Exclude origin (ires==0) from the Probability-calculation
				// This way we are invariant to additive factors
				if (ires > 0)
				{
					DIRECT_MULTIDIM_ELEM(Minvsigma2, n) = 1. / (sigma2_fudge * DIRECT_A1D_ELEM(mymodel.sigma2_noise[group_id], ires));
				}
			}

			exp_local_Minvsigma2s[my_image_no] = Minvsigma2;*/
		}
		// Wait until all threads are finsished

		//Calculate the  Minvsigma2 with GPU
		int *local_myMresol, *local_myMresol_dev;
		double *sigma2_noise_host, *sigma2_noise_dev;
		//int *group_id_host, *group_id_dev;
		int myMresol_size = (( newdim == coarse_size)? Mresol_coarse.zyxdim : Mresol_fine.zyxdim);
		int image_size = zdim*newdim*newhdim;
		local_myMresol = (int *) malloc(myMresol_size*sizeof(int));
		sigma2_noise_host = (double*)malloc(mydata.numberOfGroups()*mymodel.sigma2_noise[0].zyxdim*sizeof(double));
		
		for(int i=0 ; i< myMresol_size; i++)
		{
			local_myMresol[i] = ( newdim == coarse_size) ? Mresol_coarse.data[i] : Mresol_fine.data[i];
		}

		for(int k=0; k < mydata.numberOfGroups(); k++)
		{
			for(int i=0 ; i <mymodel.sigma2_noise[0].zyxdim; i++ )
				sigma2_noise_host[k*mymodel.sigma2_noise[0].zyxdim+i] =  DIRECT_A1D_ELEM(mymodel.sigma2_noise[k], i);
			
		}
		//double *exp_Minvsigma2s_dev;
		cudaMalloc((void **)&local_myMresol_dev, myMresol_size*sizeof(int));
		cudaMalloc((void **)&sigma2_noise_dev,mydata.numberOfGroups()*mymodel.sigma2_noise[0].zyxdim*sizeof(double) );
		cudaMemcpy(local_myMresol_dev, local_myMresol, myMresol_size*sizeof(int), cudaMemcpyHostToDevice);
		cudaMemcpy(sigma2_noise_dev, sigma2_noise_host, mydata.numberOfGroups()*mymodel.sigma2_noise[0].zyxdim*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(group_id_dev, group_id_host, exp_nr_images*sizeof(int), cudaMemcpyHostToDevice );
		
		cudaMalloc((void **)&exp_Minvsigma2s_dev, image_size*exp_nr_images*sizeof(double));
		
		cudaMemset(exp_Minvsigma2s_dev, 0.,image_size*exp_nr_images*sizeof(double));
		calculate_Minvsigma2_gpu(exp_Minvsigma2s_dev,  local_myMresol_dev, sigma2_noise_dev, group_id_dev,   sigma2_fudge,exp_nr_images,  zdim*newdim*newhdim, myMresol_size, mymodel.sigma2_noise[0].zyxdim);

		//exp_Minvsigma2_host = (double*)malloc(local_Fimgs[0].zyxdim*exp_nr_images*sizeof(double));
		//cudaMemcpy(exp_Minvsigma2_host, exp_Minvsigma2_dev,local_Fimgs[0].zyxdim*exp_nr_images*sizeof(double),cudaMemcpyDeviceToHost);

		size_fft = exp_nr_images*image_size;
		local_Faux_host = new double[size_fft];
		cudaMemcpy(local_Faux_host,exp_Minvsigma2s_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			exp_local_Minvsigma2s[my_image_no].resize(zdim, newdim, newhdim);
			memcpy(exp_local_Minvsigma2s[my_image_no].data,local_Faux_host+my_image_no*image_size, image_size*sizeof(double) );
			
		}
		delete [] local_Faux_host;
/*
#define DEBUG_MINVSIGMA2
#ifdef DEBUG_MINVSIGMA2
	//double *local_Faux_host;
	//int size_fft;
	int image_size = exp_local_Minvsigma2s[0].zyxdim;
	yxdim_four  = exp_local_Minvsigma2s[0].yxdim;
	xdim_four = exp_local_Minvsigma2s[0].xdim;
	size_fft = exp_nr_images*image_size;
	local_Faux_host = new double[size_fft];
	cudaMemcpy(local_Faux_host,exp_Minvsigma2s_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
	for(int l = 0; l < exp_nr_images; l++)
	{
		FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(exp_local_Minvsigma2s[l])
		{
			double GPU_value, CPU_value;
			GPU_value = local_Faux_host[l*image_size+n];
			CPU_value = DIRECT_MULTIDIM_ELEM(exp_local_Minvsigma2s[l], n);//FFTW_ELEM(exp_local_Fctfs[l], k, i, j);
			if((abs(GPU_value - CPU_value)> 1.e-5))
			{
				std::cout << "The Minvsigma2   value for GPU and CPU is  " <<GPU_value<< "  "<<CPU_value << std::endl;
				std::cerr << "The iamge data is not the same for windowFourierTransform_gpu FCTF between CPU and GPU" << l << "  "<<n<<std::endl;
			}
		}
	}
	delete [] local_Faux_host;
#endif
	*/
#ifdef TIMING
		timer.toc(TIMING_DIFF_SHIFT);
#endif

	cudaFree(local_Fimgs_dev);
	cudaFree(local_Fimg_nomask_dev);
	cudaFree(local_myMresol_dev);
	cudaFree(sigma2_noise_dev);
	cudaFree(group_id_dev);
	cudaFree(local_Fctf_dev);

	cudaFree(local_Faux_dev);
	cudaFree(local_Faux_nomask_dev);
	
	
	free(local_Fctf_host);
	free(exp_local_sqrtXi2_host);
	free(oversampled_translations_host);
	free(group_id_host);
	free(sigma2_noise_host);
	free(local_myMresol);
	free(local_exp_Fimgs);
	free(local_exp_Fimgs_nomask);

	//cudaFree(exp_local_sqrtXi2_dev);
	//	cudaFree(exp_local_Fimgs_shifted_dev);
	//	cudaFree(exp_local_Fimgs_shifted_nomask_dev);
	//	cudaFree(exp_Minvsigma2s_dev);
	//	cudaFree(exp_local_Fctfs_dev);
	
}
/*
void MlOptimiser::doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_gpu()
{
		//size_t first_ipart = 0, last_ipart = 0;
		//first_ipart = exp_my_first_ori_particle;
		//last_ipart = exp_my_last_ori_particle;

		//std::vector< MultidimArray<Complex> > local_Fimgs, local_Fimg_nomask;
	
		
		//WindowFourierTransform for some particles on GPU 
		cufftDoubleComplex *local_Fimgs_dev, *local_Fimg_nomask_dev;

		int ndim  = 2;
		//(local_Fimgs[0].zdim>1) ? 3:((local_Fimgs[0].ydim>1) ? 2 : 1);
		int newhdim = exp_current_image_size/2 +1;
		int newdim = exp_current_image_size;
		int mem_size;
		int xdim = mymodel.current_size/2+1;
		int ydim = mymodel.current_size;
		int zdim = 1;
		
		if(ndim == 1)
		{
			 mem_size = newhdim;
		}
		else if(ndim == 2)
		{
			mem_size = newdim*newhdim;
		}
		else if(ndim == 3)
		{
			mem_size = newdim*newdim*newhdim;
		}

		//cudaMalloc((void **)&exp_mem,mem_size*exp_nr_images);
		//cudaFree(exp_mem);
		cudaMalloc((void **)&local_Fimgs_dev, exp_nr_images*mem_size*sizeof(cufftDoubleComplex));
		cudaMalloc((void **)&local_Fimg_nomask_dev, exp_nr_images*mem_size*sizeof(cufftDoubleComplex)); 
		
		windowFourierTransform_gpu(exp_Fimgs_dev,
								 local_Fimgs_dev,
								 exp_current_image_size,
								 exp_nr_images,
 								 2,
 								 xdim,
 								 ydim,
 								 zdim);

 		windowFourierTransform_gpu(exp_Fimgs_nomask_dev,
								 local_Fimg_nomask_dev,
								 exp_current_image_size,
								 exp_nr_images,
 								 2,
 								 xdim,
 								 ydim,
 								 zdim);


	// Also precalculate the sqrt of the sum of all Xi2
			// (Could exp_current_image_size ever be different from mymodel.current_size? Probhably therefore do it here rather than in getFourierTransforms

		//double *exp_local_sqrtXi2_dev;
		double *exp_local_sqrtXi2_host;
		cudaMalloc((void **)&exp_local_sqrtXi2_dev, exp_nr_images*sizeof(double));
		exp_local_sqrtXi2_host = (double *) malloc(exp_nr_images*sizeof(double));
		if ((iter == 1 && do_firstiter_cc) || do_always_cc)
		{
 			calculate_local_sqrtXi2_gpu(local_Fimgs_dev, exp_local_sqrtXi2_dev, exp_nr_images,  mem_size);
		
			cudaMemcpy(exp_local_sqrtXi2_host, exp_local_sqrtXi2_dev, exp_nr_images*sizeof(double), cudaMemcpyDeviceToHost);
		
			for(int i = 0; i < exp_nr_images; i++)
			{
				exp_local_sqrtXi2[i] =exp_local_sqrtXi2_host[i];
				
				
			}
		}

		double * oversampled_translations_host;
		oversampled_translations_host = (double *) malloc(exp_nr_trans*exp_nr_oversampled_trans*ndim*sizeof(double));
			// Store all translated variants of Fimg
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			int my_trans_image = my_image_no * exp_nr_trans * exp_nr_oversampled_trans;
			for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
			{
				// First get the non-oversampled translations as defined by the sampling object
				std::vector<Matrix1D <double> > oversampled_translations;
				sampling.getTranslations(itrans, exp_current_oversampling, oversampled_translations);

				// Then loop over all its oversampled relatives
				for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
				{

					// Shift through phase-shifts in the Fourier transform
					// Note that the shift search range is centered around (exp_old_xoff, exp_old_yoff)
					//shiftImageInFourierTransform(local_Fimgs[my_image_no], Fshifted, tab_sin, tab_cos, (double)mymodel.ori_size, oversampled_translations[iover_trans]);
					//shiftImageInFourierTransform(local_Fimg_nomask[my_image_no], Fshifted_nomask, tab_sin, tab_cos, (double)mymodel.ori_size, oversampled_translations[iover_trans]);

					for(int i=0 ; i < ndim; i++)
						oversampled_translations_host[(itrans*exp_nr_oversampled_trans+iover_trans)*ndim+i] = oversampled_translations[iover_trans].vdata[i];
					// Store the shifted image
					//exp_local_Fimgs_shifted[my_trans_image] = Fshifted;
					//exp_local_Fimgs_shifted_nomask[my_trans_image] = Fshifted_nomask;
					my_trans_image++;
				}
			}

		}
			// Also store downsized Fctfs
			// In the second pass of the adaptive approach this will have no effect,
			// since then exp_current_image_size will be the same as the size of exp_Fctfs
		//GPU  shift Images
		
		
		//cufftDoubleComplex *exp_local_Fimgs_shifted_dev, *exp_local_Fimgs_shifted_nomask_dev;
		cudaMalloc((void **)&exp_local_Fimgs_shifted_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* xdim*ydim*zdim*2*sizeof(double)  );
		cudaMalloc((void **)&exp_local_Fimgs_shifted_nomask_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* xdim*ydim*zdim*2*sizeof(double)  );
		
		shiftImageInFourierTransform_gpu(local_Fimg_nomask_dev,
		                          				   exp_local_Fimgs_shifted_nomask_dev,
								  	(double) mymodel.ori_size,oversampled_translations_host, 
								  	 exp_nr_images,   exp_nr_trans,  exp_nr_oversampled_trans, 
								  	 xdim,  ydim,  zdim);


		shiftImageInFourierTransform_gpu(local_Fimgs_dev,
		                          				  exp_local_Fimgs_shifted_dev,
								  	(double) mymodel.ori_size,oversampled_translations_host, 
								  	 exp_nr_images,   exp_nr_trans,  exp_nr_oversampled_trans, 
								  	 xdim,  ydim,  zdim);
								  	 
		

		double *exp_local_Fimgs_shifted_host, *exp_local_Fimgs_shifted_nomask_host;
		exp_local_Fimgs_shifted_host = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* xdim*ydim*zdim*2*sizeof(double) );
		exp_local_Fimgs_shifted_nomask_host = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* xdim*ydim*zdim*2*sizeof(double) );

		cudaMemcpy(exp_local_Fimgs_shifted_host, exp_local_Fimgs_shifted_dev, exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* xdim*ydim*zdim*2*sizeof(double) , cudaMemcpyDeviceToHost);
		cudaMemcpy(exp_local_Fimgs_shifted_nomask_host, exp_local_Fimgs_shifted_nomask_dev, exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans* xdim*ydim*zdim*2*sizeof(double) , cudaMemcpyDeviceToHost);
		int img_size = newhdim*newdim*zdim;
		
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{

			long int part_id = exp_ipart_to_part_id[ipart];
			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			int my_trans_image = my_image_no * exp_nr_trans * exp_nr_oversampled_trans;
			for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
			{
				for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
				{
					exp_local_Fimgs_shifted[my_trans_image].resize(zdim, ydim, xdim); //= Fshifted;
					exp_local_Fimgs_shifted_nomask[my_trans_image].resize(zdim, ydim, xdim) ;//= Fshifted_nomask;
					
					memcpy(exp_local_Fimgs_shifted[my_trans_image].data, exp_local_Fimgs_shifted_host+my_trans_image*img_size*2, img_size*2*sizeof(double));		
					memcpy(exp_local_Fimgs_shifted_nomask[my_trans_image].data, exp_local_Fimgs_shifted_nomask_host+my_trans_image*img_size*2, img_size*2*sizeof(double));
					
					my_trans_image++;
				}
			}

		}

		free(exp_local_Fimgs_shifted_host);
		free(exp_local_Fimgs_shifted_nomask_host);
		//delete [] local_Faux_host;
		//delete [] local_Faux_nomask_host;
		double *local_Fctf_host, *local_Fctf_dev;
		int image_size_Fctf = exp_Fctfs[0].zyxdim;
		local_Fctf_host = (double *) malloc(exp_nr_images*image_size_Fctf*sizeof(double));
		
		for(int k = 0 ; k <exp_nr_images; k++ )
		{
			memcpy(local_Fctf_host+k*image_size_Fctf, exp_Fctfs[k].data, image_size_Fctf*sizeof(double)); 
		}
		cudaMalloc((void **)&local_Fctf_dev, image_size_Fctf*exp_nr_images*sizeof(double));
		cudaMemcpy(local_Fctf_dev,local_Fctf_host,image_size_Fctf*exp_nr_images*sizeof(double), cudaMemcpyHostToDevice);

		cudaMalloc((void **)&exp_local_Fctfs_dev, (exp_nr_images*mem_size*sizeof(double)));
		
		windowFourierTransform_gpu(local_Fctf_dev,
								 exp_local_Fctfs_dev,
								 exp_current_image_size,
								 exp_nr_images,
 								 2,
 								 xdim,
 								 ydim,
 								 zdim);
		double *local_Faux_host;
 		int fctf_image_size = exp_current_image_size*(exp_current_image_size/2+1);
		int yxdim_four  = exp_current_image_size;
		int xdim_four = exp_current_image_size/2+1;
		int size_fft = exp_nr_images*fctf_image_size;
		local_Faux_host = new double[size_fft];
		cudaMemcpy(local_Faux_host, exp_local_Fctfs_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
			// but some, e.g. the last, batch of pooled particles may be smaller
			//if (ipart >= exp_nr_particles)
			//	 break;

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			//MultidimArray<double> Fctf;
			///windowFourierTransform(exp_Fctfs[my_image_no], Fctf, exp_current_image_size);
			exp_local_Fctfs[my_image_no].resize(exp_current_image_size, exp_current_image_size/2+1);
			memcpy(exp_local_Fctfs[my_image_no].data,local_Faux_host+my_image_no*fctf_image_size,  fctf_image_size*sizeof(double));
			
		}
 								 
		

		int *group_id_host, *group_id_dev;
		group_id_host = (int *)malloc(exp_nr_images*sizeof(int));
		cudaMalloc((void **)&group_id_dev,exp_nr_images*sizeof(int) );
		// Get micrograph id (for choosing the right sigma2_noise)
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			// the exp_ipart_ThreadTaskDistributor was set with nr_pool,
			// but some, e.g. the last, batch of pooled particles may be smaller
			//if (ipart >= exp_nr_particles)
			//	 break;

			long int part_id = exp_ipart_to_part_id[ipart];

			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			
			int group_id = mydata.getGroupId(part_id, exp_iseries);
			
			group_id_host[ipart] = group_id;
			
		}
		// Wait until all threads are finsished

		//Calculate the  Minvsigma2 with GPU
		int *local_myMresol, *local_myMresol_dev;
		double *sigma2_noise_host, *sigma2_noise_dev;
		//int *group_id_host, *group_id_dev;
		int myMresol_size = (( ydim == coarse_size)? Mresol_coarse.zyxdim : Mresol_fine.zyxdim);
		int image_size = zdim*ydim*xdim;
		local_myMresol = (int *) malloc(myMresol_size*sizeof(int));
		sigma2_noise_host = (double*)malloc(mydata.numberOfGroups()*mymodel.sigma2_noise[0].zyxdim*sizeof(double));
		
		for(int i=0 ; i< myMresol_size; i++)
		{
			local_myMresol[i] = ( ydim == coarse_size) ? Mresol_coarse.data[i] : Mresol_fine.data[i];
		}

		for(int k=0; k < mydata.numberOfGroups(); k++)
		{
			for(int i=0 ; i <mymodel.sigma2_noise[0].zyxdim; i++ )
				sigma2_noise_host[k*mymodel.sigma2_noise[0].zyxdim+i] =  DIRECT_A1D_ELEM(mymodel.sigma2_noise[k], i);
			
		}
		//double *exp_Minvsigma2s_dev;
		cudaMalloc((void **)&local_myMresol_dev, myMresol_size*sizeof(int));
		cudaMalloc((void **)&sigma2_noise_dev,mydata.numberOfGroups()*mymodel.sigma2_noise[0].zyxdim*sizeof(double) );
		cudaMemcpy(local_myMresol_dev, local_myMresol, myMresol_size*sizeof(int), cudaMemcpyHostToDevice);
		cudaMemcpy(sigma2_noise_dev, sigma2_noise_host, mydata.numberOfGroups()*mymodel.sigma2_noise[0].zyxdim*sizeof(double), cudaMemcpyHostToDevice);
		cudaMemcpy(group_id_dev, group_id_host, exp_nr_images*sizeof(int), cudaMemcpyHostToDevice );
		
		cudaMalloc((void **)&exp_Minvsigma2s_dev, image_size*exp_nr_images*sizeof(double));
		
		cudaMemset(exp_Minvsigma2s_dev, 0.,image_size*exp_nr_images*sizeof(double));
		calculate_Minvsigma2_gpu(exp_Minvsigma2s_dev,  local_myMresol_dev, sigma2_noise_dev, group_id_dev,   sigma2_fudge,exp_nr_images,  zdim*ydim*xdim, myMresol_size, mymodel.sigma2_noise[0].zyxdim);

		//exp_Minvsigma2_host = (double*)malloc(local_Fimgs[0].zyxdim*exp_nr_images*sizeof(double));
		//cudaMemcpy(exp_Minvsigma2_host, exp_Minvsigma2_dev,local_Fimgs[0].zyxdim*exp_nr_images*sizeof(double),cudaMemcpyDeviceToHost);

		size_fft = exp_nr_images*image_size;
		local_Faux_host = new double[size_fft];
		cudaMemcpy(local_Faux_host,exp_Minvsigma2s_dev,size_fft*sizeof(double), cudaMemcpyDeviceToHost);
		for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
		{
			int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			exp_local_Minvsigma2s[my_image_no].resize(zdim, ydim, xdim);
			memcpy(exp_local_Minvsigma2s[my_image_no].data,local_Faux_host+my_image_no*image_size, image_size*sizeof(double) );
			
		}
		delete [] local_Faux_host;
		
#ifdef TIMING
		timer.toc(TIMING_DIFF_SHIFT);
#endif

	cudaFree(local_Fimgs_dev);
	cudaFree(local_Fimg_nomask_dev);
	cudaFree(local_myMresol_dev);
	cudaFree(sigma2_noise_dev);
	cudaFree(group_id_dev);
	cudaFree(local_Fctf_dev);
	
	free(local_Fctf_host);
	free(exp_local_sqrtXi2_host);
	free(oversampled_translations_host);
	free(group_id_host);
	free(sigma2_noise_host);
	free(local_myMresol);
	
}
*/



void MlOptimiser::doThreadGetSquaredDifferencesAllOrientations_syn()
{
#ifdef DEBUG_THREAD
		std::cerr << "entering doThreadGetAllSquaredDifferences" << std::endl;
#endif
	
		// Local variables
	  std::vector<double> thisthread_min_diff2;
	  std::vector< Matrix1D<double> > oversampled_orientations, oversampled_translations;
	  MultidimArray<Complex > Fimg, Fref, Frefctf, Fimg_shift;
	  MultidimArray<double> Fctf, Minvsigma2;
	  Matrix2D<double> A;
	
		// Initialise local mindiff2 for thread-safety
	  thisthread_min_diff2.clear();
	  thisthread_min_diff2.resize(exp_nr_particles, 99.e99);
	  Fref.resize(exp_local_Fimgs_shifted[0]);
	  Frefctf.resize(exp_local_Fimgs_shifted[0]);
	
		// THESE TWO FOR LOOPS WILL BE PARALLELISED USING THREADS...
		// exp_iclass loop does not always go from 0 to nr_classes!
	  long int iorientclass_offset = exp_iclass * exp_nr_rot;
	
	  size_t first_iorient = 0, last_iorient = 0;
	  long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
	  last_iorient = nr_orients;
		//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))
		{
			for (long int iorient = first_iorient; iorient < nr_orients; iorient++)
			{
	
				long int iorientclass = iorientclass_offset + iorient;
				long int idir = iorient / exp_nr_psi;
				long int ipsi = iorient % exp_nr_psi;
				// Get prior for this direction and skip calculation if prior==0
				double pdf_orientation;
				if (mymodel.orientational_prior_mode == NOPRIOR)
				{
#ifdef DEBUG_CHECKSIZES
					if (idir >= XSIZE(mymodel.pdf_direction[exp_iclass]))
					{
						std::cerr<< "idir= "<<idir<<" XSIZE(mymodel.pdf_direction[exp_iclass])= "<< XSIZE(mymodel.pdf_direction[exp_iclass]) <<std::endl;
						REPORT_ERROR("idir >= mymodel.pdf_direction[exp_iclass].size()");
					}
#endif
					pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
				}
				else
				{
					pdf_orientation = sampling.getPriorProbability(idir, ipsi);
				}
	
				// In the first pass, always proceed
				// In the second pass, check whether one of the translations for this orientation of any of the particles had a significant weight in the first pass
				// if so, proceed with projecting the reference in that direction
				bool do_proceed = (exp_ipass==0) ? true : isSignificantAnyParticleAnyTranslation(iorientclass);
	
				if (do_proceed && pdf_orientation > 0.)
				{
					// Now get the oversampled (rot, tilt, psi) triplets
					// This will be only the original (rot,tilt,psi) triplet in the first pass (exp_current_oversampling==0)
					sampling.getOrientations(idir, ipsi, exp_current_oversampling, oversampled_orientations);
	
#ifdef DEBUG_CHECKSIZES
					if (exp_nr_oversampled_rot != oversampled_orientations.size())
					{
						std::cerr<< "exp_nr_oversampled_rot= "<<exp_nr_oversampled_rot<<" oversampled_orientations.size()= "<< oversampled_orientations.size() <<std::endl;
						REPORT_ERROR("exp_nr_oversampled_rot != oversampled_orientations.size()");
					}
#endif
					// Loop over all oversampled orientations (only a single one in the first pass)
					for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++)
					{
	
						// Get the Euler matrix
						Euler_angles2matrix(XX(oversampled_orientations[iover_rot]),
											YY(oversampled_orientations[iover_rot]),
											ZZ(oversampled_orientations[iover_rot]), A);
	
						// Take tilt-series into account
						A = (exp_R_mic * A).inv();
	
						// Project the reference map (into Fref)
#ifdef TIMING
						// Only time one thread, as I also only time one MPI process
						//if (thread_id == 0)
							timer.tic(TIMING_DIFF_PROJ);
#endif
						(mymodel.PPref[exp_iclass]).get2DFourierTransform(Fref, A, IS_INV);
#ifdef TIMING
						// Only time one thread, as I also only time one MPI process
						//if (thread_id == 0)
							timer.toc(TIMING_DIFF_PROJ);
#endif
	
						/// Now that reference projection has been made loop over someParticles!
						for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
						{
							// loop over all particles inside this ori_particle
							for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
							{
								long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
	
								bool is_last_image_in_series = mydata.getNrImagesInSeries(part_id) == (exp_iseries + 1);
								// Which number was this image in the combined array of exp_iseries and part_id
								long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
	
#ifdef DEBUG_CHECKSIZES
								if (my_image_no >= exp_local_Minvsigma2s.size())
								{
									std::cerr<< "my_image_no= "<<my_image_no<<" exp_local_Minvsigma2s.size()= "<< exp_local_Minvsigma2s.size() <<std::endl;
									REPORT_ERROR("my_image_no >= exp_local_Minvsigma2.size()");
								}
#endif
								Minvsigma2 = exp_local_Minvsigma2s[my_image_no];
	
								// Apply CTF to reference projection
								if (do_ctf_correction && refs_are_ctf_corrected)
								{
	
#ifdef DEBUG_CHECKSIZES
									if (my_image_no >= exp_local_Fctfs.size())
									{
										std::cerr<< "my_image_no= "<<my_image_no<<" exp_local_Fctfs.size()= "<< exp_local_Fctfs.size() <<std::endl;
										REPORT_ERROR("my_image_no >= exp_local_Fctfs.size()");
									}
									if (MULTIDIM_SIZE(Fref) != MULTIDIM_SIZE(exp_local_Fctfs[my_image_no]))
									{
										std::cerr<< "MULTIDIM_SIZE(Fref)= "<<MULTIDIM_SIZE(Fref)<<" MULTIDIM_SIZE()= "<< MULTIDIM_SIZE(exp_local_Fctfs[my_image_no]) <<std::endl;
										REPORT_ERROR("MULTIDIM_SIZE(Fref) != MULTIDIM_SIZE(exp_local_Fctfs[my_image_no)");
									}
	
#endif
									FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Fref)
									{
										DIRECT_MULTIDIM_ELEM(Frefctf, n) = DIRECT_MULTIDIM_ELEM(Fref, n) * DIRECT_MULTIDIM_ELEM(exp_local_Fctfs[my_image_no], n);
									}
								}
								else
									Frefctf = Fref;
	
								if (do_scale_correction)
								{
									int group_id = mydata.getGroupId(part_id, exp_iseries);
#ifdef DEBUG_CHECKSIZES
									if (group_id >= mymodel.scale_correction.size())
									{
										std::cerr<< "group_id= "<<group_id<<" mymodel.scale_correction.size()= "<< mymodel.scale_correction.size() <<std::endl;
										REPORT_ERROR("group_id >= mymodel.scale_correction.size()");
									}
#endif
									double myscale = mymodel.scale_correction[group_id];
									FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Frefctf)
									{
										DIRECT_MULTIDIM_ELEM(Frefctf, n) *= myscale;
									}
								}
	
								for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
								{
									long int ihidden = iorientclass * exp_nr_trans + itrans;
	
#ifdef DEBUG_CHECKSIZES
									if (exp_ipass > 0 && ihidden >= XSIZE(exp_Mcoarse_significant))
									{
										std::cerr<< "ihidden= "<<ihidden<<" XSIZE(exp_Mcoarse_significant)= "<< XSIZE(exp_Mcoarse_significant) <<std::endl;
										REPORT_ERROR("ihidden >= XSIZE(exp_Mcoarse_significant)");
									}
#endif
									// In the first pass, always proceed
									// In the second pass, check whether this translations (&orientation) had a significant weight in the first pass
									bool do_proceed = (exp_ipass == 0) ? true : DIRECT_A2D_ELEM(exp_Mcoarse_significant, ipart, ihidden);
									if (do_proceed)
									{
	
										sampling.getTranslations(itrans, exp_current_oversampling, oversampled_translations);
										for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
										{
#ifdef TIMING
											// Only time one thread, as I also only time one MPI process
											//if (thread_id == 0)
												timer.tic(TIMING_DIFF_DIFF2);
#endif
											// Get the shifted image
											long int ishift = my_image_no * exp_nr_oversampled_trans * exp_nr_trans +
													itrans * exp_nr_oversampled_trans + iover_trans;
	
#ifdef DEBUG_CHECKSIZES
											if (ishift >= exp_local_Fimgs_shifted.size())
											{
												std::cerr<< "ishift= "<<ishift<<" exp_local_Fimgs_shifted.size()= "<< exp_local_Fimgs_shifted.size() <<std::endl;
												std::cerr << " itrans= " << itrans << std::endl;
												std::cerr << " ipart= " << ipart << std::endl;
												std::cerr << " exp_nr_oversampled_trans= " << exp_nr_oversampled_trans << " exp_nr_trans= " << exp_nr_trans << " iover_trans= " << iover_trans << std::endl;
												REPORT_ERROR("ishift >= exp_local_Fimgs_shifted.size()");
											}
#endif
	
											Fimg_shift = exp_local_Fimgs_shifted[ishift];
	//#define DEBUG_GETALLDIFF2
#ifdef DEBUG_GETALLDIFF2
											if (verb> 0)
											{
											FourierTransformer transformer;
											Image<double> tt;
											tt().resize(exp_current_image_size, exp_current_image_size);
											transformer.inverseFourierTransform(Fimg_shift, tt());
											CenterFFT(tt(),false);
											tt.write("Fimg_shift.spi");
											transformer.inverseFourierTransform(Frefctf, tt());
											CenterFFT(tt(),false);
											tt.write("Fref.spi");
											tt()=Minvsigma2;
											tt.write("Minvsigma2.spi");
											std::cerr << "written Minvsigma2.spi" << std::endl;
	
											char c;
											std::cerr << "Written Fimg_shift.spi and Fref.spi. Press any key to continue..." << std::endl;
											std::cin >> c;
											exit(1);
											}
#endif
	
											double diff2;
											if ((iter == 1 && do_firstiter_cc) || do_always_cc)
											{
												// Do not calculate squared-differences, but signal product
												// Negative values because smaller is worse in this case
												diff2 = 0.;
												double suma2 = 0.;
												FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Fimg_shift)
												{
													diff2 -= (DIRECT_MULTIDIM_ELEM(Frefctf, n)).real * (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).real;
													diff2 -= (DIRECT_MULTIDIM_ELEM(Frefctf, n)).imag * (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).imag;
													suma2 += norm(DIRECT_MULTIDIM_ELEM(Frefctf, n));
												}
												// Normalised cross-correlation coefficient: divide by power of reference (power of image is a constant)
												diff2 /= sqrt(suma2) * exp_local_sqrtXi2[my_image_no];
											}
											else
											{
	
#ifdef DEBUG_CHECKSIZES
												if (my_image_no >= exp_highres_Xi2_imgs.size())
												{
													std::cerr<< "my_image_no= "<<my_image_no<<" exp_highres_Xi2_imgs.size()= "<< exp_highres_Xi2_imgs.size() <<std::endl;
													REPORT_ERROR("my_image_no >= exp_highres_Xi2_imgs.size()");
												}
#endif
	
												// Calculate the actual squared difference term of the Gaussian probability function
												// If current_size < mymodel.ori_size diff2 is initialised to the sum of
												// all |Xij|2 terms that lie between current_size and ori_size
												// Factor two because of factor 2 in division below, NOT because of 2-dimensionality of the complex plane!
												diff2 = exp_highres_Xi2_imgs[my_image_no] / 2.;
												FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Fimg_shift)
												{
													double diff_real = (DIRECT_MULTIDIM_ELEM(Frefctf, n)).real - (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).real;
													double diff_imag = (DIRECT_MULTIDIM_ELEM(Frefctf, n)).imag - (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).imag;
													diff2 += (diff_real * diff_real + diff_imag * diff_imag) * 0.5 * DIRECT_MULTIDIM_ELEM(Minvsigma2, n);
												}
	
											}
	
#ifdef TIMING
											// Only time one thread, as I also only time one MPI process
											//if (thread_id == 0)
												timer.toc(TIMING_DIFF_DIFF2);
#endif
	
											// Store all diff2 in exp_Mweight
											long int ihidden_over = sampling.getPositionOversampledSamplingPoint(ihidden, exp_current_oversampling,
																											iover_rot, iover_trans);
	
	//#define DEBUG_DIFF2_ISNAN
#ifdef DEBUG_DIFF2_ISNAN
											if (std::isnan(diff2))
											{
												//global_mutex.lock();
												std::cerr << " ipart= " << ipart << std::endl;
												std::cerr << " diff2= " << diff2 << " thisthread_min_diff2[ipart]= " << thisthread_min_diff2[ipart] << " ipart= " << ipart << std::endl;
												std::cerr << " exp_highres_Xi2_imgs[my_image_no]= " << exp_highres_Xi2_imgs[my_image_no] << std::endl;
												std::cerr<< " exp_nr_oversampled_trans="<<exp_nr_oversampled_trans<<std::endl;
												std::cerr<< " exp_nr_oversampled_rot="<<exp_nr_oversampled_rot<<std::endl;
												std::cerr << " iover_rot= " << iover_rot << " iover_trans= " << iover_trans << " ihidden= " << ihidden << std::endl;
												std::cerr << " exp_current_oversampling= " << exp_current_oversampling << std::endl;
												std::cerr << " ihidden_over= " << ihidden_over << " XSIZE(Mweight)= " << XSIZE(exp_Mweight) << std::endl;
												int group_id = mydata.getGroupId(part_id, exp_iseries);
												std::cerr << " mymodel.scale_correction[group_id]= " << mymodel.scale_correction[group_id] << std::endl;
												if (std::isnan(mymodel.scale_correction[group_id]))
												{
													for (int i=0; i < mymodel.scale_correction.size(); i++)
														std::cerr << " i= " << i << " mymodel.scale_correction[i]= " << mymodel.scale_correction[i] << std::endl;
												}
												std::cerr << " group_id= " << group_id << std::endl;
												Image<double> It;
												It()=Minvsigma2;
												It.write("Minvsigma2.spi");
												std::cerr << "written Minvsigma2.spi" << std::endl;
												std::cerr << "Frefctf shape= "; Frefctf.printShape(std::cerr);
												std::cerr << "Fimg_shift shape= "; Fimg_shift.printShape(std::cerr);
												It()=exp_local_Fctfs[my_image_no];
												It.write("exp_local_Fctf.spi");
												std::cerr << "written exp_local_Fctf.spi" << std::endl;
												FourierTransformer transformer;
												Image<double> tt;
												tt().resize(exp_current_image_size, exp_current_image_size);
												transformer.inverseFourierTransform(Fimg_shift, tt());
												CenterFFT(tt(),false);
												tt.write("Fimg_shift.spi");
												std::cerr << "written Fimg_shift.spi" << std::endl;
												FourierTransformer transformer2;
												tt().initZeros();
												transformer2.inverseFourierTransform(Frefctf, tt());
												CenterFFT(tt(),false);
												tt.write("Frefctf.spi");
												std::cerr << "written Frefctf.spi" << std::endl;
												FourierTransformer transformer3;
												tt().initZeros();
												transformer3.inverseFourierTransform(Fref, tt());
												CenterFFT(tt(),false);
												tt.write("Fref.spi");
												std::cerr << "written Fref.spi" << std::endl;
												std::cerr << " A= " << A << std::endl;
												std::cerr << " exp_R_mic= " << exp_R_mic << std::endl;
												std::cerr << "written Frefctf.spi" << std::endl;
												REPORT_ERROR("diff2 is not a number");
												//global_mutex.unlock();
											}
#endif
	//#define DEBUG_VERBOSE
#ifdef DEBUG_VERBOSE
											//global_mutex.lock();
											if (verb > 0)
											{
												std::cout << " rot= " << XX(oversampled_orientations[iover_rot]) << " tilt= "<< YY(oversampled_orientations[iover_rot]) << " psi= " << ZZ(oversampled_orientations[iover_rot]) << std::endl;
												std::cout << " xoff= " <<XX(oversampled_translations[iover_trans]) <<" yoff= "<<YY(oversampled_translations[iover_trans])<<std::endl;
												std::cout << " ihidden_over= " << ihidden_over << " diff2= " << diff2 << " thisthread_min_diff2[ipart]= " << thisthread_min_diff2[ipart] << std::endl;
											}
											//global_mutex.unlock();
#endif
#ifdef DEBUG_CHECKSIZES
											if (ihidden_over >= XSIZE(exp_Mweight) )
											{
												std::cerr<< " exp_nr_oversampled_trans="<<exp_nr_oversampled_trans<<std::endl;
												std::cerr<< " exp_nr_oversampled_rot="<<exp_nr_oversampled_rot<<std::endl;
												std::cerr << " iover_rot= " << iover_rot << " iover_trans= " << iover_trans << " ihidden= " << ihidden << std::endl;
												std::cerr << " exp_current_oversampling= " << exp_current_oversampling << std::endl;
												std::cerr << " ihidden_over= " << ihidden_over << " XSIZE(Mweight)= " << XSIZE(exp_Mweight) << std::endl;
												REPORT_ERROR("ihidden_over >= XSIZE(Mweight)");
											}
#endif
	
											if (exp_iseries == 0)
												DIRECT_A2D_ELEM(exp_Mweight, ipart, ihidden_over) = diff2;
											else
												DIRECT_A2D_ELEM(exp_Mweight, ipart, ihidden_over) += diff2;
	
#ifdef DEBUG_CHECKSIZES
											if (ipart >= thisthread_min_diff2.size())
											{
												std::cerr<< "ipart= "<<ipart<<" thisthread_min_diff2.size()= "<< thisthread_min_diff2.size() <<std::endl;
												REPORT_ERROR("ipart >= thisthread_min_diff2.size() ");
											}
#endif
											// Keep track of minimum of all diff2, only for the last image in this series
											diff2 = DIRECT_A2D_ELEM(exp_Mweight, ipart, ihidden_over);
											//std::cerr << " exp_ipass= " << exp_ipass << " exp_iclass= " << exp_iclass << " diff2= " << diff2 << std::endl;
											if (is_last_image_in_series && diff2 < thisthread_min_diff2[ipart])
												thisthread_min_diff2[ipart] = diff2;
	
										} // end loop iover_trans
									} // end if do_proceed translations
								} // end loop itrans
							} // end loop part_id (i)
						} // end loop ori_part_id
					}// end loop iover_rot
				} // end if do_proceed orientations
			} // end loop iorient
		} // end while task distribution
	
	
		// Now inside a mutex set the minimum of the squared differences among all threads
#ifdef DEBUG_CHECKSIZES
		if (thisthread_min_diff2.size() != exp_min_diff2.size())
		{
			std::cerr<< "thisthread_min_diff2.size()= "<<thisthread_min_diff2.size()<<" exp_min_diff2.size()= "<< exp_min_diff2.size() <<std::endl;
			REPORT_ERROR("thisthread_min_diff2.size() != exp_min_diff2.size()");
		}
#endif
	
		//global_mutex.lock();
		for (int i = 0; i < exp_min_diff2.size(); i++)
		{
			if (thisthread_min_diff2[i] < exp_min_diff2[i])
			{
				exp_min_diff2[i] = thisthread_min_diff2[i];
			}
		}
		for (int i = 0; i <5; i++)
			std::cout << exp_min_diff2[i] << " ";
		std::cout <<"iter is :"<< iter << std::endl; 
	
#ifdef DEBUG_THREAD
		std::cerr << "leaving doThreadGetAllSquaredDifferences" << std::endl;
#endif
}

/*
void MlOptimiser::doThreadGetSquaredDifferencesAllOrientations_gpu()
{

TIMER_START

	long t1, t2, t3, t4;
	t1 = t2 = t3 = t4 = 0;
	struct timeval start, end;
gettimeofday(&start, NULL);			
    // Local variables
	std::vector<double> thisthread_min_diff2;
	std::vector< Matrix1D<double> > oversampled_orientations, oversampled_translations;
	MultidimArray<Complex > Fref;
	MultidimArray<double> Fctf, Minvsigma2;
	Matrix2D<double> A;

    // Initialise local mindiff2 for thread-safety
	thisthread_min_diff2.clear();
	thisthread_min_diff2.resize(exp_nr_particles, 99.e99);
	Fref.resize(exp_local_Fimgs_shifted[0]);
	Fref.resize(exp_local_Fimgs_shifted[0]);

	int image_size = exp_local_Fimgs_shifted[0].zyxdim;
			
	image_size = exp_local_Fimgs_shifted[0].zyxdim;

	cufftDoubleComplex *Fref_dev;
	cudaMalloc((void**)&Fref_dev, exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex));
 
	//double *diff2_host = new double[exp_nr_particles*exp_nr_trans*exp_nr_oversampled_trans];
	double *diff2_dev;
	cudaMalloc((void**)&diff2_dev, exp_nr_particles*exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans*sizeof(double));

	double *myscale_host = new double[exp_nr_particles];
	double *myscale_dev;
	cudaMalloc((void**)&myscale_dev, exp_nr_particles*sizeof(double));


	bool *do_proceed_host = new bool[exp_nr_particles*exp_nr_trans];
	bool *do_proceed_dev;
	cudaMalloc((void**)&do_proceed_dev, exp_nr_particles*exp_nr_trans*sizeof(bool));

	bool *is_last_image_in_series_host = new bool[exp_nr_particles];
	bool *is_last_image_in_series_dev;
	cudaMalloc((void**)&is_last_image_in_series_dev, exp_nr_particles*sizeof(bool));
	long int *my_image_no_list = new long[exp_nr_particles];

	cufftDoubleComplex *Frefctf_dev;
	cudaMalloc((void**)&Frefctf_dev, exp_nr_particles*exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex));
						
	double *exp_highres_Xi2_imgs_host = new double[exp_nr_particles];
	double *exp_highres_Xi2_imgs_dev;
	cudaMalloc((void**)&exp_highres_Xi2_imgs_dev, exp_nr_particles*sizeof(double));


TIMER_END

    // THESE TWO FOR LOOPS WILL BE PARALLELISED USING THREADS...
	// exp_iclass loop does not always go from 0 to nr_classes!
	long int iorientclass_offset = exp_iclass * exp_nr_rot;

    size_t first_iorient = 0, last_iorient = 0;
	long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
	last_iorient = nr_orients - 1;
	//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))

	for (long int iorient = first_iorient; iorient <= last_iorient; iorient++)
	{
TIMER_START

		long int iorientclass = iorientclass_offset + iorient;
		long int idir = iorient / exp_nr_psi;
		long int ipsi = iorient % exp_nr_psi;
		// Get prior for this direction and skip calculation if prior==0
		double pdf_orientation;
		if (mymodel.orientational_prior_mode == NOPRIOR)
		{
			pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
		}
		else
		{
			pdf_orientation = sampling.getPriorProbability(idir, ipsi);
		}
TIMER_END

		// In the first pass, always proceed
		// In the second pass, check whether one of the translations for this orientation of any of the particles had a significant weight in the first pass
		// if so, proceed with projecting the reference in that direction
		bool do_proceed = (exp_ipass==0) ? true : isSignificantAnyParticleAnyTranslation(iorientclass);

		if (do_proceed && pdf_orientation > 0.)
		{
			// Now get the oversampled (rot, tilt, psi) triplets
			sampling.getOrientations(idir, ipsi, exp_current_oversampling, oversampled_orientations);

			/// Now that reference projection has been made loop over someParticles!
			for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
			{
				// loop over all particles inside this ori_particle

				for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
				{
TIMER_START					
					long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];

					bool is_last_image_in_series = mydata.getNrImagesInSeries(part_id) == (exp_iseries + 1);
					// Which number was this image in the combined array of exp_iseries and part_id
					long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;

					is_last_image_in_series_host[ipart] = is_last_image_in_series;

					my_image_no_list[ipart] = my_image_no;

					if (do_scale_correction)
					{
						myscale_host[ipart] = mymodel.scale_correction[mydata.getGroupId(part_id, exp_iseries)];

					}

					if ((iter == 1 && do_firstiter_cc) || do_always_cc)
					{
						//exp_local_sqrtXi2_host[ipart] = exp_local_sqrtXi2[my_image_no_list[ipart]];
					}
					else
					{
						exp_highres_Xi2_imgs_host[ipart] = exp_highres_Xi2_imgs[my_image_no_list[ipart]];
					}
TIMER_END
TIMER_START

					for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
					{
						long int ihidden = iorientclass * exp_nr_trans + itrans;
						// In the first pass, always proceed
						// In the second pass, check whether this translations (&orientation) had a significant weight in the first pass
						do_proceed_host[ipart*exp_nr_trans + itrans] = (exp_ipass == 0) ? true : DIRECT_A2D_ELEM(exp_Mcoarse_significant, ipart, ihidden);

					}
TIMER_END
				}
			}
TIMER_START
			if (do_scale_correction)
			{
				cudaMemcpy(myscale_dev, myscale_host, exp_nr_particles*sizeof(double), cudaMemcpyHostToDevice);
			}

			cudaMemcpy(do_proceed_dev, do_proceed_host, exp_nr_particles*exp_nr_trans * sizeof(bool), cudaMemcpyHostToDevice);

			if ((iter == 1 && do_firstiter_cc) || do_always_cc)
			{
			}
			else
			{
				cudaMemcpy(exp_highres_Xi2_imgs_dev, exp_highres_Xi2_imgs_host, exp_nr_particles*sizeof(double), cudaMemcpyHostToDevice);
			}
TIMER_END
			for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++)
			{
TIMER_START
				// Get the Euler matrix
				Euler_angles2matrix(XX(oversampled_orientations[iover_rot]),
									YY(oversampled_orientations[iover_rot]),
									ZZ(oversampled_orientations[iover_rot]), A);
TIMER_END
TIMER_START
				// Take tilt-series into account
				A = (exp_R_mic * A).inv();
TIMER_END
TIMER_START
				// Project the reference map (into Fref)
				(mymodel.PPref[exp_iclass]).get2DFourierTransform(Fref, A, IS_INV);
TIMER_END
TIMER_START
				cudaMemcpy(&Fref_dev[iover_rot*image_size], Fref.data, image_size*sizeof(cufftDoubleComplex), cudaMemcpyHostToDevice);
TIMER_END
			}
			gettimeofday(&start, NULL);	
TIMER_START
			calculate_frefctf_gpu(Frefctf_dev, Fref_dev, exp_local_Fctfs_dev, myscale_dev, exp_nr_particles, exp_nr_oversampled_rot, image_size, do_ctf_correction && refs_are_ctf_corrected, do_scale_correction);
			gettimeofday(&end, NULL);
			t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
TIMER_END

TIMER_START
			if ((iter == 1 && do_firstiter_cc) || do_always_cc)
			{
				calculate_diff2_no_do_squared_difference_gpu(diff2_dev, Frefctf_dev, exp_local_Fimgs_shifted_dev, exp_local_sqrtXi2_dev, do_proceed_dev, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, image_size);
			}
			else
			{
				calculate_diff2_do_squared_difference_gpu(diff2_dev, Frefctf_dev, exp_local_Fimgs_shifted_dev, exp_highres_Xi2_imgs_dev, exp_Minvsigma2s_dev, do_proceed_dev, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, image_size);
			}
TIMER_END
TIMER_START
			cudaMemcpy(is_last_image_in_series_dev, is_last_image_in_series_host, exp_nr_particles*sizeof(bool), cudaMemcpyHostToDevice);
TIMER_END
TIMER_START
			calculate_exp_mweight_gpu(exp_Mweight_dev, exp_min_diff2_dev, diff2_dev, do_proceed_dev, is_last_image_in_series_dev, iorientclass, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, exp_Mweight_dev_size, exp_iseries);
TIMER_END		

		} // end if do_proceed orientations
	} // end loop iorient


gettimeofday(&end, NULL);
t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
//	std::cout << "doThreadGetSquaredDifferencesAllOrientations_gpu " <<  t1/1000000. << " " << t2/1000000. << " " << t3/1000000. << " " << t4/1000000. << std::endl;

TIMER_START

	//delete[] diff2_host;
	delete[] myscale_host;
	delete[] do_proceed_host;
	delete[] is_last_image_in_series_host;
	delete[] my_image_no_list;
	delete[] exp_highres_Xi2_imgs_host;
	//delete[] thisthread_min_diff2_host;



    cudaFree(Fref_dev);
	cudaFree(diff2_dev);
	cudaFree(myscale_dev);
	cudaFree(do_proceed_dev);
	cudaFree(Frefctf_dev);
	cudaFree(exp_highres_Xi2_imgs_dev);
	cudaFree(is_last_image_in_series_dev);
	//cudaFree(thisthread_min_diff2_dev);
TIMER_END

}

*/
void MlOptimiser::doThreadGetSquaredDifferencesAllOrientations_gpu()
{

TIMER_START

	long t1, t2, t3, t4;
	t1 = t2 = t3 = t4 = 0;
	struct timeval start, end;
gettimeofday(&start, NULL);			
    // Local variables
	std::vector<double> thisthread_min_diff2;
	std::vector< Matrix1D<double> > oversampled_orientations, oversampled_translations;
	MultidimArray<Complex > Fref;
	MultidimArray<double> Fctf, Minvsigma2;
	Matrix2D<double> A;

    // Initialise local mindiff2 for thread-safety
	thisthread_min_diff2.clear();
	thisthread_min_diff2.resize(exp_nr_particles, 99.e99);
	Fref.resize(exp_local_Fimgs_shifted[0]);
	Fref.resize(exp_local_Fimgs_shifted[0]);

	int image_size = exp_local_Fimgs_shifted[0].zyxdim;
			
	image_size = exp_local_Fimgs_shifted[0].zyxdim;

	cufftDoubleComplex *Fref_dev;
	cudaMalloc((void**)&Fref_dev, exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex));
 
	//double *diff2_host = new double[exp_nr_particles*exp_nr_trans*exp_nr_oversampled_trans];
	double *diff2_dev;
	cudaMalloc((void**)&diff2_dev, exp_nr_particles*exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans*sizeof(double));

	double *myscale_host = new double[exp_nr_particles];
	double *myscale_dev;
	cudaMalloc((void**)&myscale_dev, exp_nr_particles*sizeof(double));


	bool *do_proceed_host = new bool[exp_nr_particles*exp_nr_trans];
	bool *do_proceed_dev;
	cudaMalloc((void**)&do_proceed_dev, exp_nr_particles*exp_nr_trans*sizeof(bool));

	bool *is_last_image_in_series_host = new bool[exp_nr_particles];
	bool *is_last_image_in_series_dev;
	cudaMalloc((void**)&is_last_image_in_series_dev, exp_nr_particles*sizeof(bool));
	long int *my_image_no_list = new long[exp_nr_particles];

	cufftDoubleComplex *Frefctf_dev;
	cudaMalloc((void**)&Frefctf_dev, exp_nr_particles*exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex));
						
	double *exp_highres_Xi2_imgs_host = new double[exp_nr_particles];
	double *exp_highres_Xi2_imgs_dev;
	cudaMalloc((void**)&exp_highres_Xi2_imgs_dev, exp_nr_particles*sizeof(double));

	long int iorientclass_offset = exp_iclass * exp_nr_rot;

    size_t first_iorient = 0, last_iorient = 0;
	long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
	last_iorient = nr_orients - 1;

	double *A_dev;
	int nr_A = 0, iA = 0;

	cufftDoubleComplex *data_dev;
	cufftDoubleComplex *Fref_all_dev;
	int f2d_x, f2d_y, data_x, data_y, data_z, data_starty, data_startz;

	if ((exp_ipass == 0) && (mymodel.orientational_prior_mode == NOPRIOR)) {

	} else {
		cudaMalloc((void**)&A_dev, nr_orients*exp_nr_oversampled_rot*9*sizeof(double));
		cudaMalloc((void**)&Fref_all_dev, nr_orients*exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex));
		cudaMemset(Fref_all_dev, 0., nr_orients*exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex));
		cudaMalloc((void**)&data_dev,  (mymodel.PPref[exp_iclass]).data.zyxdim*sizeof(cufftDoubleComplex));
		cudaMemcpy(data_dev, (mymodel.PPref[exp_iclass]).data.data, (mymodel.PPref[exp_iclass]).data.zyxdim*2*sizeof(double), cudaMemcpyHostToDevice);
		//printf("%d %d %d %d %d %d %d\n", nr_orients, exp_nr_oversampled_rot, image_size, NSIZE((mymodel.PPref[exp_iclass]).data), XSIZE((mymodel.PPref[exp_iclass]).data), YSIZE((mymodel.PPref[exp_iclass]).data), ZSIZE((mymodel.PPref[exp_iclass]).data));


		f2d_x = XSIZE(exp_local_Fimgs_shifted[0]);
		f2d_y = YSIZE(exp_local_Fimgs_shifted[0]);

		data_x = XSIZE(mymodel.PPref[exp_iclass].data);
		data_y = YSIZE(mymodel.PPref[exp_iclass].data);
		data_z = ZSIZE(mymodel.PPref[exp_iclass].data);

		data_starty = STARTINGY(mymodel.PPref[exp_iclass].data);
		data_startz = STARTINGZ(mymodel.PPref[exp_iclass].data);

		//printf("%d %d %d %d %d %d %d\n", f2d_x, f2d_y, data_x, data_y, data_z, data_starty, data_startz);
	}

TIMER_END
    // THESE TWO FOR LOOPS WILL BE PARALLELISED USING THREADS...
	// exp_iclass loop does not always go from 0 to nr_classes!

	//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))

	for (long int iorient = first_iorient; iorient <= last_iorient; iorient++)
	{
TIMER_START
		long int iorientclass = iorientclass_offset + iorient;
		long int idir = iorient / exp_nr_psi;
		long int ipsi = iorient % exp_nr_psi;
		// Get prior for this direction and skip calculation if prior==0
		double pdf_orientation;
		if (mymodel.orientational_prior_mode == NOPRIOR)
		{
			pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
		}
		else
		{
			pdf_orientation = sampling.getPriorProbability(idir, ipsi);
		}
TIMER_END

		// In the first pass, always proceed
		// In the second pass, check whether one of the translations for this orientation of any of the particles had a significant weight in the first pass
		// if so, proceed with projecting the reference in that direction
		bool do_proceed = (exp_ipass==0) ? true : isSignificantAnyParticleAnyTranslation(iorientclass);

		if (do_proceed && pdf_orientation > 0.)
		{
			// Now get the oversampled (rot, tilt, psi) triplets
			sampling.getOrientations(idir, ipsi, exp_current_oversampling, oversampled_orientations);

			/// Now that reference projection has been made loop over someParticles!
			for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
			{
				// loop over all particles inside this ori_particle

				for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
				{
TIMER_START
					long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];

					bool is_last_image_in_series = mydata.getNrImagesInSeries(part_id) == (exp_iseries + 1);
					// Which number was this image in the combined array of exp_iseries and part_id
					long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;

					is_last_image_in_series_host[ipart] = is_last_image_in_series;

					my_image_no_list[ipart] = my_image_no;

					if (do_scale_correction)
					{
						myscale_host[ipart] = mymodel.scale_correction[mydata.getGroupId(part_id, exp_iseries)];

					}

					if ((iter == 1 && do_firstiter_cc) || do_always_cc)
					{
						//exp_local_sqrtXi2_host[ipart] = exp_local_sqrtXi2[my_image_no_list[ipart]];
					}
					else
					{
						exp_highres_Xi2_imgs_host[ipart] = exp_highres_Xi2_imgs[my_image_no_list[ipart]];
					}
TIMER_END
				}
			}
TIMER_START
			if (do_scale_correction)
			{
				cudaMemcpy(myscale_dev, myscale_host, exp_nr_particles*sizeof(double), cudaMemcpyHostToDevice);
			}


			if ((iter == 1 && do_firstiter_cc) || do_always_cc)
			{
			}
			else
			{
				cudaMemcpy(exp_highres_Xi2_imgs_dev, exp_highres_Xi2_imgs_host, exp_nr_particles*sizeof(double), cudaMemcpyHostToDevice);
			}
TIMER_END
TIMER_START
			cudaMemcpy(is_last_image_in_series_dev, is_last_image_in_series_host, exp_nr_particles*sizeof(bool), cudaMemcpyHostToDevice);
TIMER_END
			break;

		} // end if do_proceed orientations
	} // end loop iorient

	//printf("%d %d %d %d\n", exp_ipass, mymodel.orientational_prior_mode, nr_orients, exp_nr_oversampled_rot);
	if ((exp_ipass == 0) && (mymodel.orientational_prior_mode == NOPRIOR)) {
		if (first_get2d) {
TIMER_START
			first_get2d = false;
			value_get2d = new double[nr_orients*exp_nr_oversampled_rot*image_size*2];
			memset(value_get2d, 0, nr_orients*exp_nr_oversampled_rot*image_size*2*sizeof(double));
			for (long int iorient = first_iorient; iorient <= last_iorient; iorient++) {
				long int iorientclass = iorientclass_offset + iorient;
				long int idir = iorient / exp_nr_psi;
				long int ipsi = iorient % exp_nr_psi;
				// Get prior for this direction and skip calculation if prior==0
				sampling.getOrientations(idir, ipsi, exp_current_oversampling, oversampled_orientations);

				double pdf_orientation;
				pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);

				for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++) {
					// Get the Euler matrix
					Euler_angles2matrix(XX(oversampled_orientations[iover_rot]),
										YY(oversampled_orientations[iover_rot]),
										ZZ(oversampled_orientations[iover_rot]), A);

					// Take tilt-series into account
					A = (exp_R_mic * A).inv();

					// Project the reference map (into Fref)
					(mymodel.PPref[exp_iclass]).get2DFourierTransform(Fref, A, IS_INV); 

					memcpy(&value_get2d[(iorient*exp_nr_oversampled_rot + iover_rot)*image_size*2], Fref.data, image_size*2*sizeof(double));
				}
			}
TIMER_END
		}
	} else {
		for (long int iorient = first_iorient; iorient <= last_iorient; iorient++)
		{
TIMER_START

			long int iorientclass = iorientclass_offset + iorient;
			long int idir = iorient / exp_nr_psi;
			long int ipsi = iorient % exp_nr_psi;
			// Get prior for this direction and skip calculation if prior==0
			double pdf_orientation;
			if (mymodel.orientational_prior_mode == NOPRIOR)
			{
				pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
			}
			else
			{
				pdf_orientation = sampling.getPriorProbability(idir, ipsi);
			}
TIMER_END

			// In the first pass, always proceed
			// In the second pass, check whether one of the translations for this orientation of any of the particles had a significant weight in the first pass
			// if so, proceed with projecting the reference in that direction
			bool do_proceed = (exp_ipass==0) ? true : isSignificantAnyParticleAnyTranslation(iorientclass);

			if (do_proceed && pdf_orientation > 0.)
			{
				// Now get the oversampled (rot, tilt, psi) triplets
				sampling.getOrientations(idir, ipsi, exp_current_oversampling, oversampled_orientations);
				for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++)
				{

TIMER_START
					// Get the Euler matrix
					Euler_angles2matrix(XX(oversampled_orientations[iover_rot]),
										YY(oversampled_orientations[iover_rot]),
										ZZ(oversampled_orientations[iover_rot]), A);
TIMER_END
TIMER_START
					// Take tilt-series into account
					A = (exp_R_mic * A).inv();
					A *= (double)(mymodel.PPref[exp_iclass]).padding_factor;
TIMER_END
TIMER_START
					// Project the reference map (into Fref)
					//(mymodel.PPref[exp_iclass]).get2DFourierTransform(Fref, A, IS_INV); 
					cudaMemcpy(&A_dev[nr_A*9], A.mdata, 9*sizeof(double), cudaMemcpyHostToDevice);
					nr_A++;

TIMER_END
						//int index_rot=iorient*exp_nr_oversampled_rot+iover_rot; nr_get2d[index_rot]++; if (nr_get2d[index_rot]==1) value_get2d[index_rot]=Fref.data[3].real; if (fabs(Fref.data[3].real-value_get2d[index_rot])>1e-5) {printf("%f %f exit!\n", Fref.data[3].real, value_get2d[index_rot]); exit(1);} }
					//printf("%d %d %d ", exp_ipass, iorient, iover_rot); printf("%d %d %d ", idir, ipsi, exp_current_oversampling); printf("%f %f %f ", XX(oversampled_orientations[iover_rot]), YY(oversampled_orientations[iover_rot]), ZZ(oversampled_orientations[iover_rot])); for (int i = 0; i < 10; i++) printf("%f ", Fref.data[i].real); printf("\n");
				}
			}
		}
TIMER_START
		calculate_A_gpu(Fref_all_dev, A_dev, data_dev, (mymodel.PPref[exp_iclass]).r_max, (mymodel.PPref[exp_iclass]).r_min_nn, f2d_x, f2d_y, data_x, data_y, data_z, data_starty, data_startz, nr_A);
cudaDeviceSynchronize();
TIMER_END
	}

	for (long int iorient = first_iorient; iorient <= last_iorient; iorient++)
	{
TIMER_START

		long int iorientclass = iorientclass_offset + iorient;
		long int idir = iorient / exp_nr_psi;
		long int ipsi = iorient % exp_nr_psi;
		// Get prior for this direction and skip calculation if prior==0
		double pdf_orientation;
		if (mymodel.orientational_prior_mode == NOPRIOR)
		{
			pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
		}
		else
		{
			pdf_orientation = sampling.getPriorProbability(idir, ipsi);
		}

TIMER_END

		// In the first pass, always proceed
		// In the second pass, check whether one of the translations for this orientation of any of the particles had a significant weight in the first pass
		// if so, proceed with projecting the reference in that direction
		bool do_proceed = (exp_ipass==0) ? true : isSignificantAnyParticleAnyTranslation(iorientclass);

		if (do_proceed && pdf_orientation > 0.)
		{
			// Now get the oversampled (rot, tilt, psi) triplets
			sampling.getOrientations(idir, ipsi, exp_current_oversampling, oversampled_orientations);

			/// Now that reference projection has been made loop over someParticles!
			for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
			{
				// loop over all particles inside this ori_particle

				for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
				{
TIMER_START
					for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
					{
						long int ihidden = iorientclass * exp_nr_trans + itrans;
						// In the first pass, always proceed
						// In the second pass, check whether this translations (&orientation) had a significant weight in the first pass
						do_proceed_host[ipart*exp_nr_trans + itrans] = (exp_ipass == 0) ? true : DIRECT_A2D_ELEM(exp_Mcoarse_significant, ipart, ihidden);

					}
TIMER_END
				}
			}
TIMER_START
			cudaMemcpy(do_proceed_dev, do_proceed_host, exp_nr_particles*exp_nr_trans * sizeof(bool), cudaMemcpyHostToDevice);
TIMER_END
			if ((exp_ipass == 0) && (mymodel.orientational_prior_mode == NOPRIOR)) {
TIMER_START
				cudaMemcpy(Fref_dev, &value_get2d[iorient*exp_nr_oversampled_rot*image_size*2], exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex), cudaMemcpyHostToDevice);
TIMER_END
			} else {
TIMER_START
				cudaMemcpy(Fref_dev, &Fref_all_dev[iA*exp_nr_oversampled_rot*image_size], exp_nr_oversampled_rot*image_size*sizeof(cufftDoubleComplex), cudaMemcpyDeviceToDevice);
				iA++; 
TIMER_END
			}
			gettimeofday(&start, NULL);	

TIMER_START
			/*
			double *Fref_host = new double[exp_nr_oversampled_rot*image_size*2];
			cudaMemcpy(Fref_host, Fref_dev, exp_nr_oversampled_rot * image_size * 2 * sizeof(double), cudaMemcpyDeviceToHost);
			for (int i = 0; i < 10; i++) printf("%f ", Fref_host[i]); printf("\n");
			*/
			calculate_frefctf_gpu(Frefctf_dev, Fref_dev, exp_local_Fctfs_dev, myscale_dev, exp_nr_particles, exp_nr_oversampled_rot, image_size, do_ctf_correction && refs_are_ctf_corrected, do_scale_correction);
cudaDeviceSynchronize();
			gettimeofday(&end, NULL);
			t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
TIMER_END

TIMER_START
			if ((iter == 1 && do_firstiter_cc) || do_always_cc)
			{
				calculate_diff2_no_do_squared_difference_gpu(diff2_dev, Frefctf_dev, exp_local_Fimgs_shifted_dev, exp_local_sqrtXi2_dev, do_proceed_dev, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, image_size);
			}
			else
			{
				calculate_diff2_do_squared_difference_gpu(diff2_dev, Frefctf_dev, exp_local_Fimgs_shifted_dev, exp_highres_Xi2_imgs_dev, exp_Minvsigma2s_dev, do_proceed_dev, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, image_size);
			}
cudaDeviceSynchronize();
TIMER_END

TIMER_START
			calculate_exp_mweight_gpu(exp_Mweight_dev, exp_min_diff2_dev, diff2_dev, do_proceed_dev, is_last_image_in_series_dev, iorientclass, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, exp_Mweight_dev_size, exp_iseries);
cudaDeviceSynchronize();
TIMER_END		

		} // end if do_proceed orientations
	} // end loop iorient	


gettimeofday(&end, NULL);
t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);
//	std::cout << "doThreadGetSquaredDifferencesAllOrientations_gpu " <<  t1/1000000. << " " << t2/1000000. << " " << t3/1000000. << " " << t4/1000000. << std::endl;

TIMER_START

	delete[] myscale_host;
	delete[] do_proceed_host;
	delete[] is_last_image_in_series_host;
	delete[] my_image_no_list;
	delete[] exp_highres_Xi2_imgs_host;



    cudaFree(Fref_dev);
	cudaFree(diff2_dev);
	cudaFree(myscale_dev);
	cudaFree(do_proceed_dev);
	cudaFree(Frefctf_dev);
	cudaFree(exp_highres_Xi2_imgs_dev);
	cudaFree(is_last_image_in_series_dev);

	if ((exp_ipass == 0) && (mymodel.orientational_prior_mode == NOPRIOR)) {

	} else {
		cudaFree(A_dev);
		cudaFree(Fref_all_dev);
		cudaFree(data_dev);
	}	
TIMER_END

}

void MlOptimiser::convertAllSquaredDifferencesToWeights_gpu()
{
	// Convert the squared differences into weights
	// Note there is only one weight for each part_id, because a whole series of images is treated as one particle
	//std::cout << "entering ConvertSquaredDifferencesToWeights_gpu" << std::endl;
	// Initialising...

t_convert_cpu.start();
	exp_sum_weight.resize(exp_nr_particles);
	for (int i = 0; i < exp_nr_particles; i++)
		exp_sum_weight[i] = 0.;

	//TMP DEBUGGING
	//DEBUGGING_COPY_exp_Mweight = exp_Mweight;

	// Loop from iclass_min to iclass_max to deal with seed generation in first iteration
	exp_iimage = 0;
	exp_ipart = 0;
	/******
	for (long int ori_part_id = exp_my_first_ori_particle; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	{
		// loop over all particles inside this ori_particle
		for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, exp_ipart++)
		{
			exp_part_id = mydata.ori_particles[ori_part_id].particles_id[i];
			exp_thisparticle_sumweight = 0.;
			long int nr_exp_ipart = mydata.ori_particles[ori_part_id].particles_id.size(); //loop size 1
	******/
t_convert_cpu.stop();

	if ((iter == 1 && do_firstiter_cc) || do_always_cc)
	{
		cudaError_t cudaStat;

t_convert_cpu.start();
    	exp_Mweight.resize(exp_nr_particles, mymodel.nr_classes * sampling.NrSamplingPoints(exp_current_oversampling, false));
t_convert_cpu.stop();

t_convert_prepare.start();
		cudaStat = cudaMemcpy(exp_Mweight.data, exp_Mweight_dev, exp_nr_particles * exp_Mweight_dev_size*sizeof(double), cudaMemcpyDeviceToHost);
t_convert_prepare.stop();

		if (cudaStat != cudaSuccess) 
		{
	    	printf ("device memory copy 1 failed, exp_Mweight_dev");
    	//return EXIT_FAILURE;
    	}

t_convert_cpu.start();
		for (long int ori_part_id = exp_my_first_ori_particle; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
		{
			// loop over all particles inside this ori_particle
			for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, exp_ipart++)
			{
				exp_part_id = mydata.ori_particles[ori_part_id].particles_id[i];
				exp_thisparticle_sumweight = 0.;
				//nr_particles = exp_nr_ori_particles * mydata.ori_particles[ori_part_id].particles_id.size();
					// Binarize the squared differences array to skip marginalisation
					// Note this loop is not threaded. This is not so important because it will only be executed in the 1st iteration and is fast anyway
				double mymindiff2 = 99.e10, mymaxprob = -99.e10;
				long int myminidx = -1;
				// Find the smallest element in this row of exp_Mweight
				for (long int i = 0; i < XSIZE(exp_Mweight); i++)
				{

					double cc = DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, i);
					// ignore non-determined cc
					if (cc == -999.)
						continue;

					if (do_sim_anneal && iter > 1)
					{
						// P_accept = exp ( - (CCold -CC)/temperature)
						// cc is negative value, so use "+ cc"
						double my_prob = rnd_unif() * exp(-(exp_local_oldcc[exp_ipart] + cc)/temperature);
						if (my_prob > mymaxprob)
						{
							mymaxprob = my_prob;
							mymindiff2 = cc;
							myminidx = i;
						}
					}
					else
					{
						// just search for the maximum
						if (cc < mymindiff2)
						{
							mymindiff2 = cc;
							myminidx = i;
						}
					}
				}
				// Set all except for the best hidden variable to zero and the smallest element to 1
				for (long int i = 0; i < XSIZE(exp_Mweight); i++)
					DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, i)= 0.;

				DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, myminidx)= 1.;
				exp_thisparticle_sumweight += 1.;
				exp_iimage += mydata.getNrImagesInSeries(exp_part_id);
				exp_sum_weight[exp_ipart] = exp_thisparticle_sumweight;
			}
		}
t_convert_cpu.stop();
	}

	else
	{

t_convert_prepare.start();
		//double exp_min_diff2_ipart = exp_min_diff2[exp_ipart];
		//size_t first_iorient = 0, last_iorient = 0;
		//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))
		//long int nr_particles = exp_nr_ori_particles;
		size_t first_iorient = 0, last_iorient = 0;
		long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
		last_iorient = nr_orients - 1;
		int exp_nr_classes = iclass_max - iclass_min + 1;
		int model_nr_classes = mymodel.nr_classes;
		//std::cout << "exp_nr_classes	" << exp_nr_classes <<"	" << "model_nr_classes	" << model_nr_classes << std::endl;
		cudaError_t cudaStat;

		//double *exp_Mweight_dev;
		long int xdim_Mweight = exp_Mweight_dev_size;
		long int nzyxdim_Mweight = exp_nr_particles * exp_Mweight_dev_size;
		/*
		cudaStat = cudaMalloc((void**)&exp_Mweight_dev, nzyxdim_Mweight*sizeof(double));
		if (cudaStat != cudaSuccess) 
		{
        	printf ("device memory allocation 1 failed, exp_Mweight_dev");
        	//return EXIT_FAILURE;
    	}

		cudaStat = cudaMemcpy(exp_Mweight_dev, exp_Mweight.data, nzyxdim_Mweight*sizeof(double), cudaMemcpyHostToDevice);
		if (cudaStat != cudaSuccess) 
		{
			printf ("device memory copy 1 failed, exp_Mweight_dev");
    	//return EXIT_FAILURE;
    	}
    	

		double *exp_min_diff2_host, *exp_min_diff2_dev;
		exp_min_diff2_host = new double [exp_nr_particles];
		for(int i=0; i<exp_nr_particles; i++)
		{
			exp_min_diff2_host[i] = exp_min_diff2[i];
		}
		cudaStat = cudaMalloc((void **)&exp_min_diff2_dev, exp_nr_particles*sizeof(double));
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory allocation 2 failed, exp_min_diff2_dev");
    	//return EXIT_FAILURE;
    	}
  
		cudaStat = cudaMemcpy(exp_min_diff2_dev, exp_min_diff2_host, exp_nr_particles*sizeof(double), cudaMemcpyHostToDevice);
		if (cudaStat != cudaSuccess) 
		{
	    	printf ("device memory copy 2 failed, exp_min_diff2_dev");
    	//return EXIT_FAILURE;
    	}
    	*/

		double *pdf_orientation_host, *pdf_orientation_dev;
		pdf_orientation_host = new double[exp_nr_particles*model_nr_classes*nr_orients];
		cudaStat = cudaMalloc((void **)&pdf_orientation_dev, exp_nr_particles*model_nr_classes*nr_orients*sizeof(double));
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory allocation 3 failed, pdf_orientation_dev");
    		printf(" %d %d %d\n",exp_nr_particles, model_nr_classes, nr_orients);
    	//return EXIT_FAILURE;
    	}

		double *pdf_offset_host, *pdf_offset_dev;
		pdf_offset_host = new double[exp_nr_particles*model_nr_classes*exp_nr_trans];
		cudaStat = cudaMalloc((void **)&pdf_offset_dev, exp_nr_particles*model_nr_classes*exp_nr_trans*sizeof(double));
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory allocation 4 failed, pdf_offset_dev");
    	//return EXIT_FAILURE;
    	}

		double *exp_sum_weight_dev, *exp_sum_weight_host;
		cudaStat = cudaMalloc((void **)&exp_sum_weight_dev, exp_nr_particles*sizeof(double));//nr_exp_ipart*sizeof(double));
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory allocation 5 failed, exp_sum_weight_dev");
    	//return EXIT_FAILURE;
    	}

		cudaStat = cudaMemset(exp_sum_weight_dev, 0., exp_nr_particles*sizeof(double));
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory set failed, exp_sum_weight_dev");
    	//return EXIT_FAILURE;
    	}
t_convert_prepare.stop();

t_convert_cpu.start();
		exp_sum_weight_host = new double[exp_nr_particles];

		long int nr_elements = nr_orients*exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans;
		//std::cout << "exp_Mweight.xdim = " << exp_Mweight.xdim << "	" << "exp_ipass" << "	" << exp_ipass << "	" << "nr_elements" << "	" << nr_elements << std::endl;

		for (long int ori_part_id = exp_my_first_ori_particle; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
		{
			// loop over all particles inside this ori_particle
			for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, exp_ipart++)
			{
				if(mydata.ori_particles[ori_part_id].particles_id.size()!=1)
					std::cout << "Warning: mydata.ori_particles[ori_part_id].particles_id.size() != 1" << "	" << mydata.ori_particles[ori_part_id].particles_id.size() << std::endl;

				exp_part_id = mydata.ori_particles[ori_part_id].particles_id[i];
				//exp_thisparticle_sumweight = 0.;
				
				for (exp_iclass = iclass_min; exp_iclass <= iclass_max; exp_iclass++)
				{

					// The loops over all orientations are parallelised using threads
					//exp_iorient_ThreadTaskDistributor->reset(); // reset thread distribution tasks
					//global_ThreadManager->run(globalThreadConvertSquaredDifferencesToWeightsAllOrientations);
					//doThreadConvertSquaredDifferencesToWeightsAllOrientations_gpu();
							// exp_iclass loop does not always go from 0 to nr_classes!
					long int iorientclass_offset = exp_iclass * exp_nr_rot;

					//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))
														
					for (long int iorient = first_iorient; iorient <= last_iorient; iorient++)
					{
						long int idir = iorient / exp_nr_psi;
						long int ipsi = iorient % exp_nr_psi;

						// Get prior for this direction
						if (mymodel.orientational_prior_mode == NOPRIOR)
						{
							pdf_orientation_host[(exp_ipart*model_nr_classes+exp_iclass)*nr_orients+iorient] = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
						}
						else
						{
							pdf_orientation_host[(exp_ipart*model_nr_classes+exp_iclass)*nr_orients+iorient] = sampling.getPriorProbability(idir, ipsi);
						}
							

						// Loop over all translations
						
					}
					for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
					{

						// To speed things up, only calculate pdf_offset at the coarse sampling.
						// That should not matter much, and that way one does not need to calculate all the OversampledTranslations
						Matrix1D<double> my_offset, my_prior;
						sampling.getTranslation(itrans, my_offset);
						// Convert offsets back to Angstroms to calculate PDF!
						// TODO: if series, then have different exp_old_xoff for each my_image_no....
						// WHAT TO DO WITH THIS?!!!

						//double pdf_offset;
						if (mymodel.ref_dim == 2)
							pdf_offset_host[(exp_ipart*model_nr_classes+exp_iclass)*exp_nr_trans+itrans] = calculatePdfOffset(exp_old_offset[exp_iimage] + my_offset, mymodel.prior_offset_class[exp_iclass]);
						else
							pdf_offset_host[(exp_ipart*model_nr_classes+exp_iclass)*exp_nr_trans+itrans] = calculatePdfOffset(exp_old_offset[exp_iimage] + my_offset, exp_prior[exp_iimage]);
						//std::cout << "pdf_offset_host	" << pdf_offset_host[(exp_ipart*nr_orients+iorient)*exp_nr_trans + itrans] << std::endl;
					}
					
				}
				// Keep track of number of processed images
				exp_iimage += mydata.getNrImagesInSeries(exp_part_id);
			}

		}
t_convert_cpu.stop();
t_convert_prepare.start();
		cudaStat = cudaMemcpy(pdf_orientation_dev, pdf_orientation_host, exp_nr_particles*model_nr_classes*nr_orients*sizeof(double), cudaMemcpyHostToDevice);
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory copy 3 failed, pdf_orientation_dev");
    	//return EXIT_FAILURE;
    	}

		cudaStat = cudaMemcpy(pdf_offset_dev, pdf_offset_host, exp_nr_particles*model_nr_classes*exp_nr_trans*sizeof(double), cudaMemcpyHostToDevice);
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory copy 4 failed, pdf_offset_dev");
    	//return EXIT_FAILURE;
    	}
t_convert_prepare.stop();
t_convert_kernel.start();	
		calculate_weight_gpu(exp_nr_particles, exp_sum_weight_dev, exp_Mweight_dev, exp_min_diff2_dev, pdf_orientation_dev, pdf_offset_dev, xdim_Mweight, iclass_min, iclass_max, model_nr_classes, exp_nr_classes, nr_elements, nr_orients, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans);
t_convert_kernel.stop();
t_convert_prepare.start();
		cudaStat = cudaMemcpy(exp_sum_weight_host, exp_sum_weight_dev, exp_nr_particles*sizeof(double), cudaMemcpyDeviceToHost);
t_convert_prepare.stop();
		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory copy 5 failed, exp_sum_weight_host");
    	//return EXIT_FAILURE;
    	}
t_convert_cpu.start();
    	exp_Mweight.resize(exp_nr_particles, mymodel.nr_classes * sampling.NrSamplingPoints(exp_current_oversampling, false));
t_convert_cpu.stop();
t_convert_prepare.start();
		cudaStat = cudaMemcpy(exp_Mweight.data, exp_Mweight_dev, nzyxdim_Mweight*sizeof(double), cudaMemcpyDeviceToHost);

		if (cudaStat != cudaSuccess) 
		{
    		printf ("device memory copy 6 failed, exp_Mweight_host");
    	//return EXIT_FAILURE;
    	}
		//std::cout << std::endl << "GPU computed exp_sum_weight" << std::endl;
		for(int i = 0; i < exp_nr_particles; i++)
		{
			exp_sum_weight[i] = exp_sum_weight_host[i];
			//std::cout << exp_sum_weight_host[i] << std::endl;
		}
		
		//cudaFree(exp_Mweight_dev);
		//cudaFree(exp_min_diff2_dev);
		cudaFree(pdf_orientation_dev);
		cudaFree(pdf_offset_dev);
		cudaFree(exp_sum_weight_dev);

		//delete [] exp_min_diff2_host;
		delete [] pdf_orientation_host;
		delete [] pdf_offset_host;
		delete [] exp_sum_weight_host;
t_convert_prepare.stop();

	}
	// The remainder of this function is not threaded.
	//cudaFree(exp_Mweight_dev);
	// Initialise exp_Mcoarse_significant
t_convert_cpu.start();	
	if (exp_ipass==0)
		exp_Mcoarse_significant.resize(exp_nr_particles, XSIZE(exp_Mweight));

	// Now, for each particle,  find the exp_significant_weight that encompasses adaptive_fraction of exp_sum_weight
	exp_significant_weight.clear();
	exp_significant_weight.resize(exp_nr_particles, 0.);
	for (long int ori_part_id = exp_my_first_ori_particle, my_image_no = 0, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	{
		// loop over all particles inside this ori_particle
		for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
		{
			long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];

#ifdef TIMING
	timer.tic(TIMING_WEIGHT_SORT);
#endif
			MultidimArray<double> sorted_weight;
			// Get the relevant row for this particle
			exp_Mweight.getRow(ipart, sorted_weight);

			// Only select non-zero probabilities to speed up sorting
			long int np = 0;
			FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(sorted_weight)
			{
				if (DIRECT_MULTIDIM_ELEM(sorted_weight, n) > 0.)
				{
					DIRECT_MULTIDIM_ELEM(sorted_weight, np) = DIRECT_MULTIDIM_ELEM(sorted_weight, n);
					np++;
				}
			}
			sorted_weight.resize(np);

			// Sort from low to high values
			sorted_weight.sort();

#ifdef TIMING
	timer.toc(TIMING_WEIGHT_SORT);
#endif
			double frac_weight = 0.;
			double my_significant_weight;
			long int my_nr_significant_coarse_samples = 0;
			for (long int i = XSIZE(sorted_weight) - 1; i >= 0; i--)
			{
				if (exp_ipass==0) my_nr_significant_coarse_samples++;
				my_significant_weight = DIRECT_A1D_ELEM(sorted_weight, i);
				frac_weight += my_significant_weight;
				if (frac_weight > adaptive_fraction * exp_sum_weight[ipart])
					break;
			}

#ifdef DEBUG_SORT
			// Check sorted array is really sorted
			double prev = 0.;
			FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(sorted_weight)
			{
				if (DIRECT_MULTIDIM_ELEM(sorted_weight, n) < prev)
				{
					Image<double> It;
					It()=sorted_weight;
					It() *= 10000;
					It.write("sorted_weight.spi");
					std::cerr << "written sorted_weight.spi" << std::endl;
					REPORT_ERROR("Error in sorting!");
				}
				prev=DIRECT_MULTIDIM_ELEM(sorted_weight, n);
			}
#endif

			if (exp_ipass==0 && my_nr_significant_coarse_samples == 0)
			{
				std::cerr << " ipart= " << ipart << " adaptive_fraction= " << adaptive_fraction << std::endl;
				std::cerr << " frac-weight= " << frac_weight << std::endl;
				std::cerr << " exp_sum_weight[ipart]= " << exp_sum_weight[ipart] << std::endl;
				Image<double> It;
				std::cerr << " XSIZE(exp_Mweight)= " << XSIZE(exp_Mweight) << std::endl;
				It()=exp_Mweight;
				It() *= 10000;
				It.write("Mweight2.spi");
				std::cerr << "written Mweight2.spi" << std::endl;
				std::cerr << " np= " << np << std::endl;
				It()=sorted_weight;
				It() *= 10000;
				std::cerr << " XSIZE(sorted_weight)= " << XSIZE(sorted_weight) << std::endl;
				if (XSIZE(sorted_weight) > 0)
				{
					It.write("sorted_weight.spi");
					std::cerr << "written sorted_weight.spi" << std::endl;
				}
				REPORT_ERROR("my_nr_significant_coarse_samples == 0");
			}

			if (exp_ipass==0)
			{
				// Store nr_significant_coarse_samples for all images in this series
				for (int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++, my_image_no++)
				{
					DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NR_SIGN) = (double)my_nr_significant_coarse_samples;
				}

				// Keep track of which coarse samplings were significant were significant for this particle
				for (int ihidden = 0; ihidden < XSIZE(exp_Mcoarse_significant); ihidden++)
				{
					if (DIRECT_A2D_ELEM(exp_Mweight, ipart, ihidden) >= my_significant_weight)
						DIRECT_A2D_ELEM(exp_Mcoarse_significant, ipart, ihidden) = true;
					else
						DIRECT_A2D_ELEM(exp_Mcoarse_significant, ipart, ihidden) = false;
				}

			}
			exp_significant_weight[ipart] = my_significant_weight;
#ifdef DEBUG_OVERSAMPLING
			std::cerr << " sum_weight[ipart]= " << exp_sum_weight[ipart] << " my_significant_weight= " << my_significant_weight << std::endl;
			std::cerr << " my_nr_significant_coarse_samples= " << my_nr_significant_coarse_samples << std::endl;
			std::cerr << " ipass= " << exp_ipass << " Pmax="<<DIRECT_A1D_ELEM(sorted_weight,XSIZE(sorted_weight) - 1)/frac_weight
					<<" nr_sign_sam= "<<nr_significant_samples<<" sign w= "<<exp_significant_weight<< "sum_weight= "<<exp_sum_weight<<std::endl;
#endif

		} // end loop part_id (i)
	} // end loop ori_part_id


#ifdef DEBUG_CONVERTDIFF2W
	//Image<double> tt;
	//tt()=sorted_weight;
	//tt.write("sorted_weight.spi");
	//std::cerr << "written sorted_weight.spi" << std::endl;
	std::cerr << " ipass= " << exp_ipass << " exp_part_id= " << exp_part_id << std::endl;
	std::cerr << " diff2w: opt_xoff= " << opt_xoff << " opt_yoff= " << opt_yoff << " opt_psi= " << opt_psi << std::endl;
	std::cerr << " diff2w: opt_iover_rot= " << opt_iover_rot << " opt_iover_trans= " << opt_iover_trans << " opt_ipsi= " << opt_ipsi << std::endl;
	std::cerr << " diff2w: opt_itrans= " << opt_itrans << " opt_ihidden= " << opt_ihidden << " opt_ihidden_over= " << opt_ihidden_over << std::endl;
	std::cerr << "significant_weight= " << exp_significant_weight << " max_weight= " << max_weight << std::endl;
	std::cerr << "nr_significant_coarse_samples= " << nr_significant_coarse_samples <<std::endl;
	debug2 = (double)opt_ihidden_over;
#endif

#ifdef TIMING
	if (exp_ipass == 0) timer.toc(TIMING_ESP_WEIGHT1);
	else timer.toc(TIMING_ESP_WEIGHT2);
#endif
t_convert_cpu.stop();	
}


void MlOptimiser::doThreadConvertSquaredDifferencesToWeightsAllOrientations_gpu()
{
#ifdef DEBUG_THREAD
		std::cerr << "entering doThreadConvertSquaredDifferencesToWeightsAllOrientations" << std::endl;
#endif
	
	
		// Store local sum of weights for this thread and then combined all threads at the end of this function inside a mutex.
		double thisthread_sumweight = 0.;
	
		// exp_iclass loop does not always go from 0 to nr_classes!
		long int iorientclass_offset = exp_iclass * exp_nr_rot;
	
		size_t first_iorient = 0, last_iorient = 0;
		long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
		last_iorient = nr_orients;
		//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))
		{
			for (long int iorient = first_iorient; iorient < last_iorient; iorient++)
			{
	
				double pdf_orientation;
				long int iorientclass = iorientclass_offset + iorient;
				long int idir = iorient / exp_nr_psi;
				long int ipsi = iorient % exp_nr_psi;
	
				// Get prior for this direction
				if (mymodel.orientational_prior_mode == NOPRIOR)
				{
#ifdef DEBUG_CHECKSIZES
					if (idir >= XSIZE(mymodel.pdf_direction[exp_iclass]))
					{
						std::cerr<< "idir= "<<idir<<" XSIZE(mymodel.pdf_direction[exp_iclass])= "<< XSIZE(mymodel.pdf_direction[exp_iclass]) <<std::endl;
						REPORT_ERROR("idir >= mymodel.pdf_direction[exp_iclass].size()");
					}
#endif
					pdf_orientation = DIRECT_MULTIDIM_ELEM(mymodel.pdf_direction[exp_iclass], idir);
				}
				else
				{
					pdf_orientation = sampling.getPriorProbability(idir, ipsi);
				}
			
				// Loop over all translations
				for (long int itrans = 0; itrans < exp_nr_trans; itrans++)
				{
	
					long int ihidden = iorientclass * exp_nr_trans + itrans;
	
					// To speed things up, only calculate pdf_offset at the coarse sampling.
					// That should not matter much, and that way one does not need to calculate all the OversampledTranslations
					Matrix1D<double> my_offset, my_prior;
					sampling.getTranslation(itrans, my_offset);
					// Convert offsets back to Angstroms to calculate PDF!
					// TODO: if series, then have different exp_old_xoff for each my_image_no....
					// WHAT TO DO WITH THIS?!!!
	
					double pdf_offset;
					if (mymodel.ref_dim == 2)
						pdf_offset = calculatePdfOffset(exp_old_offset[exp_iimage] + my_offset, mymodel.prior_offset_class[exp_iclass]);
					else
						pdf_offset = calculatePdfOffset(exp_old_offset[exp_iimage] + my_offset, exp_prior[exp_iimage]);
	
					// TMP DEBUGGING
					if (mymodel.orientational_prior_mode != NOPRIOR && (pdf_offset==0. || pdf_orientation==0.))
					{
						//global_mutex.lock();
						std::cerr << " pdf_offset= " << pdf_offset << " pdf_orientation= " << pdf_orientation << std::endl;
						std::cerr << " exp_ipart= " << exp_ipart << " exp_part_id= " << exp_part_id << std::endl;
						std::cerr << " iorient= " << iorient << " idir= " << idir << " ipsi= " << ipsi << std::endl;
						std::cerr << " exp_nr_psi= " << exp_nr_psi << " exp_nr_dir= " << exp_nr_dir << " exp_nr_trans= " << exp_nr_trans << std::endl;
						for (long int i = 0; i < sampling.directions_prior.size(); i++)
							std::cerr << " sampling.directions_prior["<<i<<"]= " << sampling.directions_prior[i] << std::endl;
						for (long int i = 0; i < sampling.psi_prior.size(); i++)
							std::cerr << " sampling.psi_prior["<<i<<"]= " << sampling.psi_prior[i] << std::endl;
						REPORT_ERROR("ERROR! pdf_offset==0.|| pdf_orientation==0.");
						//global_mutex.unlock();
					}
					if (exp_nr_oversampled_rot == 0)
						REPORT_ERROR("exp_nr_oversampled_rot == 0");
					if (exp_nr_oversampled_trans == 0)
						REPORT_ERROR("exp_nr_oversampled_trans == 0");
	
	
#ifdef TIMING
					// Only time one thread, as I also only time one MPI process
					//if (thread_id == 0)
						timer.tic(TIMING_WEIGHT_EXP);
#endif
	
					// Now first loop over iover_rot, because that is the order in exp_Mweight as well
					long int ihidden_over = ihidden * exp_nr_oversampled_rot * exp_nr_oversampled_trans;
					
					for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++)
					{
						// Then loop over iover_trans
						for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++, ihidden_over++)
						{
	
#ifdef DEBUG_CHECKSIZES
							if (ihidden_over >= XSIZE(exp_Mweight))
							{
								std::cerr<< "ihidden_over= "<<ihidden_over<<" XSIZE(Mweight)= "<< XSIZE(exp_Mweight) <<std::endl;
								REPORT_ERROR("ihidden_over >= XSIZE(exp_Mweight)");
							}
#endif
	
							// Only exponentiate for determined values of exp_Mweight
							// (this is always true in the first pass, but not so in the second pass)
							// Only deal with this sampling point if its weight was significant
#ifdef DEBUG_CHECKSIZES
							if (exp_iimage >= YSIZE(exp_Mweight))
							{
								std::cerr<< "exp_iimage= "<<exp_iimage<<" YSIZE(exp_Mweight)= "<< YSIZE(exp_Mweight) <<std::endl;
								std::cerr << " exp_ipart= " << exp_ipart << std::endl;
								std::cerr << " DIRECT_A2D_ELEM(exp_Mweight, exp_iimage, ihidden_over)= " << DIRECT_A2D_ELEM(exp_Mweight, exp_iimage, ihidden_over) << std::endl;
								std::cerr << " DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, ihidden_over)= " << DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, ihidden_over) << std::endl;
								REPORT_ERROR("exp_iimage >= YSIZE(exp_Mweight)");
							}
#endif
	
							if (DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, ihidden_over) < 0.)
							{
								DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, ihidden_over) = 0.;
							}
							else
							{
								double weight = pdf_orientation * pdf_offset;
	
								double diff2 = DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, ihidden_over) - exp_min_diff2[exp_ipart];
	
								// next line because of numerical precision of exp-function
								if (diff2 > 700.) weight = 0.;
								// TODO: use tabulated exp function?
								else weight *= exp(-diff2);
	//#define DEBUG_PSIANGLE_PDISTRIBUTION
#ifdef DEBUG_PSIANGLE_PDISTRIBUTION
								std::cout << ipsi*360./sampling.NrPsiSamplings() << " "<< weight << std::endl;
#endif
								// Store the weight
								DIRECT_A2D_ELEM(exp_Mweight, exp_ipart, ihidden_over) = weight;
	
#ifdef DEBUG_CHECKSIZES
								if (std::isnan(weight))
								{
									//global_mutex.lock();
									std::cerr<< "weight= "<<weight<<" is not a number! " <<std::endl;
									std::cerr << " exp_min_diff2[exp_ipart]= " << exp_min_diff2[exp_ipart] << std::endl;
									std::cerr << " exp_ipart= " << exp_ipart << std::endl;
									std::cerr << " exp_part_id= " << exp_part_id << std::endl;
									std::cerr << " mydata.getNrImagesInSeries(exp_part_id)= " << mydata.getNrImagesInSeries(exp_part_id) << std::endl;
									long int my_image_no = exp_iimage + 0;
									std::cerr << " my_image_no= " << my_image_no << std::endl;
									std::cerr << " exp_iimage= " << exp_iimage << std::endl;
									std::cerr << " DIRECT_A2D_ELEM(exp_Mweight, my_image_no, ihidden_over)= " << DIRECT_A2D_ELEM(exp_Mweight, my_image_no, ihidden_over) << std::endl;
									REPORT_ERROR("weight is not a number");
									//global_mutex.unlock();
								}
#endif
	
								// Keep track of sum and maximum of all weights for this particle
								// Later add all to exp_thisparticle_sumweight, but inside this loop sum to local thisthread_sumweight first
								thisthread_sumweight += weight;
	
							} // end if/else exp_Mweight < 0.
						} // end loop iover_trans
					}// end loop iover_rot
#ifdef TIMING
					// Only time one thread, as I also only time one MPI process
					//if (thread_id == 0)
						timer.toc(TIMING_WEIGHT_EXP);
#endif
				} // end loop itrans
	
			} // end loop iorient
		} // end while task distributor
	
		// Now inside a mutex update the sum of all weights
		//global_mutex.lock();
		exp_thisparticle_sumweight += thisthread_sumweight;
		//global_mutex.unlock();
	
		// Wait until all threads have finished
		//global_barrier->wait();
	
	///////////GPU computation/////////////////
		//long int iorientclass_offset = exp_iclass * exp_nr_rot;
		//size_t first_iorient = 0;
		//long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
		//last_iorient = nr_orients;
	

#ifdef DEBUG_THREAD
		std::cerr << "leaving doThreadConvertSquaredDifferencesToWeightsAllOrientations" << std::endl;
#endif


	}


void MlOptimiser::storeWeightedSums_gpu()
{
	long t1, t2, t3 ,t4;
	t1 = t2 = t3 = t4 = 0;
	struct timeval start, end;

#ifdef TIMING
	timer.tic(TIMING_ESP_WSUM);
#endif

	// Initialise the maximum of all weights to a negative value
	exp_max_weight.resize(exp_nr_particles);
	for (int n = 0; n < exp_nr_particles; n++)
		exp_max_weight[n] = -1.;

	// In doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s() the origin of the exp_local_Minvsigma2s was omitted.
	// Set those back here
gettimeofday(&start, NULL);
	for (exp_iseries = 0; exp_iseries < mydata.getNrImagesInSeries((mydata.ori_particles[exp_my_first_ori_particle]).particles_id[0]); exp_iseries++)
	{
		//doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_syn();
		//doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_gpu();
	}
	//cudaFree(exp_local_sqrtXi2_dev);
	//cudaFree(exp_Minvsigma2s_dev);
	//cudaFree(exp_local_Fctfs_dev);

	for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	{
		// loop over all particles inside this ori_particle
		for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
		{
			long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];

			//std::cout << part_id << std::endl;

			for (exp_iseries = 0; exp_iseries < mydata.getNrImagesInSeries(part_id); exp_iseries++)
			{
				// Re-get all shifted versions of the (current_sized) images, their (current_sized) CTFs and their inverted Sigma2 matrices
				// This may be necessary for when using --strict_highres_exp. Otherwise norm estimation may become unstable!!
				//exp_ipart_ThreadTaskDistributor->reset(); // reset thread distribution tasks
				//global_ThreadManager->run(globalThreadPrecalculateShiftedImagesCtfsAndInvSigma2s);

				//doThreadPrecalculateShiftedImagesCtfsAndInvSigma2s_syn();

				int group_id = mydata.getGroupId(part_id, exp_iseries);
				int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
				DIRECT_MULTIDIM_ELEM(exp_local_Minvsigma2s[my_image_no], 0) = 1. / (sigma2_fudge * DIRECT_A1D_ELEM(mymodel.sigma2_noise[group_id], 0));
			}
		}
	}
gettimeofday(&end, NULL);
t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

	for (exp_iseries = 0; exp_iseries < mydata.getNrImagesInSeries((mydata.ori_particles[exp_my_first_ori_particle]).particles_id[0]); exp_iseries++)
	{
		// TODO: check this!!!
		// I think this is just done for the first ipart
		int my_image_no = exp_starting_image_no[0] + exp_iseries;
		// Get micrograph transformation matrix
		exp_R_mic.resize(3,3);
		exp_R_mic(0,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_0);
		exp_R_mic(0,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_1);
		exp_R_mic(0,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_0_2);
		exp_R_mic(1,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_0);
		exp_R_mic(1,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_1);
		exp_R_mic(1,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_1_2);
		exp_R_mic(2,0) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_0);
		exp_R_mic(2,1) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_1);
		exp_R_mic(2,2) = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_MAT_2_2);

		// For norm_correction of this iseries image:
		exp_wsum_norm_correction.resize(exp_nr_particles);
		for (int n = 0; n < exp_nr_particles; n++)
			exp_wsum_norm_correction[n] = 0.;

		// For scale_correction of this iseries image:
		if (do_scale_correction)
		{
			MultidimArray<double> aux;
			aux.initZeros(mymodel.ori_size/2 + 1);
			exp_wsum_scale_correction_XA.resize(exp_nr_particles);
			exp_wsum_scale_correction_AA.resize(exp_nr_particles);
			for (int n = 0; n < exp_nr_particles; n++)
			{
				exp_wsum_scale_correction_XA[n] = aux;
				exp_wsum_scale_correction_AA[n] = aux;
			}
		}

		// Loop from iclass_min to iclass_max to deal with seed generation in first iteration
gettimeofday(&start, NULL);
		for (exp_iclass = iclass_min; exp_iclass <= iclass_max; exp_iclass++)
		{

			// The loops over all orientations are parallelised using threads

			//HERE!!
			//exp_iorient_ThreadTaskDistributor->reset(); // reset thread distribution tasks
			//global_ThreadManager->run(globalThreadStoreWeightedSumsAllOrientations);
			doThreadStoreWeightedSumsAllOrientations_gpu();
			//doThreadStoreWeightedSumsAllOrientations_syn();

		} // end loop iclass
gettimeofday(&end, NULL);
t2 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);			


		// Extend norm_correction and sigma2_noise estimation to higher resolutions for all particles
gettimeofday(&start, NULL);
		for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
		{
			// loop over all particles inside this ori_particle
			for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
			{
				long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];

				// Which number was this image in the combined array of exp_iseries and exp_part_id
				long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;

				// If the current images were smaller than the original size, fill the rest of wsum_model.sigma2_noise with the power_class spectrum of the images
				int group_id = mydata.getGroupId(part_id, exp_iseries);
				for (int ires = mymodel.current_size/2 + 1; ires < mymodel.ori_size/2 + 1; ires++)
				{
					DIRECT_A1D_ELEM(wsum_model.sigma2_noise[group_id], ires) += DIRECT_A1D_ELEM(exp_power_imgs[my_image_no], ires);
					// Also extend the weighted sum of the norm_correction
					exp_wsum_norm_correction[ipart] += DIRECT_A1D_ELEM(exp_power_imgs[my_image_no], ires);
				}

				// Store norm_correction
				// Multiply by old value because the old norm_correction term was already applied to the image
				if (do_norm_correction)
				{
					double old_norm_correction = DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM);
					old_norm_correction /= mymodel.avg_norm_correction;
					// Now set the new norm_correction in the relevant position of exp_metadata
					// The factor two below is because exp_wsum_norm_correctiom is similar to sigma2_noise, which is the variance for the real/imag components
					// The variance of the total image (on which one normalizes) is twice this value!
					DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM) = old_norm_correction * sqrt(exp_wsum_norm_correction[ipart] * 2.);
					wsum_model.avg_norm_correction += old_norm_correction * sqrt(exp_wsum_norm_correction[ipart] * 2.);

					if (!(iter == 1 && do_firstiter_cc) && DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM) > 10.)
					{
						std::cout << " WARNING: norm_correction= "<< DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM) << " for particle " << part_id << " in group " << group_id + 1 << "; Are your groups large enough?" << std::endl;
						std::cout << " mymodel.current_size= " << mymodel.current_size << " mymodel.ori_size= " << mymodel.ori_size << " part_id= " << part_id << std::endl;
						std::cout << " coarse_size= " << coarse_size << std::endl;
						std::cout << " DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM)= " << DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM) << std::endl;
						std::cout << " mymodel.avg_norm_correction= " << mymodel.avg_norm_correction << std::endl;
						std::cout << " exp_wsum_norm_correction[ipart]= " << exp_wsum_norm_correction[ipart] << std::endl;
						std::cout << " old_norm_correction= " << old_norm_correction << std::endl;
						std::cout << " wsum_model.avg_norm_correction= " << wsum_model.avg_norm_correction << std::endl;
						std::cout << " group_id= " << group_id << " mymodel.scale_correction[group_id]= " << mymodel.scale_correction[group_id] << std::endl;
						std::cout << " mymodel.sigma2_noise[group_id]= " << mymodel.sigma2_noise[group_id] << std::endl;
						std::cout << " wsum_model.sigma2_noise[group_id]= " << wsum_model.sigma2_noise[group_id] << std::endl;
						std::cout << " exp_power_imgs[my_image_no]= " << exp_power_imgs[my_image_no] << std::endl;
						std::cout << " exp_wsum_scale_correction_XA[ipart]= " << exp_wsum_scale_correction_XA[ipart] << " exp_wsum_scale_correction_AA[ipart]= " << exp_wsum_scale_correction_AA[ipart] << std::endl;
						std::cout << " wsum_model.wsum_signal_product_spectra[group_id]= " << wsum_model.wsum_signal_product_spectra[group_id] << " wsum_model.wsum_reference_power_spectra[group_id]= " << wsum_model.wsum_reference_power_spectra[group_id] << std::endl;
						std::cout << " exp_min_diff2[ipart]= " << exp_min_diff2[ipart] << std::endl;
						std::cout << " ml_model.scale_correction[group_id]= " << mymodel.scale_correction[group_id] << std::endl;
						std::cout << " exp_significant_weight[ipart]= " << exp_significant_weight[ipart] << std::endl;
						std::cout << " exp_max_weight[ipart]= " << exp_max_weight[ipart] << std::endl;

					}
					//TMP DEBUGGING
					/*
					if (!(iter == 1 && do_firstiter_cc) && DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM) > 10.)
					{
						std::cerr << " mymodel.current_size= " << mymodel.current_size << " mymodel.ori_size= " << mymodel.ori_size << " part_id= " << part_id << std::endl;
						std::cerr << " DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM)= " << DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_NORM) << std::endl;
						std::cerr << " mymodel.avg_norm_correction= " << mymodel.avg_norm_correction << std::endl;
						std::cerr << " exp_wsum_norm_correction[ipart]= " << exp_wsum_norm_correction[ipart] << std::endl;
						std::cerr << " old_norm_correction= " << old_norm_correction << std::endl;
						std::cerr << " wsum_model.avg_norm_correction= " << wsum_model.avg_norm_correction << std::endl;
						std::cerr << " group_id= " << group_id << " mymodel.scale_correction[group_id]= " << mymodel.scale_correction[group_id] << std::endl;
						std::cerr << " mymodel.sigma2_noise[group_id]= " << mymodel.sigma2_noise[group_id] << std::endl;
						std::cerr << " wsum_model.sigma2_noise[group_id]= " << wsum_model.sigma2_noise[group_id] << std::endl;
						std::cerr << " exp_power_imgs[my_image_no]= " << exp_power_imgs[my_image_no] << std::endl;
						std::cerr << " exp_wsum_scale_correction_XA[ipart]= " << exp_wsum_scale_correction_XA[ipart] << " exp_wsum_scale_correction_AA[ipart]= " << exp_wsum_scale_correction_AA[ipart] << std::endl;
						std::cerr << " wsum_model.wsum_signal_product_spectra[group_id]= " << wsum_model.wsum_signal_product_spectra[group_id] << " wsum_model.wsum_reference_power_spectra[group_id]= " << wsum_model.wsum_reference_power_spectra[group_id] << std::endl;
						std::cerr << " exp_min_diff2[ipart]= " << exp_min_diff2[ipart] << std::endl;
						std::cerr << " ml_model.scale_correction[group_id]= " << mymodel.scale_correction[group_id] << std::endl;
						std::cerr << " exp_significant_weight[ipart]= " << exp_significant_weight[ipart] << std::endl;
						std::cerr << " exp_max_weight[ipart]= " << exp_max_weight[ipart] << std::endl;
						mymodel.write("debug");
						std::cerr << "written debug_model.star" << std::endl;
						REPORT_ERROR("MlOptimiser::storeWeightedSums ERROR: normalization is larger than 10");
					}
					*/

				}

				// Store weighted sums for scale_correction
				if (do_scale_correction)
				{
					// Divide XA by the old scale_correction and AA by the square of that, because was incorporated into Fctf
					exp_wsum_scale_correction_XA[ipart] /= mymodel.scale_correction[group_id];
					exp_wsum_scale_correction_AA[ipart] /= mymodel.scale_correction[group_id] * mymodel.scale_correction[group_id];

					wsum_model.wsum_signal_product_spectra[group_id] += exp_wsum_scale_correction_XA[ipart];
					wsum_model.wsum_reference_power_spectra[group_id] += exp_wsum_scale_correction_AA[ipart];
				}

			} // end loop part_id (i)
		} // end loop ori_part_id
gettimeofday(&end, NULL);
t3 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);			

	} // end loop exp_iseries
//std::cout << "storeWeightedSums_GPU " << t1/1000000. << " " << t2/1000000. << " " << t3/1000000. << " " << t4/1000000. << std::endl;


#ifdef DEBUG_OVERSAMPLING
	std::cerr << " max_weight= " << max_weight << " nr_sign_sam= "<<nr_significant_samples<<" sign w= "<<exp_significant_weight<<std::endl;
#endif

	// Some analytics...
	// Calculate normalization constant for dLL
	for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	{
		// loop over all particles inside this ori_particle
		for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
		{
			long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
			double logsigma2 = 0.;
			for (long int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				int group_id = mydata.getGroupId(part_id, iseries);
				FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Mresol_fine)
				{
					int ires = DIRECT_MULTIDIM_ELEM(Mresol_fine, n);
					// Note there is no sqrt in the normalisation term because of the 2-dimensionality of the complex-plane
					// Also exclude origin from logsigma2, as this will not be considered in the P-calculations
					if (ires > 0)
						logsigma2 += log( 2. * PI * DIRECT_A1D_ELEM(mymodel.sigma2_noise[group_id], ires));
				}

			}

			if (exp_sum_weight[ipart]==0)
			{
				std::cerr << " part_id= " << part_id << std::endl;
				std::cerr << " ipart= " << ipart << std::endl;
				std::cerr << " exp_min_diff2[ipart]= " << exp_min_diff2[ipart] << std::endl;
				std::cerr << " logsigma2= " << logsigma2 << std::endl;
				int group_id = mydata.getGroupId(part_id, 0);
				std::cerr << " group_id= " << group_id << std::endl;
				std::cerr << " ml_model.scale_correction[group_id]= " << mymodel.scale_correction[group_id] << std::endl;
				std::cerr << " exp_significant_weight[ipart]= " << exp_significant_weight[ipart] << std::endl;
				std::cerr << " exp_max_weight[ipart]= " << exp_max_weight[ipart] << std::endl;
				std::cerr << " ml_model.sigma2_noise[group_id]= " << mymodel.sigma2_noise[group_id] << std::endl;
				REPORT_ERROR("ERROR: exp_sum_weight[ipart]==0");
			}

			double dLL;

			if ((iter==1 && do_firstiter_cc) || do_always_cc)
				dLL = -exp_min_diff2[ipart];
			else
				dLL = log(exp_sum_weight[ipart]) - exp_min_diff2[ipart] - logsigma2;

			wsum_model.LL += dLL;
			wsum_model.ave_Pmax += DIRECT_A2D_ELEM(exp_metadata, exp_starting_image_no[ipart], METADATA_PMAX);

			// Also store dLL of each image in the output array
			for (long int iseries = 0; iseries < mydata.getNrImagesInSeries(part_id); iseries++)
			{
				long int my_image_no = exp_starting_image_no[ipart] + iseries;
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_DLL) = dLL;
			}
		} // end loop part_id
	} // end loop ori_part_id
#ifdef TIMING
	timer.toc(TIMING_ESP_WSUM);
#endif

}
	
void MlOptimiser::doThreadStoreWeightedSumsAllOrientations_syn( )
{
	
		long t1, t2, t3 ,t4;
		t1 = t2 = t3 = t4 = 0;
		struct timeval start, end;
	
	gettimeofday(&start, NULL);
	
		std::vector< Matrix1D<double> > oversampled_orientations, oversampled_translations;
		Matrix2D<double> A;
		MultidimArray<Complex > Fimg, Fref, Frefctf, Fimg_shift, Fimg_shift_nomask;
		MultidimArray<double> Minvsigma2, Mctf, Fweight;
		double rot, tilt, psi;
		bool have_warned_small_scale = false;
	
		// Initialising...
		Fref.resize(exp_Fimgs[0]);
		Frefctf.resize(exp_Fimgs[0]);
		Fweight.resize(exp_Fimgs[0]);
	
		// Initialise Mctf to all-1 for if !do_ctf_corection
		Mctf.resize(exp_Fimgs[0]);
		Mctf.initConstant(1.);
	
		// Initialise Minvsigma2 to all-1 for if !do_map
		Minvsigma2.resize(exp_Fimgs[0]);
		Minvsigma2.initConstant(1.);
	
		// Make local copies of weighted sums (excepts BPrefs, which are too big)
		// so that there are not too many mutex locks below
		std::vector<MultidimArray<double> > thr_wsum_sigma2_noise, thr_wsum_scale_correction_XA, thr_wsum_scale_correction_AA, thr_wsum_pdf_direction;
		std::vector<double> thr_wsum_norm_correction, thr_sumw_group, thr_wsum_pdf_class, thr_wsum_prior_offsetx_class, thr_wsum_prior_offsety_class, thr_max_weight;
		double thr_wsum_sigma2_offset;
		MultidimArray<double> thr_metadata, zeroArray;
	
		// Wsum_sigma_noise2 is a 1D-spectrum for each group
		zeroArray.initZeros(mymodel.ori_size/2 + 1);
		thr_wsum_sigma2_noise.resize(mymodel.nr_groups);
		for (int n = 0; n < mymodel.nr_groups; n++)
		{
			thr_wsum_sigma2_noise[n] = zeroArray;
		}
		// scale-correction terms are a spectrum for each particle
		thr_wsum_scale_correction_XA.resize(exp_nr_particles);
		thr_wsum_scale_correction_AA.resize(exp_nr_particles);
		for (int n = 0; n < exp_nr_particles; n++)
		{
			thr_wsum_scale_correction_XA[n] = zeroArray;
			thr_wsum_scale_correction_AA[n] = zeroArray;
		}
		// wsum_pdf_direction is a 1D-array (of length sampling.NrDirections(0, true)) for each class
		zeroArray.initZeros(sampling.NrDirections(0, true));
		thr_wsum_pdf_direction.resize(mymodel.nr_classes);
		for (int n = 0; n < mymodel.nr_classes; n++)
		{
			thr_wsum_pdf_direction[n] = zeroArray;
		}
		// wsum_norm_correction is a double for each particle
		thr_wsum_norm_correction.resize(exp_nr_particles, 0.);
		// sumw_group is a double for each group
		thr_sumw_group.resize(mymodel.nr_groups, 0.);
		// wsum_pdf_class is a double for each class
		thr_wsum_pdf_class.resize(mymodel.nr_classes, 0.);
		if (mymodel.ref_dim == 2)
		{
			thr_wsum_prior_offsetx_class.resize(mymodel.nr_classes, 0.);
			thr_wsum_prior_offsety_class.resize(mymodel.nr_classes, 0.);
		}
		// max_weight is a double for each particle
		thr_max_weight.resize(exp_nr_particles, 0.);
		// wsum_sigma2_offset is just a double
		thr_wsum_sigma2_offset = 0.;
		// metadata is a 2D array of nr_particles x METADATA_LINE_LENGTH
		thr_metadata.initZeros(exp_metadata);
	
		// exp_iclass loop does not always go from 0 to nr_classes!
		long int iorientclass_offset = exp_iclass * exp_nr_rot;
		long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
		
		size_t first_iorient = 0, last_iorient = nr_orients;
		//while (exp_iorient_ThreadTaskDistributor->getTasks(first_iorient, last_iorient))
		{
			for (long int iorient = first_iorient; iorient < last_iorient; iorient++)
			{
	
				long int iorientclass = iorientclass_offset + iorient;
	
				// Only proceed if any of the particles had any significant coarsely sampled translation
				if (isSignificantAnyParticleAnyTranslation(iorientclass))
				{
	
					long int idir = iorient / exp_nr_psi;
					long int ipsi = iorient % exp_nr_psi;
	
					// Now get the oversampled (rot, tilt, psi) triplets
					// This will be only the original (rot,tilt,psi) triplet if (adaptive_oversampling==0)
					sampling.getOrientations(idir, ipsi, adaptive_oversampling, oversampled_orientations);
	
					// Loop over all oversampled orientations (only a single one in the first pass)
					for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++)
					{
						rot = XX(oversampled_orientations[iover_rot]);
						tilt = YY(oversampled_orientations[iover_rot]);
						psi = ZZ(oversampled_orientations[iover_rot]);
						// Get the Euler matrix
						Euler_angles2matrix(rot, tilt, psi, A);
	
						// Take tilt-series into account
						A = (exp_R_mic * A).inv();
	
						// Project the reference map (into Fref)
						if (!do_skip_maximization)
							(mymodel.PPref[exp_iclass]).get2DFourierTransform(Fref, A, IS_INV);
	
						// Inside the loop over all translations and all part_id sum all shift Fimg's and their weights
						// Then outside this loop do the actual backprojection
						Fimg.initZeros(Fref);
						Fweight.initZeros(Fref);
						int number_valid_weight = 0;
						/// Now that reference projection has been made loop over someParticles!
						for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
						{
							// loop over all particles inside this ori_particle
							for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
							{
								long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
								// Which number was this image in the combined array of iseries and part_idpart_id
								long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
								int group_id = mydata.getGroupId(part_id, exp_iseries);
	
								if (!do_skip_maximization)
								{
									if (do_map)
										Minvsigma2 = exp_local_Minvsigma2s[my_image_no];
									// else Minvsigma2 was initialised to ones
	
									// Apply CTF to reference projection
									if (do_ctf_correction)
									{
										Mctf = exp_local_Fctfs[my_image_no];
										if (refs_are_ctf_corrected)
										{
											FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Fref)
											{
												DIRECT_MULTIDIM_ELEM(Frefctf, n) = DIRECT_MULTIDIM_ELEM(Fref, n) * DIRECT_MULTIDIM_ELEM(Mctf, n);
											}
										}
										else
										{
											Frefctf = Fref;
										}
									}
									else
									{
										// initialise because there are multiple particles and Mctf gets selfMultiplied for scale_correction
										Mctf.initConstant(1.);
										Frefctf = Fref;
									}
	
									if (do_scale_correction)
									{
										// TODO: implemenent B-factor as well...
										double myscale = mymodel.scale_correction[group_id];
										if (myscale > 10000.)
										{
											std::cerr << " rlnMicrographScaleCorrection= " << myscale << " group= " << group_id + 1 << " my_image_no= " << my_image_no << std::endl;
											REPORT_ERROR("ERROR: rlnMicrographScaleCorrection is very high. Did you normalize your data?");
										}
										else if (myscale < 0.001)
										{
	
											if (!have_warned_small_scale)
											{
												std::cout << " WARNING: ignoring group " << group_id + 1 << " with very small or negative scale (" << myscale <<
														"); Use larger groups for more stable scale estimates." << std::endl;
												have_warned_small_scale = true;
											}
											myscale = 0.001;
										}
										FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Frefctf)
										{
											DIRECT_MULTIDIM_ELEM(Frefctf, n) *= myscale;
										}
										// For CTF-terms in BP
										Mctf *= myscale;
									}
								} // end if !do_skip_maximization
								
								long int ihidden = iorientclass * exp_nr_trans;
								for (long int itrans = 0; itrans < exp_nr_trans; itrans++, ihidden++)
								{
	
									sampling.getTranslations(itrans, adaptive_oversampling, oversampled_translations);
	
									for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
									{
	
										// Only deal with this sampling point if its weight was significant
										long int ihidden_over = ihidden * exp_nr_oversampled_trans * exp_nr_oversampled_rot +
												iover_rot * exp_nr_oversampled_trans + iover_trans;
	
										double weight = DIRECT_A2D_ELEM(exp_Mweight, ipart, ihidden_over);
	
										// Only sum weights for non-zero weights
										if (weight >= exp_significant_weight[ipart])
										{
											number_valid_weight++;
											// Normalise the weight (do this after the comparison with exp_significant_weight!)
											weight /= exp_sum_weight[ipart];
	
											if (!do_skip_maximization)
											{
												// Get the shifted image
												long int ishift = my_image_no * exp_nr_oversampled_trans * exp_nr_trans +
														itrans * exp_nr_oversampled_trans + iover_trans;
												Fimg_shift = exp_local_Fimgs_shifted[ishift];
												Fimg_shift_nomask = exp_local_Fimgs_shifted_nomask[ishift];
	
												// Store weighted sum of squared differences for sigma2_noise estimation
												FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Mresol_fine)
												{
													int ires = DIRECT_MULTIDIM_ELEM(Mresol_fine, n);
													if (ires > -1)
													{
														// Use FT of masked image for noise estimation!
														double diff_real = (DIRECT_MULTIDIM_ELEM(Frefctf, n)).real - (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).real;
														double diff_imag = (DIRECT_MULTIDIM_ELEM(Frefctf, n)).imag - (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).imag;
														double wdiff2 = weight * (diff_real*diff_real + diff_imag*diff_imag);
	
														// group-wise sigma2_noise
														DIRECT_MULTIDIM_ELEM(thr_wsum_sigma2_noise[group_id], ires) += wdiff2;
														// For norm_correction
														thr_wsum_norm_correction[ipart] += wdiff2;
													}
												}
	
												// Store the weighted sums of the norm_correction terms
												if (do_scale_correction)
												{
													double sumXA = 0.;
													double sumA2 = 0.;
													FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Mresol_fine)
													{
														int ires = DIRECT_MULTIDIM_ELEM(Mresol_fine, n);
	
														// Once the reference becomes strongly regularised one does no longer want to store XA and AA!
														if (ires > -1 && DIRECT_A1D_ELEM(mymodel.data_vs_prior_class[exp_iclass], ires) > 3.)
														{
															sumXA = (DIRECT_MULTIDIM_ELEM(Frefctf, n)).real * (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).real;
															sumXA += (DIRECT_MULTIDIM_ELEM(Frefctf, n)).imag * (DIRECT_MULTIDIM_ELEM(Fimg_shift, n)).imag;
															DIRECT_A1D_ELEM(thr_wsum_scale_correction_XA[ipart], ires) += weight * sumXA;
	
															// This could be pre-calculated above...
															sumA2 = (DIRECT_MULTIDIM_ELEM(Frefctf, n)).real * (DIRECT_MULTIDIM_ELEM(Frefctf, n)).real;
															sumA2 += (DIRECT_MULTIDIM_ELEM(Frefctf, n)).imag * (DIRECT_MULTIDIM_ELEM(Frefctf, n)).imag;
															DIRECT_A1D_ELEM(thr_wsum_scale_correction_AA[ipart], ires) += weight * sumA2;
														}
													}
												}
	
												// Store sum of weights for this group
												thr_sumw_group[group_id] += weight;
	
												// Store weights for this class and orientation
												thr_wsum_pdf_class[exp_iclass] += weight;
	
												if (mymodel.ref_dim ==2)
												{
													// Also store weighted offset differences for prior_offsets of each class
													thr_wsum_prior_offsetx_class[exp_iclass] += weight * XX(exp_old_offset[my_image_no] + oversampled_translations[iover_trans]);
													thr_wsum_prior_offsety_class[exp_iclass] += weight * YY(exp_old_offset[my_image_no] + oversampled_translations[iover_trans]);
	
													// Store weighted sum2 of origin offsets (in Angstroms instead of pixels!!!)
													thr_wsum_sigma2_offset += weight * ((mymodel.prior_offset_class[exp_iclass] - exp_old_offset[my_image_no] - oversampled_translations[iover_trans]).sum2());
	
												}
												else
												{
													// Store weighted sum2 of origin offsets (in Angstroms instead of pixels!!!)
													thr_wsum_sigma2_offset += weight * ((exp_prior[my_image_no] - exp_old_offset[my_image_no] - oversampled_translations[iover_trans]).sum2());
												}
	
												// Store weight for this direction of this class
												if (mymodel.orientational_prior_mode == NOPRIOR)
												{
													DIRECT_MULTIDIM_ELEM(thr_wsum_pdf_direction[exp_iclass], idir) += weight;
												}
												else
												{
													// In the case of orientational priors, get the original number of the direction back
													long int mydir = sampling.getDirectionNumberAlsoZeroPrior(idir);
													DIRECT_MULTIDIM_ELEM(thr_wsum_pdf_direction[exp_iclass], mydir) += weight;
												}
	
												// Store sum of weight*SSNR*Fimg in data and sum of weight*SSNR in weight
												// Use the FT of the unmasked image to back-project in order to prevent reconstruction artefacts! SS 25oct11
												FOR_ALL_DIRECT_ELEMENTS_IN_MULTIDIMARRAY(Fimg_shift)
												{
													double myctf = DIRECT_MULTIDIM_ELEM(Mctf, n);
													// Note that weightxinvsigma2 already contains the CTF!
													double weightxinvsigma2 = weight * myctf * DIRECT_MULTIDIM_ELEM(Minvsigma2, n);
													// now Fimg stores sum of all shifted w*Fimg
													(DIRECT_MULTIDIM_ELEM(Fimg, n)).real += (DIRECT_MULTIDIM_ELEM(Fimg_shift_nomask, n)).real * weightxinvsigma2;
													(DIRECT_MULTIDIM_ELEM(Fimg, n)).imag += (DIRECT_MULTIDIM_ELEM(Fimg_shift_nomask, n)).imag * weightxinvsigma2;
													// now Fweight stores sum of all w
													// Note that CTF needs to be squared in Fweight, weightxinvsigma2 already contained one copy
													DIRECT_MULTIDIM_ELEM(Fweight, n) += weightxinvsigma2 * myctf;
												}
	
											} // end if !do_skip_maximization
	
											// Keep track of max_weight and the corresponding optimal hidden variables
											if (weight > thr_max_weight[ipart])
											{
												// Store optimal image parameters
												thr_max_weight[ipart] = weight;
	
												// Calculate the angles back from the Euler matrix because for tilt series exp_R_mic may have changed them...
												//std::cerr << " ORI rot= " << rot << " tilt= " << tilt << " psi= " << psi << std::endl;
												Euler_matrix2angles(A.inv(), rot, tilt, psi);
												//std::cerr << " BACK rot= " << rot << " tilt= " << tilt << " psi= " << psi << std::endl;
	
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_ROT) = rot;
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_TILT) = tilt;
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PSI) = psi;
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_XOFF) = XX(exp_old_offset[my_image_no]) + XX(oversampled_translations[iover_trans]);
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_YOFF) = YY(exp_old_offset[my_image_no]) + YY(oversampled_translations[iover_trans]);
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_CLASS) = (double)exp_iclass + 1;
												DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PMAX) = thr_max_weight[ipart];
											}
	
										} // end if weight >= exp_significant_weight
									} // end loop iover_trans
								} // end loop itrans
								
							}// end loop part_id (i)
						} // end loop ori_part_id
						if (!do_skip_maximization)
						{

							// Perform the actual back-projection.
							// This is done with the sum of all (in-plane) shifted Fimg's
							// Perform this inside a mutex
							//global_mutex2.lock();
							(wsum_model.BPref[exp_iclass]).set2DFourierTransform(Fimg, A, IS_INV, &Fweight);
							//global_mutex2.unlock();
						} // end if !do_skip_maximization
	
					}// end if iover_rot
				}// end loop do_proceed
	
			} // end loop ipsi
		} // end loop idir
	
		// Now, inside a global_mutex, update the weighted sums among all threads
		//global_mutex.lock();
	
		if (!do_skip_maximization)
		{
			if (do_scale_correction)
			{
				for (int n = 0; n < exp_nr_particles; n++)
				{
					exp_wsum_scale_correction_XA[n] += thr_wsum_scale_correction_XA[n];
					exp_wsum_scale_correction_AA[n] += thr_wsum_scale_correction_AA[n];
				}
			}
			for (int n = 0; n < exp_nr_particles; n++)
			{
				exp_wsum_norm_correction[n] += thr_wsum_norm_correction[n];
			}
			for (int n = 0; n < mymodel.nr_groups; n++)
			{
				wsum_model.sigma2_noise[n] += thr_wsum_sigma2_noise[n];
				wsum_model.sumw_group[n] += thr_sumw_group[n];
			}
			for (int n = 0; n < mymodel.nr_classes; n++)
			{
				wsum_model.pdf_class[n] += thr_wsum_pdf_class[n];
	
				if (mymodel.ref_dim == 2)
				{
					XX(wsum_model.prior_offset_class[n]) += thr_wsum_prior_offsetx_class[n];
					YY(wsum_model.prior_offset_class[n]) += thr_wsum_prior_offsety_class[n];
				}
				wsum_model.pdf_direction[n] += thr_wsum_pdf_direction[n];
			}
			wsum_model.sigma2_offset += thr_wsum_sigma2_offset;
		} // end if !do_skip_maximization
	
		// Check max_weight for each particle and set exp_metadata
		for (int n = 0; n < exp_nr_particles; n++)
		{
			// Equal-to because of the series: the nth images in a series will have the same maximum as the first one
			if (thr_max_weight[n] >= exp_max_weight[n])
			{
				// Set max_weight
				exp_max_weight[n] = thr_max_weight[n];
	
				// Set metadata
				long int my_image_no = exp_starting_image_no[n] + exp_iseries;
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT)  = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_ROT);
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_TILT);
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI)  = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PSI);
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_XOFF);
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_YOFF);
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CLASS)= DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_CLASS);
				DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PMAX) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PMAX);
			}
		}
		//global_mutex.unlock();
	
		// Wait until all threads have finished
		//global_barrier->wait();
	
	gettimeofday(&end, NULL);
	t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);			
	std::cout << "doThreadStoreWeightedSumsAllOrientations all " << t1/1000000. << std::endl;
	
}
	
void MlOptimiser::doThreadStoreWeightedSumsAllOrientations_gpu( )
{

TIMER_START
	long t1, t2, t3 ,t4;
	t1 = t2 = t3 = t4 = 0;
	struct timeval start, end;
	struct timeval start_mem, end_mem;

	gettimeofday(&start, NULL);

	std::vector< Matrix1D<double> > oversampled_orientations, oversampled_translations;
	Matrix2D<double> A;
	MultidimArray<Complex > Fimg, Fref, Frefctf, Fimg_shift, Fimg_shift_nomask;
	MultidimArray<double> Minvsigma2, Mctf, Fweight;
	double rot, tilt, psi;
	bool have_warned_small_scale = false;

	// Initialising...
	Fref.resize(exp_Fimgs[0]);
	Frefctf.resize(exp_Fimgs[0]);
	Fweight.resize(exp_Fimgs[0]);

	std::vector< MultidimArray<double> > Mctf_Array;
	std::vector< MultidimArray<Complex> > Frefctf_Array;

	Mctf_Array.clear();
	Frefctf_Array.clear();

	Mctf_Array.resize(exp_nr_particles);
	Frefctf_Array.resize(exp_nr_images);
	for(int i=0; i < exp_nr_particles; i++)
	{
		Frefctf_Array[i].resize(exp_Fimgs[0]);
		Mctf_Array[i].resize(exp_Fimgs[0]);
		Mctf_Array[i].initConstant(1.);
	}
	// Initialise Mctf to all-1 for if !do_ctf_corection
	Mctf.resize(exp_Fimgs[0]);
	Mctf.initConstant(1.);

	// Initialise Minvsigma2 to all-1 for if !do_map
	Minvsigma2.resize(exp_Fimgs[0]);
	Minvsigma2.initConstant(1.);

	// Make local copies of weighted sums (excepts BPrefs, which are too big)
	// so that there are not too many mutex locks below
	std::vector<MultidimArray<double> > thr_wsum_sigma2_noise, thr_wsum_scale_correction_XA, thr_wsum_scale_correction_AA, thr_wsum_pdf_direction;
	std::vector<double> thr_wsum_norm_correction, thr_sumw_group, thr_wsum_pdf_class, thr_wsum_prior_offsetx_class, thr_wsum_prior_offsety_class, thr_max_weight;
	double thr_wsum_sigma2_offset;
	MultidimArray<double> thr_metadata, zeroArray;
	
	//==========================================================================
	//GPU computing related data structures
	cufftDoubleComplex *Fref_dev, *Frefctf_dev, *Fimg_dev;
	double *Mctf_dev, *Mctf_host;
	double *myscale_dev, *myscale_host;
	double *Frefctf_H;
	double *Fweight_dev;
	double *Fimg_H, *Fweight_H;

	Fimg_H = (double *) malloc( exp_Fimgs[0].zyxdim*sizeof(cufftDoubleComplex));
	Fweight_H = (double *) malloc(exp_Fimgs[0].zyxdim*sizeof(double));

	cudaMalloc((void **)&Frefctf_dev, exp_Fimgs[0].zyxdim*exp_nr_images*sizeof(cufftDoubleComplex) );
	cudaMalloc((void **)&Fref_dev, exp_Fimgs[0].zyxdim*sizeof(cufftDoubleComplex) );
	cudaMalloc((void **)&Mctf_dev, exp_Fimgs[0].zyxdim*exp_nr_images*sizeof(double) );
	cudaMalloc((void **)&myscale_dev, exp_nr_images*sizeof(double) );
	//cudaMemset(Mctf_dev, 1.,exp_Fimgs[0].zyxdim*exp_nr_images );
	cudaMalloc((void **)&Fimg_dev, exp_Fimgs[0].zyxdim*sizeof(cufftDoubleComplex) );
	cudaMalloc((void **)&Fweight_dev, exp_Fimgs[0].zyxdim*sizeof(double) );
	
	Mctf_host = (double*) malloc(exp_Fimgs[0].zyxdim*exp_nr_images*sizeof(double));
	myscale_host = (double*) malloc(exp_nr_images*sizeof(double));
	Frefctf_H = (double *) malloc( exp_nr_images*exp_Fimgs[0].zyxdim*sizeof(cufftDoubleComplex));

	double *exp_local_Fimgs_shifted_host, *exp_local_Fimgs_nomask_shifted_host;
	exp_local_Fimgs_shifted_host = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*exp_local_Fimgs_shifted[0].yxdim*2*sizeof(double));
	exp_local_Fimgs_nomask_shifted_host = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*exp_local_Fimgs_shifted[0].yxdim*2*sizeof(double));
	double *exp_significant_sum_weight_dev, *exp_significant_sum_weight_host;
	cudaMalloc((void **)&exp_significant_sum_weight_dev, exp_nr_images*2*sizeof(double));
	exp_significant_sum_weight_host = (double *)malloc(exp_nr_images*2*sizeof(double));
	
	double * oversampled_translations_host;
	oversampled_translations_host = (double *) malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*exp_Fimgs[0].getDim()*sizeof(double));
	double *weight_array;
	double *valid_weight;
	int *ipart_id, *ishift_id, *group_id_H, *iover_trans_H;
	weight_array = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(double));
	valid_weight = (double *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(double));
	ipart_id =  (int *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));
	ishift_id =  (int *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));
	group_id_H =  (int *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));
	iover_trans_H =  (int *)malloc(exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));


	double *valid_weight_dev;
	int *ipart_id_dev, *ishift_id_dev, *group_id_dev;
	cudaMalloc((void **)&valid_weight_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(double));
	cudaMalloc((void **)&ipart_id_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));
	cudaMalloc((void **)&ishift_id_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));
	cudaMalloc((void **)&group_id_dev,exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(int));
	
	double *exp_local_weight_dev;
	cudaMalloc((void **)&exp_local_weight_dev, exp_nr_images*exp_nr_trans*exp_nr_oversampled_trans*sizeof(double));
	cudaStream_t *streams = (cudaStream_t *) malloc(4 * sizeof(cudaStream_t));   
	for (int i = 0; i < 4; i++)    
	{      
		cudaStreamCreate(&(streams[i]));   
	}

	double *thr_wsum_sigma2_noise_D, *thr_wsum_norm_correction_D,
			 *thr_wsum_scale_correction_XA_D,*thr_wsum_scale_correction_AA_D;
			 
	double *thr_wsum_sigma2_noise_H, *thr_wsum_norm_correction_H,
		   *thr_wsum_scale_correction_XA_H,*thr_wsum_scale_correction_AA_H;
	double *data_vs_prior_class_D, *data_vs_prior_class_H;
	int *Mresol_fine_D, *Mresol_fine_H;
	cudaMalloc((void **)&thr_wsum_sigma2_noise_D, mymodel.nr_groups*(mymodel.ori_size/2 + 1)*sizeof(double) );
	cudaMalloc((void **)&thr_wsum_norm_correction_D, exp_nr_particles*sizeof(double) );
	cudaMalloc((void **)&thr_wsum_scale_correction_XA_D, exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double) );
	cudaMalloc((void **)&thr_wsum_scale_correction_AA_D, exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double) );
	cudaMalloc((void **)&Mresol_fine_D, Mresol_fine.zyxdim*sizeof(int));
	cudaMalloc((void **)&data_vs_prior_class_D, mymodel.data_vs_prior_class[exp_iclass].zyxdim*sizeof(double));
	
	cudaMemset(thr_wsum_sigma2_noise_D, 0. ,mymodel.nr_groups*(mymodel.ori_size/2 + 1)*sizeof(double));
	cudaMemset(thr_wsum_norm_correction_D, 0. ,exp_nr_particles*sizeof(double));
	cudaMemset(thr_wsum_scale_correction_XA_D, 0. ,exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double));
	cudaMemset(thr_wsum_scale_correction_AA_D, 0. ,exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double));

  	thr_wsum_sigma2_noise_H = (double *)malloc(mymodel.nr_groups*(mymodel.ori_size/2 + 1)*sizeof(double));
  	thr_wsum_norm_correction_H = (double *)malloc(exp_nr_particles*sizeof(double));
  	thr_wsum_scale_correction_XA_H = (double *)malloc(exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double));
 	thr_wsum_scale_correction_AA_H = (double *)malloc(exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double));

	//============================================================================
	// Wsum_sigma_noise2 is a 1D-spectrum for each group
	zeroArray.initZeros(mymodel.ori_size/2 + 1);
	thr_wsum_sigma2_noise.resize(mymodel.nr_groups);
	for (int n = 0; n < mymodel.nr_groups; n++)
	{
		thr_wsum_sigma2_noise[n] = zeroArray;
	}
	// scale-correction terms are a spectrum for each particle
	thr_wsum_scale_correction_XA.resize(exp_nr_particles);
	thr_wsum_scale_correction_AA.resize(exp_nr_particles);
	for (int n = 0; n < exp_nr_particles; n++)
	{
		thr_wsum_scale_correction_XA[n] = zeroArray;
		thr_wsum_scale_correction_AA[n] = zeroArray;
	}
	// wsum_pdf_direction is a 1D-array (of length sampling.NrDirections(0, true)) for each class
	zeroArray.initZeros(sampling.NrDirections(0, true));
	thr_wsum_pdf_direction.resize(mymodel.nr_classes);
	for (int n = 0; n < mymodel.nr_classes; n++)
	{
		thr_wsum_pdf_direction[n] = zeroArray;
	}
	// wsum_norm_correction is a double for each particle
	thr_wsum_norm_correction.resize(exp_nr_particles, 0.);
	// sumw_group is a double for each group
	thr_sumw_group.resize(mymodel.nr_groups, 0.);
	// wsum_pdf_class is a double for each class
	thr_wsum_pdf_class.resize(mymodel.nr_classes, 0.);
	if (mymodel.ref_dim == 2)
	{
		thr_wsum_prior_offsetx_class.resize(mymodel.nr_classes, 0.);
		thr_wsum_prior_offsety_class.resize(mymodel.nr_classes, 0.);
	}
	// max_weight is a double for each particle
	thr_max_weight.resize(exp_nr_particles, 0.);
	// wsum_sigma2_offset is just a double
	thr_wsum_sigma2_offset = 0.;
	// metadata is a 2D array of nr_particles x METADATA_LINE_LENGTH
	thr_metadata.initZeros(exp_metadata);

	// exp_iclass loop does not always go from 0 to nr_classes!
	long int iorientclass_offset = exp_iclass * exp_nr_rot;
	long int nr_orients = sampling.NrDirections() * sampling.NrPsiSamplings();
	
	size_t first_iorient = 0, last_iorient = nr_orients;
	
	//================================================================
	//GPU preparing
	double *exp_Minvsigma2s_H;
	exp_Minvsigma2s_H = (double*) malloc(exp_nr_images*exp_local_Minvsigma2s[0].zyxdim*sizeof(double));

TIMER_END
	for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
	{
						// loop over all particles inside this ori_particle
		for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
		{

TIMER_START
			long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
			// Which number was this image in the combined array of iseries and part_idpart_id
			long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
			if(do_map)
				memcpy(exp_Minvsigma2s_H+my_image_no*exp_local_Minvsigma2s[0].zyxdim, exp_local_Minvsigma2s[my_image_no].data,exp_local_Minvsigma2s[0].zyxdim*sizeof(double) );
			else
				memcpy(exp_Minvsigma2s_H+my_image_no*exp_local_Minvsigma2s[0].zyxdim, Minvsigma2.data,exp_local_Minvsigma2s[0].zyxdim*sizeof(double) );
TIMER_END
		}
	}
TIMER_START	
	cudaMemcpy(exp_Minvsigma2s_dev, exp_Minvsigma2s_H, exp_nr_images*exp_Fimgs[0].yxdim*sizeof(double), cudaMemcpyHostToDevice);
	cudaMemcpy(Mresol_fine_D, Mresol_fine.data, Mresol_fine.zyxdim*sizeof(int), cudaMemcpyHostToDevice );				
	cudaMemcpy(data_vs_prior_class_D, mymodel.data_vs_prior_class[exp_iclass].data, mymodel.data_vs_prior_class[exp_iclass].zyxdim*sizeof(double), cudaMemcpyHostToDevice);
TIMER_END
		
	//================================================================

	for (long int iorient = first_iorient; iorient < last_iorient; iorient++)
	{

			long int iorientclass = iorientclass_offset + iorient;

			// Only proceed if any of the particles had any significant coarsely sampled translation
			if (isSignificantAnyParticleAnyTranslation(iorientclass))
			{

				long int idir = iorient / exp_nr_psi;
				long int ipsi = iorient % exp_nr_psi;

				// Now get the oversampled (rot, tilt, psi) triplets
				// This will be only the original (rot,tilt,psi) triplet if (adaptive_oversampling==0)
				sampling.getOrientations(idir, ipsi, adaptive_oversampling, oversampled_orientations);

				// Loop over all oversampled orientations (only a single one in the first pass)
				for (long int iover_rot = 0; iover_rot < exp_nr_oversampled_rot; iover_rot++)
				{
					rot = XX(oversampled_orientations[iover_rot]);
					tilt = YY(oversampled_orientations[iover_rot]);
					psi = ZZ(oversampled_orientations[iover_rot]);
					// Get the Euler matrix
					Euler_angles2matrix(rot, tilt, psi, A);

					// Take tilt-series into account
					A = (exp_R_mic * A).inv();

					// Project the reference map (into Fref)
					if (!do_skip_maximization)
					{
TIMER_START
						(mymodel.PPref[exp_iclass]).get2DFourierTransform(Fref, A, IS_INV);
TIMER_END
					}

					// Inside the loop over all translations and all part_id sum all shift Fimg's and their weights
					// Then outside this loop do the actual backprojection
					Fimg.initZeros(Fref);
					Fweight.initZeros(Fref);
					int num_valid_weight = 0;
					/// Now that reference projection has been made loop over someParticles!
					for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
					{
TIMER_START
						// loop over all particles inside this ori_particle
						for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
						{

							long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
							// Which number was this image in the combined array of iseries and part_idpart_id
							long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
							int group_id = mydata.getGroupId(part_id, exp_iseries);

							if (!do_skip_maximization)
							{

								// Apply CTF to reference projection
								if (do_ctf_correction)
								{
									Mctf_Array[my_image_no]  = exp_local_Fctfs[my_image_no];
								}
								else
								{
									// initialise because there are multiple particles and Mctf gets selfMultiplied for scale_correction
									Mctf_Array[my_image_no] .initConstant(1.);
									//Frefctf_Array[my_image_no] = Fref;
								}
								
								memcpy(Mctf_host+my_image_no*exp_Fimgs[0].zyxdim, Mctf_Array[my_image_no].data, exp_Fimgs[0].zyxdim*sizeof(double));

								if (do_scale_correction)
								{
									// TODO: implemenent B-factor as well...
									double myscale = mymodel.scale_correction[group_id];
									if (myscale > 10000.)
									{
										std::cerr << " rlnMicrographScaleCorrection= " << myscale << " group= " << group_id + 1 << " my_image_no= " << my_image_no << std::endl;
										REPORT_ERROR("ERROR: rlnMicrographScaleCorrection is very high. Did you normalize your data?");
									}
									else if (myscale < 0.001)
									{

										if (!have_warned_small_scale)
										{
											std::cout << " WARNING: ignoring group " << group_id + 1 << " with very small or negative scale (" << myscale <<
													"); Use larger groups for more stable scale estimates." << std::endl;
											have_warned_small_scale = true;
										}
										myscale = 0.001;
									}

									Mctf_Array[my_image_no] *= myscale;
									myscale_host[my_image_no] = myscale;
								}
							} // end if !do_skip_maximization
						}
TIMER_END
					}
					//=============================================================
					//GPU  for Frefctf computing and scaling 
					if(!do_skip_maximization)
					{
TIMER_START
						cudaMemcpyAsync(Fref_dev, Fref.data, exp_Fimgs[0].zyxdim*sizeof(cufftDoubleComplex),cudaMemcpyHostToDevice, streams[1] );
						cudaMemcpyAsync(Mctf_dev, Mctf_host, exp_nr_images*exp_Fimgs[0].zyxdim*sizeof(double), cudaMemcpyHostToDevice,  streams[1] );
						cudaMemcpyAsync(myscale_dev, myscale_host, exp_nr_images*sizeof(double), cudaMemcpyHostToDevice,  streams[1] );
						calculate_frefctf_Mctf_gpu(Frefctf_dev, Fref_dev, Mctf_dev, myscale_dev, exp_nr_images, exp_Fimgs[0].zyxdim, do_ctf_correction && refs_are_ctf_corrected, do_scale_correction, streams[1]);
cudaDeviceSynchronize();
TIMER_END

					}
					

TIMER_START				
					for (long int ori_part_id = exp_my_first_ori_particle, ipart = 0; ori_part_id <= exp_my_last_ori_particle; ori_part_id++)
					{
						// loop over all particles inside this ori_particle
						for (long int i = 0; i < mydata.ori_particles[ori_part_id].particles_id.size(); i++, ipart++)
						{
							long int part_id = mydata.ori_particles[ori_part_id].particles_id[i];
							// Which number was this image in the combined array of iseries and part_idpart_id
							long int my_image_no = exp_starting_image_no[ipart] + exp_iseries;
							int group_id = mydata.getGroupId(part_id, exp_iseries);
							
							long int ihidden = iorientclass * exp_nr_trans;
							
							for (long int itrans = 0; itrans < exp_nr_trans; itrans++, ihidden++)
							{

								sampling.getTranslations(itrans, adaptive_oversampling, oversampled_translations);

								for (long int iover_trans = 0; iover_trans < exp_nr_oversampled_trans; iover_trans++)
								{

									// Only deal with this sampling point if its weight was significant
									long int ihidden_over = ihidden * exp_nr_oversampled_trans * exp_nr_oversampled_rot +
											iover_rot * exp_nr_oversampled_trans + iover_trans;

									double weight = DIRECT_A2D_ELEM(exp_Mweight, ipart, ihidden_over);

									// Only sum weights for non-zero weights
									if (weight >= exp_significant_weight[ipart])
									{
										long int ishift = my_image_no * exp_nr_oversampled_trans * exp_nr_trans +
													itrans * exp_nr_oversampled_trans + iover_trans;
										weight /= exp_sum_weight[ipart];
										valid_weight[num_valid_weight] = weight;
										ipart_id[num_valid_weight] = my_image_no;
										ishift_id[num_valid_weight] = ishift;	
										group_id_H[num_valid_weight] = group_id;
										iover_trans_H[num_valid_weight] = iover_trans;
										for(int k=0; k < exp_Fimgs[0].getDim(); k++)
											oversampled_translations_host[num_valid_weight*exp_Fimgs[0].getDim()+k] = oversampled_translations[iover_trans].vdata[k];	
								
										num_valid_weight++;																
									}

									
								}
							}
							
						}
						
					}
TIMER_END
					//=============================================================================
					//GPU program for computing the wdiff2 and XA AA  sum
TIMER_START
					if(!do_skip_maximization)
					{
		
						cudaMemcpyAsync(exp_local_weight_dev, valid_weight, num_valid_weight*sizeof(double), cudaMemcpyHostToDevice, streams[1] );
						cudaMemcpyAsync(ipart_id_dev, ipart_id, num_valid_weight*sizeof(int), cudaMemcpyHostToDevice, streams[1]  );
						cudaMemcpyAsync(ishift_id_dev, ishift_id, num_valid_weight*sizeof(int), cudaMemcpyHostToDevice, streams[1] );
						cudaMemcpyAsync(group_id_dev, group_id_H, num_valid_weight*sizeof(int), cudaMemcpyHostToDevice, streams[1]  );
						gettimeofday(&start_mem, NULL);
						cudaMemset(Fimg_dev, 0. , exp_Fimgs[0].zyxdim *sizeof(double)*2);
						cudaMemset(Fweight_dev, 0. , exp_Fimgs[0].zyxdim *sizeof(double));
						calculate_sum_shift_img_gpu( Fimg_dev, Fweight_dev, exp_local_Fimgs_shifted_nomask_dev, exp_Minvsigma2s_dev, exp_local_weight_dev,  Mctf_dev, ipart_id_dev, ishift_id_dev, exp_local_Fimgs_shifted[0].yxdim,  num_valid_weight, streams[1]);
						gettimeofday(&end_mem, NULL);
						cudaMemcpyAsync(Fimg.data, Fimg_dev, exp_local_Fimgs_shifted[0].yxdim*sizeof(double)*2, cudaMemcpyDeviceToHost, streams[1] );
						cudaMemcpyAsync(Fweight.data, Fweight_dev, exp_local_Fimgs_shifted[0].yxdim*sizeof(double), cudaMemcpyDeviceToHost, streams[1] );
						calculate_wdiff2_sumXA_total_gpu(  
													Frefctf_dev, 
													exp_local_Fimgs_shifted_dev,
													exp_local_weight_dev, 
													ipart_id_dev, 
													ishift_id_dev,
													Mresol_fine.nzyxdim,  
													do_scale_correction,
													num_valid_weight,
													exp_local_Fimgs_shifted[0].yxdim,
													thr_wsum_sigma2_noise_D,
													thr_wsum_norm_correction_D,
													thr_wsum_scale_correction_XA_D, 
													thr_wsum_scale_correction_AA_D,
													data_vs_prior_class_D,
													Mresol_fine_D, 
													group_id_dev,
													(mymodel.ori_size/2 + 1),
													streams[1] 
													);
					}
					cudaDeviceSynchronize();
TIMER_END

TIMER_START
					//end of GPU program of computing wdiff2
					//=============================================================================
					for(int i=0; i < num_valid_weight; i++)
					{
						double weight = valid_weight[i];
						int my_image_no= ipart_id[i] ;
						//int ishift=ishift_id[i] ;	
						int group_id =group_id_H[i]  ;
						int iover_trans = iover_trans_H[i];	
						for(int k=0;k < exp_Fimgs[0].getDim(); k++)
							oversampled_translations[iover_trans].vdata[k] =oversampled_translations_host[i*exp_Fimgs[0].getDim()+k] ;	
						

										if (!do_skip_maximization)
										{

											// Store sum of weights for this group
											thr_sumw_group[group_id] += weight;

											// Store weights for this class and orientation
											thr_wsum_pdf_class[exp_iclass] += weight;

											if (mymodel.ref_dim ==2)
											{
												// Also store weighted offset differences for prior_offsets of each class
												thr_wsum_prior_offsetx_class[exp_iclass] += weight * XX(exp_old_offset[my_image_no] + oversampled_translations[iover_trans]);
												thr_wsum_prior_offsety_class[exp_iclass] += weight * YY(exp_old_offset[my_image_no] + oversampled_translations[iover_trans]);

												// Store weighted sum2 of origin offsets (in Angstroms instead of pixels!!!)
												thr_wsum_sigma2_offset += weight * ((mymodel.prior_offset_class[exp_iclass] - exp_old_offset[my_image_no] - oversampled_translations[iover_trans]).sum2());

											}
											else
											{
												// Store weighted sum2 of origin offsets (in Angstroms instead of pixels!!!)
												thr_wsum_sigma2_offset += weight * ((exp_prior[my_image_no] - exp_old_offset[my_image_no] - oversampled_translations[iover_trans]).sum2());
											}

											// Store weight for this direction of this class
											if (mymodel.orientational_prior_mode == NOPRIOR)
											{
												DIRECT_MULTIDIM_ELEM(thr_wsum_pdf_direction[exp_iclass], idir) += weight;
											}
											else
											{
												// In the case of orientational priors, get the original number of the direction back
												long int mydir = sampling.getDirectionNumberAlsoZeroPrior(idir);
												DIRECT_MULTIDIM_ELEM(thr_wsum_pdf_direction[exp_iclass], mydir) += weight;
											}

											// Store sum of weight*SSNR*Fimg in data and sum of weight*SSNR in weight
											// Use the FT of the unmasked image to back-project in order to prevent reconstruction artefacts! SS 25oct11

										} // end if !do_skip_maximization

										// Keep track of max_weight and the corresponding optimal hidden variables
										if (weight > thr_max_weight[my_image_no])
										{
											// Store optimal image parameters
											thr_max_weight[my_image_no] = weight;

											// Calculate the angles back from the Euler matrix because for tilt series exp_R_mic may have changed them...
											//std::cerr << " ORI rot= " << rot << " tilt= " << tilt << " psi= " << psi << std::endl;
											Euler_matrix2angles(A.inv(), rot, tilt, psi);
											//std::cerr << " BACK rot= " << rot << " tilt= " << tilt << " psi= " << psi << std::endl;

											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_ROT) = rot;
											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_TILT) = tilt;
											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PSI) = psi;
											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_XOFF) = XX(exp_old_offset[my_image_no]) + XX(oversampled_translations[iover_trans]);
											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_YOFF) = YY(exp_old_offset[my_image_no]) + YY(oversampled_translations[iover_trans]);
											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_CLASS) = (double)exp_iclass + 1;
											DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PMAX) = thr_max_weight[my_image_no];
										}
					}		//std::cout<<"The number of valid weight is: " << num_valid_weight << std::endl;
					cudaDeviceSynchronize();
TIMER_END					
					if (!do_skip_maximization)
					{
						// Perform the actual back-projection.
						// This is done with the sum of all (in-plane) shifted Fimg's
						// Perform this inside a mutex
						//global_mutex2.lock();
TIMER_START
						(wsum_model.BPref[exp_iclass]).set2DFourierTransform(Fimg, A, IS_INV, &Fweight);
TIMER_END
						//global_mutex2.unlock();

					} // end if !do_skip_maximization

				}// end if iover_rot
			}// end loop do_proceed
			
		} // end loop ipsi

	// Now, inside a global_mutex, update the weighted sums among all threads
	//global_mutex.lock();
TIMER_START
	cudaMemcpy(thr_wsum_sigma2_noise_H, thr_wsum_sigma2_noise_D, mymodel.nr_groups*(mymodel.ori_size/2 + 1)*sizeof(double), cudaMemcpyDeviceToHost );
	cudaMemcpy(thr_wsum_norm_correction_H, thr_wsum_norm_correction_D, exp_nr_particles*sizeof(double), cudaMemcpyDeviceToHost );
	cudaMemcpy(thr_wsum_scale_correction_XA_H, thr_wsum_scale_correction_XA_D, exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double), cudaMemcpyDeviceToHost );
	cudaMemcpy(thr_wsum_scale_correction_AA_H, thr_wsum_scale_correction_AA_D, exp_nr_particles*(mymodel.ori_size/2 + 1)*sizeof(double), cudaMemcpyDeviceToHost );
TIMER_END

TIMER_START

	if (!do_skip_maximization)
	{
		if (do_scale_correction)
		{
			for (int n = 0; n < exp_nr_particles; n++)
			{
				for(int i =0; i< (mymodel.ori_size/2+1); i++)
				{
					exp_wsum_scale_correction_XA[n].data[i] += thr_wsum_scale_correction_XA_H[n*(mymodel.ori_size/2+1)+i];
					exp_wsum_scale_correction_AA[n].data[i] += thr_wsum_scale_correction_AA_H[n*(mymodel.ori_size/2+1)+i];

				}

				//exp_wsum_scale_correction_XA[n] += thr_wsum_scale_correction_XA[n];
				//exp_wsum_scale_correction_AA[n] += thr_wsum_scale_correction_AA[n];
			}
		}
		
		for (int n = 0; n < exp_nr_particles; n++)
		{
			exp_wsum_norm_correction[n] += thr_wsum_norm_correction_H[n];
			//exp_wsum_norm_correction[n] += thr_wsum_norm_correction[n];
		}
		for (int n = 0; n < mymodel.nr_groups; n++)
		{
			for(int i =0; i< mymodel.ori_size/2+1; i++)
			{
				wsum_model.sigma2_noise[n].data[i] += thr_wsum_sigma2_noise_H[n*(mymodel.ori_size/2+1)+i];
			}

			//wsum_model.sigma2_noise[n] += thr_wsum_sigma2_noise[n];
			wsum_model.sumw_group[n] += thr_sumw_group[n];
		}
		for (int n = 0; n < mymodel.nr_classes; n++)
		{
			wsum_model.pdf_class[n] += thr_wsum_pdf_class[n];

			if (mymodel.ref_dim == 2)
			{
				XX(wsum_model.prior_offset_class[n]) += thr_wsum_prior_offsetx_class[n];
				YY(wsum_model.prior_offset_class[n]) += thr_wsum_prior_offsety_class[n];
			}
#ifdef CHECKSIZES
			if (XSIZE(wsum_model.pdf_direction[n]) != XSIZE(thr_wsum_pdf_direction[n]))
			{
				std::cerr << " XSIZE(wsum_model.pdf_direction[n])= " << XSIZE(wsum_model.pdf_direction[n]) << " XSIZE(thr_wsum_pdf_direction[n])= " << XSIZE(thr_wsum_pdf_direction[n]) << std::endl;
				REPORT_ERROR("XSIZE(wsum_model.pdf_direction[n]) != XSIZE(thr_wsum_pdf_direction[n])");
			}
#endif
			wsum_model.pdf_direction[n] += thr_wsum_pdf_direction[n];
		}
		wsum_model.sigma2_offset += thr_wsum_sigma2_offset;
	} // end if !do_skip_maximization

	// Check max_weight for each particle and set exp_metadata
	for (int n = 0; n < exp_nr_particles; n++)
	{
		// Equal-to because of the series: the nth images in a series will have the same maximum as the first one
		if (thr_max_weight[n] >= exp_max_weight[n])
		{
			// Set max_weight
			exp_max_weight[n] = thr_max_weight[n];

			// Set metadata
			long int my_image_no = exp_starting_image_no[n] + exp_iseries;
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_ROT)  = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_ROT);
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_TILT) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_TILT);
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PSI)  = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PSI);
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_XOFF) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_XOFF);
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_YOFF) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_YOFF);
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_CLASS)= DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_CLASS);
			DIRECT_A2D_ELEM(exp_metadata, my_image_no, METADATA_PMAX) = DIRECT_A2D_ELEM(thr_metadata, my_image_no, METADATA_PMAX);
		}
	}
TIMER_END

TIMER_START
	gettimeofday(&end, NULL);
	t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);			
	//std::cout << "doThreadStoreWeightedSumsAllOrientations all " << t1/1000000. << std::endl;
	free(Frefctf_H);
	free(Mctf_host );
	free(myscale_host);
	cudaFree(Frefctf_dev);
	cudaFree(Fref_dev);
	cudaFree(myscale_dev);
	cudaFree(Mctf_dev);
	//cudaFree(exp_local_Fimgs_shifted_dev);
	//cudaFree(exp_local_Fimgs_shifted_nomask_dev);

	free(exp_local_Fimgs_shifted_host);
	free(exp_local_Fimgs_nomask_shifted_host);
	cudaFree(exp_significant_sum_weight_dev);
	free(exp_significant_sum_weight_host);
	free(oversampled_translations_host);
	free(weight_array);
	cudaFree(exp_local_weight_dev);
	//cudaFree(wdiff2_dev);
	//cudaFree(sumXA_dev);
	//cudaFree(sumAA_dev);
	//free(wdiff2_host);
	//free(sumXA_host);
	//free(sumAA_host);

	cudaFree(valid_weight_dev);
	cudaFree(ipart_id_dev);
	cudaFree(ishift_id_dev);
	cudaFree(group_id_dev);
	free(group_id_H);
	free(valid_weight);
	free(ipart_id);
	free(ishift_id);
	free(iover_trans_H);
	cudaFree(Fimg_dev);
	cudaFree(Fweight_dev);
	free(Fimg_H);
	free(Fweight_H);
	free(exp_Minvsigma2s_H);

	cudaFree(thr_wsum_sigma2_noise_D);
	cudaFree(thr_wsum_norm_correction_D);
	cudaFree(thr_wsum_scale_correction_XA_D);
	cudaFree(thr_wsum_scale_correction_AA_D);
	cudaFree(Mresol_fine_D);
	cudaFree(data_vs_prior_class_D);

	free(thr_wsum_sigma2_noise_H);
	free(thr_wsum_norm_correction_H);
	free(thr_wsum_scale_correction_XA_H);
	free(thr_wsum_scale_correction_AA_H);	
	for (int i = 0; i < 4; i++)    
	{        
		cudaStreamDestroy(streams[i]);  
	}
TIMER_END
}



void MlOptimiser::expectationSomeParticles_compare(long int my_first_ori_particle, long int my_last_ori_particle)
{

#ifdef TIMING
	timer.tic(TIMING_ESP);
#endif

//#define DEBUG_EXPSINGLE
#ifdef DEBUG_EXPSINGLE
	std::cerr << "Entering expectationSomeParticles..." << std::endl;
#endif

#ifdef TIMING
    timer.tic(TIMING_ESP);
    timer.tic(TIMING_ESP_READ);
#endif

    // Use global variables for thread visibility
	exp_my_first_ori_particle = my_first_ori_particle;
    exp_my_last_ori_particle = my_last_ori_particle;
    exp_nr_ori_particles = exp_my_last_ori_particle - exp_my_first_ori_particle + 1;

    // Find out how many particles there are in these ori_particles
    exp_nr_particles = 0;
    for (long int i = my_first_ori_particle; i <= my_last_ori_particle; i++)
    	exp_nr_particles += mydata.ori_particles[i].particles_id.size();

    // If there are more than one particle in each ori_particle, then do these in parallel with threads
    if (nr_pool == 1 && exp_nr_particles/exp_nr_ori_particles > 1)
    {
    	int my_pool = exp_nr_particles/exp_nr_ori_particles;
    	exp_ipart_ThreadTaskDistributor->resize(my_pool, 1);
    }

    // TODO: MAKE SURE THAT ALL PARTICLES IN SomeParticles ARE FROM THE SAME AREA, SO THAT THE R_mic CAN BE RE_USED!!!

	// In the first iteration, multiple seeds will be generated
	// A single random class is selected for each pool of images, and one does not marginalise over the orientations
	// The optimal orientation is based on signal-product (rather than the signal-intensity sensitive Gaussian)
    // If do_firstiter_cc, then first perform a single iteration with K=1 and cross-correlation criteria, afterwards

    // Generally: use all references
    iclass_min = 0;
    iclass_max = mymodel.nr_classes - 1;
    // low-pass filter again and generate the seeds
    if (do_generate_seeds)
    {
    	if (do_firstiter_cc && iter == 1)
    	{
    		// In first (CC) iter, use a single reference (and CC)
    		iclass_min = iclass_max = 0;
    	}
    	else if ( (do_firstiter_cc && iter == 2) || (!do_firstiter_cc && iter == 1))
		{
			// In second CC iter, or first iter without CC: generate the seeds
    		// Now select a single random class
    		// exp_part_id is already in randomized order (controlled by -seed)
    		// WARNING: USING SAME iclass_min AND iclass_max FOR SomeParticles!!
			iclass_min = iclass_max = divide_equally_which_group(mydata.numberOfOriginalParticles(), mymodel.nr_classes, exp_my_first_ori_particle);
		}
    }


//		    std::cout << "iclass_min = " << iclass_min << " iclass_max = " << iclass_max << std::endl;
//		    std::cout << nr_pool << std::endl;
//		    std::cout << mydata.numberOfOriginalParticles() << " " << mymodel.nr_classes << " " << exp_my_first_ori_particle << std::endl;
	// TODO: think of a way to have the different images in a single series have DIFFERENT offsets!!!
	// Right now, they are only centered with a fixed relative translation!!!!

// Thid debug is a good one to step through the separate steps of the expectation to see where trouble lies....
//#define DEBUG_ESP_MEM
#ifdef DEBUG_ESP_MEM
	char c;
	std::cerr << "Before getFourierTransformsAndCtfs, press any key to continue... " << std::endl;
	std::cin >> c;
#endif

	// Read all image of this series into memory, apply old origin offsets and store Fimg, Fctf, exp_old_xoff and exp_old_yoff in vectors./
	long t1, t21, t22, t31, t32, t41, t42;
	t1 = t21 = t22 = t31 = t32 = t41 = t42 = 0;
	struct timeval start, end;

gettimeofday(&start, NULL);
	std::vector<MultidimArray<Complex > > exp_Fimgs_cpu, exp_Fimgs_gpu, exp_Fimgs_nomask_cpu, exp_Fimgs_nomask_gpu;
	std::vector<MultidimArray<double> > exp_Fctfs_cpu, exp_Fctfs_gpu;
	std::vector<MultidimArray<double> > exp_power_imgs_cpu, exp_power_imgs_gpu;
	std::vector<double> exp_highres_Xi2_imgs_cpu,exp_highres_Xi2_imgs_gpu ;
	

	do_nothing();
	
	exp_Fimgs_cpu = exp_Fimgs;
	exp_Fimgs_nomask_cpu = exp_Fimgs_nomask;
	exp_Fctfs_cpu = exp_Fctfs;
	exp_power_imgs_cpu = exp_power_imgs;
	exp_highres_Xi2_imgs_cpu =exp_highres_Xi2_imgs;

	doThreadGetFourierTransformsAndCtfs_gpu();
	exp_Fimgs_gpu = exp_Fimgs;
	exp_Fimgs_nomask_gpu = exp_Fimgs_nomask;
	exp_Fctfs_gpu = exp_Fctfs;
	exp_power_imgs_gpu = exp_power_imgs;
	exp_highres_Xi2_imgs_gpu =exp_highres_Xi2_imgs;
	
	for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
	{
			FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(exp_Fimgs[ipart])
			{
				Complex  img_value, ima_value_nomask ;
				img_value = FFTW_ELEM(exp_Fimgs[ipart], k, i, j);
				ima_value_nomask = FFTW_ELEM(exp_Fimgs_nomask[ipart], k, i, j);
				if (fabs(FFTW_ELEM(exp_Fimgs_cpu[ipart], k, i, j).real -FFTW_ELEM(exp_Fimgs_gpu[ipart], k, i, j).real)>1e-5 || fabs(FFTW_ELEM(exp_Fimgs_cpu[ipart], k, i, j).imag -FFTW_ELEM(exp_Fimgs_gpu[ipart], k, i, j).imag)>1e-5)
				{	
					std::cout << "exp_Fimgs wrong in real  # " << i << " : " << FFTW_ELEM(exp_Fimgs_cpu[ipart], k, i, j).real << " != " << FFTW_ELEM(exp_Fimgs_gpu[ipart], k, i, j).real << " exp_Fimgs wrong in img # " << i << " : " << FFTW_ELEM(exp_Fimgs_cpu[ipart], k, i, j).imag << " != " << FFTW_ELEM(exp_Fimgs_gpu[ipart], k, i, j).imag << std::endl;
				}
				//printf("%lf = %lf = %lf\n", fabs(FFTW_ELEM(exp_Fimgs_cpu[ipart], k, i, j) - FFTW_ELEM(exp_Fimgs_gpu[ipart], k, i, j)), exp_Mweight.data[i], exp_Mweight_gpu[i]);
				if (fabs(FFTW_ELEM(exp_Fimgs_nomask_cpu[ipart], k, i, j).real-FFTW_ELEM(exp_Fimgs_nomask_gpu[ipart], k, i, j).real)>1e-5|| fabs(FFTW_ELEM(exp_Fimgs_nomask_cpu[ipart], k, i, j).imag-FFTW_ELEM(exp_Fimgs_nomask_gpu[ipart], k, i, j).imag)>1e-5) 
				{
					std::cout << "exp_Fimgs_nomask_cpu wrong in real# " << i << " : " << FFTW_ELEM(exp_Fimgs_nomask_cpu[ipart], k, i, j).real << " != " << FFTW_ELEM(exp_Fimgs_nomask_gpu[ipart], k, i, j).real << " exp_Fimgs_nomask_cpu wrong in img # " << i << " : " << FFTW_ELEM(exp_Fimgs_nomask_cpu[ipart], k, i, j).imag << " != " << FFTW_ELEM(exp_Fimgs_nomask_gpu[ipart], k, i, j).imag << std::endl;
					std::cout << k << " "  << i << " " << j << std::endl;
				}
				//break;			
			}
	}

	for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
	{
			FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(exp_Fctfs[ipart])
			{
				if (fabs(FFTW_ELEM(exp_Fctfs_cpu[ipart], k, i, j) -FFTW_ELEM(exp_Fctfs_gpu[ipart], k, i, j))>1e-5 )
				{	
					std::cout << "exp_Fctfs_cpu wrong in real  # " << i << " : " << FFTW_ELEM(exp_Fctfs_cpu[ipart], k, i, j) << " != " << FFTW_ELEM(exp_Fctfs_gpu[ipart], k, i, j) << std::endl;
					std::cout << k << " "  << i << " " << j << std::endl;
				}
				//break;			
			}
	}

	for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
	{
			FOR_ALL_ELEMENTS_IN_FFTW_TRANSFORM(exp_power_imgs[ipart])
			{
				if (fabs(FFTW_ELEM(exp_power_imgs_cpu[ipart], k, i, j) -FFTW_ELEM(exp_power_imgs_gpu[ipart], k, i, j))>1e-5 )
				{	
					std::cout << "exp_power_imgs wrong in real  # " << i << " : " << FFTW_ELEM(exp_power_imgs_cpu[ipart], k, i, j) << " != " << FFTW_ELEM(exp_power_imgs_gpu[ipart], k, i, j) << std::endl;
					std::cout << k << " "  << i << " " << j << std::endl;
				}
				//break;			
			}
	}
	for (long int ipart = 0;  ipart < exp_nr_images; ipart++)
	{
		if (fabs(exp_highres_Xi2_imgs_cpu[ipart] -exp_highres_Xi2_imgs_gpu[ipart])>1e-5)
		{	
					std::cout << "exp_highres_Xi2_imgs wrong in real  # " << ipart << " : " << (exp_highres_Xi2_imgs_cpu[ipart]) << " != " << (exp_highres_Xi2_imgs_gpu[ipart]) << std::endl;
		}

	}
	

gettimeofday(&end, NULL);
	t1 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

	//doThreadGetFourierTransformsAndCtfs
	if (do_realign_movies )//&& movie_frame_running_avg_side > 0)
	{
		calculateRunningAveragesOfMovieFrames();
	}

#ifdef DEBUG_ESP_MEM
	std::cerr << "After getFourierTransformsAndCtfs, press any key to continue... " << std::endl;
	std::cin >> c;
#endif

	#ifdef TIMING
    timer.toc(TIMING_ESP_READ);
#endif

	// Initialise significant weight to minus one, so that all coarse sampling points will be handled in the first pass
	exp_significant_weight.clear();
	exp_significant_weight.resize(exp_nr_particles);
	for (int n = 0; n < exp_nr_particles; n++)
		exp_significant_weight[n] = -1.;

	// Number of rotational and translational sampling points
	exp_nr_trans = sampling.NrTranslationalSamplings();

	exp_nr_dir = sampling.NrDirections();
	exp_nr_psi = sampling.NrPsiSamplings();
	exp_nr_rot = exp_nr_dir * exp_nr_psi;

	// Only perform a second pass when using adaptive oversampling
	int nr_sampling_passes = (adaptive_oversampling > 0) ? 2 : 1;

	// Pass twice through the sampling of the entire space of rot, tilt and psi
	// The first pass uses a coarser angular sampling and possibly smaller FFTs than the second pass.
	// Only those sampling points that contribute to the highest x% of the weights in the first pass are oversampled in the second pass
	// Only those sampling points will contribute to the weighted sums in the third loop below
	for (exp_ipass = 0; exp_ipass < nr_sampling_passes; exp_ipass++)
	{

		if (strict_highres_exp > 0.)
			// Use smaller images in both passes and keep a maximum on coarse_size, just like in FREALIGN
			exp_current_image_size = coarse_size;
		else if (adaptive_oversampling > 0)
			// Use smaller images in the first pass, larger ones in the second pass
			exp_current_image_size = (exp_ipass == 0) ? coarse_size : mymodel.current_size;
		else
			exp_current_image_size = mymodel.current_size;

		// Use coarse sampling in the first pass, oversampled one the second pass
		exp_current_oversampling = (exp_ipass == 0) ? 0 : adaptive_oversampling;
		exp_nr_oversampled_rot = sampling.oversamplingFactorOrientations(exp_current_oversampling);
		exp_nr_oversampled_trans = sampling.oversamplingFactorTranslations(exp_current_oversampling);


#ifdef DEBUG_ESP_MEM

		std::cerr << "Before getAllSquaredDifferences, use top to see memory usage and then press any key to continue... " << std::endl;
		std::cin >> c;
#endif
gettimeofday(&start, NULL);
		// Calculate the squared difference terms inside the Gaussian kernel for all hidden variables
		getAllSquaredDifferences();
gettimeofday(&end, NULL);
		t21 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

gettimeofday(&start, NULL);
		getAllSquaredDifferences_gpu();
gettimeofday(&end, NULL);
		t22 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

		

		int tot = 0;
		double *exp_Mweight_gpu = new double[exp_nr_particles * exp_Mweight_dev_size];
		cudaMemcpy(exp_Mweight_gpu, exp_Mweight_dev,  exp_nr_particles * exp_Mweight_dev_size*sizeof(double), cudaMemcpyDeviceToHost);
		for (int i = 0; i < exp_nr_particles * exp_Mweight_dev_size; i++) {
			if (fabs(exp_Mweight.data[i]-exp_Mweight_gpu[i])>1e-4 && tot < 5) {
				std::cout << "exp_Mweight_gpu wrong in # " << i << " : " << exp_Mweight.data[i] << " != " << exp_Mweight_gpu[i] << std::endl;
				printf("%lf = %lf = %lf\n", fabs(exp_Mweight.data[i] - exp_Mweight_gpu[i]), exp_Mweight.data[i], exp_Mweight_gpu[i]);
				tot++;
				//break;
			}
		}
		free(exp_Mweight_gpu);

		double exp_min_diff2_gpu[exp_nr_particles];
		cudaMemcpy(exp_min_diff2_gpu, exp_min_diff2_dev,  exp_nr_particles*sizeof(double), cudaMemcpyDeviceToHost);
		for (int i = 0; i < exp_nr_particles; i++)
			if (fabs(exp_min_diff2_gpu[i]-exp_min_diff2[i])>1e-4) {
				std::cout << "exp_min_diff2_gpu wrong in # " << i << " : " << exp_min_diff2[i] << " != " << exp_min_diff2_gpu[i] << std::endl;
				//break;
			}
		

		
		cudaMemcpy(exp_Mweight_dev, exp_Mweight.data,  exp_nr_particles * exp_Mweight_dev_size*sizeof(double), cudaMemcpyHostToDevice);
		for (int i = 0; i < exp_nr_particles; i++) {
			exp_min_diff2_gpu[i] = exp_min_diff2[i];
		}
		cudaMemcpy(exp_min_diff2_dev, exp_min_diff2_gpu, exp_nr_particles*sizeof(double), cudaMemcpyHostToDevice);
		





#ifdef DEBUG_ESP_MEM
		std::cerr << "After getAllSquaredDifferences, use top to see memory usage and then press any key to continue... " << std::endl;
		std::cin >> c;
#endif

		// Now convert the squared difference terms to weights,
		// also calculate exp_sum_weight, and in case of adaptive oversampling also exp_significant_weight


		MultidimArray<double> exp_Mweight_before, exp_Mweight_after;
		std::vector<double> exp_sum_weight_before, exp_sum_weight_after;
		MultidimArray<bool> exp_Mcoarse_significant_before, exp_Mcoarse_significant_after;
		std::vector<double> exp_significant_weight_before, exp_significant_weight_after;

		exp_Mweight_before = exp_Mweight;
		exp_sum_weight_before = exp_sum_weight;
		exp_Mcoarse_significant_before = exp_Mcoarse_significant;
		exp_significant_weight_before = exp_significant_weight;

gettimeofday(&start, NULL);
		convertAllSquaredDifferencesToWeights();
gettimeofday(&end, NULL);
		t31 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);

		

		exp_Mweight_after = exp_Mweight;
		exp_sum_weight_after = exp_sum_weight;
		exp_Mcoarse_significant_after = exp_Mcoarse_significant;
		exp_significant_weight_after = exp_significant_weight;

		exp_Mweight = exp_Mweight_before;
		exp_sum_weight = exp_sum_weight_before;
		exp_Mcoarse_significant = exp_Mcoarse_significant_before;
		exp_significant_weight = exp_significant_weight_before;

	

gettimeofday(&start, NULL);
		convertAllSquaredDifferencesToWeights_gpu();
gettimeofday(&end, NULL);
		t32 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);


		

		for (int i = 0; i < XSIZE(exp_Mweight); i++) {
			if (fabs(exp_Mweight.data[i]-exp_Mweight_after.data[i])>1e-6) {
				std::cout << "exp_Mweight wrong in # " << i << " : " << exp_Mweight.data[i] << " != " << exp_Mweight_after.data[i] << std::endl;
				//exp_Mweight = exp_Mweight_after;
			}
		}
		for (int i = 0; i < exp_sum_weight.size(); i++) {
			if (fabs(exp_sum_weight[i]-exp_sum_weight_after[i])>1e-6) {
				std::cout << "exp_sum_weight wrong in # " << i << " : " << exp_sum_weight[i] << " != " << exp_sum_weight_after[i] << std::endl;
				//exp_sum_weight = exp_sum_weight_after;
			}
		}
		for (int i = 0; i < XSIZE(exp_Mcoarse_significant); i++) {
			if (exp_Mcoarse_significant.data[i] != exp_Mcoarse_significant_after.data[i]) {
				std::cout << "exp_Mcoarse_significant wrong in # " << i << " : " << exp_Mcoarse_significant.data[i] << " != " << exp_Mcoarse_significant_after.data[i] << std::endl;
				//exp_Mcoarse_significant = exp_Mcoarse_significant_after;
			}
		}
		for (int i = 0; i < exp_significant_weight.size(); i++) {
			if (fabs(exp_significant_weight[i]-exp_significant_weight_after[i])>1e-6) {
				std::cout << "exp_significant_weight wrong in #" << i << " : " << exp_significant_weight[i] << " != " << exp_significant_weight_after[i] << std::endl;
				//exp_significant_weight = exp_significant_weight_after;
			}
		}

		

		if (exp_ipass == 0)
		{
			cudaFree(exp_local_sqrtXi2_dev);
			cudaFree(exp_local_Fimgs_shifted_dev);
			cudaFree(exp_local_Fimgs_shifted_nomask_dev);
			cudaFree(exp_Minvsigma2s_dev);
			cudaFree(exp_local_Fctfs_dev);		
		}

		double* exp_min_diff2_host = new double[exp_nr_particles];
		cudaMemcpy(exp_min_diff2_host, exp_min_diff2_dev, exp_nr_particles*sizeof(double), cudaMemcpyDeviceToHost);
		exp_min_diff2.resize(exp_nr_particles);
		for (int i = 0; i < exp_nr_particles; i++) 
			exp_min_diff2[i] = exp_min_diff2_host[i];

		delete[]exp_min_diff2_host;

		cudaFree(exp_min_diff2_dev);
		cudaFree(exp_Mweight_dev);
		
		

#ifdef DEBUG_ESP_MEM
		std::cerr << "After convertAllSquaredDifferencesToWeights, press any key to continue... " << std::endl;
		std::cin >> c;
#endif

	}// end loop over 2 exp_ipass iterations


	// For the reconstruction step use mymodel.current_size!
	exp_current_image_size = mymodel.current_size;

#ifdef DEBUG_ESP_MEM
	std::cerr << "Before storeWeightedSums, press any key to continue... " << std::endl;
	std::cin >> c;
#endif


	MultidimArray<double> exp_metadata_before, exp_metadata_after;
	exp_metadata_before = exp_metadata;


	std::vector<MultidimArray<double> > exp_wsum_scale_correction_XA_before, exp_wsum_scale_correction_XA_after;
	exp_wsum_scale_correction_XA_before = exp_wsum_scale_correction_XA;

	std::vector<double> exp_wsum_norm_correction_before, exp_wsum_norm_correction_after;
	exp_wsum_norm_correction_before = exp_wsum_norm_correction;

	std::vector<MultidimArray<double> > wsum_model_sigma2_noise_before, wsum_model_sigma2_noise_after;
	wsum_model_sigma2_noise_before = wsum_model.sigma2_noise;

	std::vector<double> wsum_model_sumw_group_before, wsum_model_sumw_group_after;
	wsum_model_sumw_group_before = wsum_model.sumw_group;

	std::vector<double> wsum_model_pdf_class_before, wsum_model_pdf_class_after;
	wsum_model_pdf_class_before = wsum_model.pdf_class;

	std::vector<MultidimArray<double> > wsum_model_pdf_direction_before, wsum_model_pdf_direction_after;
	wsum_model_pdf_direction_before = wsum_model.pdf_direction;

gettimeofday(&start, NULL);
	storeWeightedSums();
gettimeofday(&end, NULL);
	t41 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);	

	exp_metadata_after = exp_metadata;
	exp_metadata = exp_metadata_before;

	exp_wsum_scale_correction_XA_after = exp_wsum_scale_correction_XA;
	exp_wsum_scale_correction_XA = exp_wsum_scale_correction_XA_before;

	exp_wsum_norm_correction_after = exp_wsum_norm_correction;
	exp_wsum_norm_correction = exp_wsum_norm_correction_before;	


	wsum_model_sigma2_noise_after = wsum_model.sigma2_noise;
	wsum_model.sigma2_noise = wsum_model_sigma2_noise_before;

	wsum_model_sumw_group_after = wsum_model.sumw_group;
	wsum_model.sumw_group = wsum_model_sumw_group_before;

	wsum_model_pdf_class_after = wsum_model.pdf_class;
	wsum_model.pdf_class = wsum_model_pdf_class_before;

	wsum_model_pdf_direction_after = wsum_model.pdf_direction;
	wsum_model.pdf_direction = wsum_model_pdf_direction_before;

gettimeofday(&start, NULL);
	storeWeightedSums_gpu();
gettimeofday(&end, NULL);
	t42 += (end.tv_sec-start.tv_sec)*1000000+(end.tv_usec-start.tv_usec);	

	for (int i = 0; i < MULTIDIM_SIZE(exp_metadata); i++) {
		if (fabs(exp_metadata.data[i]-exp_metadata_after.data[i])>1e-6) {
			std::cout << "exp_metadata wrong in # " << i << " : " << exp_metadata.data[i] << " != " << exp_metadata_after.data[i] << std::endl;
			//exp_metadata = exp_metadata_after;
		}
	}

	for (int i = 0; i < exp_nr_particles; i++) {
		for (int j = 0; j < MULTIDIM_SIZE(exp_wsum_scale_correction_XA[i]); j++) {
			if (fabs(exp_wsum_scale_correction_XA[i].data[j] - exp_wsum_scale_correction_XA_after[i].data[j])>1e-6) {
				std::cout << "exp_wsum_scale_correction_XA in # " << i << " " << j << " : " << exp_wsum_scale_correction_XA[i].data[j] << " != " << exp_wsum_scale_correction_XA_after[i].data[j] << std::endl;
				//exp_metadata = exp_metadata_after;
			}
		}
	}

	for (int i = 0; i < exp_wsum_norm_correction.size(); i++) {
		if (fabs(exp_wsum_norm_correction_after[i] - exp_wsum_norm_correction[i])>1e-6) {
			std::cout << "exp_wsum_norm_correction wrong in # " << i << " : " << exp_wsum_norm_correction_after[i] << " != " << exp_wsum_norm_correction[i] << std::endl;
			//exp_metadata = exp_metadata_after;
		}
	}

	for (int i = 0; i < mymodel.nr_groups; i++) {
		for (int j = 0; j < MULTIDIM_SIZE(wsum_model.sigma2_noise[i]); j++) {
			if (fabs(wsum_model.sigma2_noise[i].data[j] - wsum_model_sigma2_noise_after[i].data[j])>1e-6) {
				std::cout << "wsum_model_sigma2_noise in # " << i << " " << j << " : " << wsum_model.sigma2_noise[i].data[j] << " != " << wsum_model_sigma2_noise_after[i].data[j] << std::endl;
				//exp_metadata = exp_metadata_after;
			}
		}
	}

	for (int i = 0; i < wsum_model.sumw_group.size(); i++) {
		if (fabs(wsum_model.sumw_group[i] - wsum_model_sumw_group_after[i])>1e-6) {
			std::cout << "wsum_model.sumw_group wrong in # " << i << " : " << wsum_model.sumw_group[i] << " != " << wsum_model_sumw_group_after[i] << std::endl;
		}
	}

	for (int i = 0; i < wsum_model.pdf_class.size(); i++) {
		if (fabs(wsum_model.pdf_class[i] - wsum_model_pdf_class_after[i])>1e-6) {
			std::cout << "wsum_model.pdf_class wrong in # " << i << " : " << wsum_model.pdf_class[i] << " != " << wsum_model_pdf_class_after[i] << std::endl;
		}
	}

	for (int i = 0; i < mymodel.nr_classes; i++) {
		for (int j = 0; j < MULTIDIM_SIZE(wsum_model.pdf_direction[i]); j++) {
			if (fabs(wsum_model.pdf_direction[i].data[j] - wsum_model_pdf_direction_after[i].data[j])>1e-6) {
				std::cout << "wsum_model_sigma2_noise in # " << i << " " << j << " : " << wsum_model.pdf_direction[i].data[j] << " != " << wsum_model_pdf_direction_after[i].data[j] << std::endl;
				//exp_metadata = exp_metadata_after;
			}
		}
	}

	std::cout << my_first_ori_particle << " expectationSomeParticles_compare " << t1/1000000. << " " << t21/1000000. << "=" << t22/1000000. << " " << t31/1000000. << "=" << t32/1000000. << " " << t41/1000000. << "=" << t42/1000000. << std::endl;

	cudaFree(exp_local_sqrtXi2_dev);
	cudaFree(exp_local_Fimgs_shifted_dev);
	cudaFree(exp_local_Fimgs_shifted_nomask_dev);
	cudaFree(exp_Minvsigma2s_dev);
	cudaFree(exp_local_Fctfs_dev);	

	cudaFree(exp_Fimgs_dev);
	cudaFree(exp_Fimgs_nomask_dev);
	// Now calculate the optimal translation for each of the individual images in the series
	//if (mydata.maxNumberOfImagesPerOriginalParticle(my_first_ori_particle, my_last_ori_particle) > 1 && !(do_firstiter_cc && iter == 1))
	//	getOptimalOrientationsForIndividualImagesInSeries();

#ifdef DEBUG_ESP_MEM
	std::cerr << "After storeWeightedSums, press any key to continue... " << std::endl;
	std::cin >> c;
#endif
#ifdef DEBUG_EXPSINGLE
		std::cerr << "Leaving expectationSingleParticle..." << std::endl;
#endif

#ifdef TIMING
	timer.toc(TIMING_ESP);
#endif

}


